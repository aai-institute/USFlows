
@article{achille_emergence_2018,
  title = {Emergence of {{Invariance}} and {{Disentanglement}} in {{Deep Representations}}},
  author = {Achille, Alessandro and Soatto, Stefano},
  year = {2018},
  month = jun,
  journal = {arXiv:1706.01350 [cs, stat]},
  eprint = {1706.01350},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overfitting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underfitting and overfitting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00000},
  file = {/Users/fariedabuzaid/Zotero/storage/DDBPJIAS/Achille and Soatto - 2018 - Emergence of Invariance and Disentanglement in Dee.pdf}
}

@inproceedings{adi_turning_2018,
  title = {Turning {{Your Weakness Into}} a {{Strength}}: {{Watermarking Deep Neural Networks}} by {{Backdooring}}},
  shorttitle = {Turning {{Your Weakness Into}} a {{Strength}}},
  booktitle = {27th \{\vphantom\}{{USENIX}}\vphantom\{\} {{Security Symposium}} (\{\vphantom\}{{USENIX}}\vphantom\{\} {{Security}} 18)},
  author = {Adi, Yossi and Baum, Carsten and Cisse, Moustapha and Pinkas, Benny and Keshet, Joseph},
  year = {2018},
  eprint = {1802.04633},
  eprinttype = {arxiv},
  pages = {1615--1631},
  abstract = {Deep Neural Networks have recently gained lots of success after enabling several breakthroughs in notoriously challenging problems. Training these networks is computationally expensive and requires vast amounts of training data. Selling such pre-trained models can, therefore, be a lucrative business model. Unfortunately, once the models are sold they can be easily copied and redistributed. To avoid this, a tracking mechanism to identify models as the intellectual property of a particular vendor is necessary. In this work, we present an approach for watermarking Deep Neural Networks in a black-box way. Our scheme works for general classification tasks and can easily be combined with current learning algorithms. We show experimentally that such a watermark has no noticeable impact on the primary task that the model is designed for and evaluate the robustness of our proposal against a multitude of practical attacks. Moreover, we provide a theoretical analysis, relating our approach to previous work on backdooring.},
  archiveprefix = {arXiv},
  isbn = {978-1-939133-04-5},
  langid = {english},
  annotation = {video: https://youtu.be/Fj-4i7BwKGM},
  file = {/Users/fariedabuzaid/Zotero/storage/JPP9NQMP/Adi et al. - 2018 - Turning Your Weakness Into a Strength Watermarkin.pdf;/Users/fariedabuzaid/Zotero/storage/ZW7M7Q4X/security18_slides_baum.pdf}
}

@book{aggarwal_outlier_2017,
  title = {Outlier {{Analysis}}},
  author = {Aggarwal, Charu C.},
  year = {2017},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-47578-3},
  abstract = {Outliers are also referred to as abnormalities, discordants, deviants, or anomalies in the data mining and statistics literature. In most applications, the data is created by one or more generating processes, which could either reflect activity in the system or observations collected about entities. When the generating process behaves unusually, it results in the creation of outliers. Therefore, an outlier often contains useful information about abnormal characteristics of the systems and entities that impact the data generation process. The recognition of such unusual characteristics provides useful application-specific insights.},
  isbn = {978-3-319-47577-6 978-3-319-47578-3},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/QN53AWRZ/Aggarwal - 2017 - Outlier Analysis.pdf}
}

@inproceedings{ahn_spanning_2022,
  title = {Spanning {{Tree-based Graph Generation}} for {{Molecules}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Ahn, Sungsoo and Chen, Binghong and Wang, Tianzhe and Song, Le},
  year = {2022},
  abstract = {In this paper, we explore the problem of generating molecules using deep neural networks, which has recently gained much interest in chemistry. To this end, we propose a spanning tree-based graph...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/86FJKTHZ/Ahn et al. - 2021 - Spanning Tree-based Graph Generation for Molecules.pdf}
}

@article{al-aradi_solving_2018,
  title = {Solving {{Nonlinear}} and {{High-Dimensional Partial Differential Equations}} via {{Deep Learning}}},
  author = {{Al-Aradi}, Ali and Correia, Adolfo and Naiff, Danilo and Jardim, Gabriel and Saporito, Yuri},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.08782 [q-fin]},
  eprint = {1811.08782},
  eprinttype = {arxiv},
  primaryclass = {q-fin},
  abstract = {In this work we apply the Deep Galerkin Method (DGM) described in Sirignano and Spiliopoulos (2018) to solve a number of partial differential equations that arise in quantitative finance applications including option pricing, optimal execution, mean field games, etc. The main idea behind DGM is to represent the unknown function of interest using a deep neural network. A key feature of this approach is the fact that, unlike other commonly used numerical approaches such as finite difference methods, it is mesh-free. As such, it does not suffer (as much as other numerical methods) from the curse of dimensionality associated with highdimensional PDEs and PDE systems. The main goals of this paper are to elucidate the features, capabilities and limitations of DGM by analyzing aspects of its implementation for a number of different PDEs and PDE systems. Additionally, we present: (1) a brief overview of PDEs in quantitative finance along with numerical methods for solving them; (2) a brief overview of deep learning and, in particular, the notion of neural networks; (3) a discussion of the theoretical foundations of DGM with a focus on the justification of why this method is expected to perform well.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/K93K88UX/Al-Aradi et al. - 2018 - Solving Nonlinear and High-Dimensional Partial Dif.pdf}
}

@inproceedings{alemi_deep_2017,
  title = {Deep {{Variational Information Bottleneck}}},
  booktitle = {Proceedings of {{ICLR}} 2017},
  author = {Alemi, Alex and Fischer, Ian and Dillon, Josh and Murphy, Kevin},
  year = {2017},
  month = apr,
  address = {{Toulon, France}},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/R3SU67RC/Alemi et al. - 2017 - Deep Variational Information Bottleneck.pdf}
}

@article{alom_history_2018,
  title = {The {{History Began}} from {{AlexNet}}: {{A Comprehensive Survey}} on {{Deep Learning Approaches}}},
  author = {Alom, Zahangir and Taha, Tarek M and Yakopcic, Chris and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima},
  year = {2018},
  month = sep,
  pages = {39},
  abstract = {In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly, and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bio-informatics, natural language processing (NLP), cybersecurity, and many others.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/WTEEVLDJ/Alom et al. - The History Began from AlexNet A Comprehensive Su.pdf}
}

@article{alvarez_estimation_2014,
  title = {Estimation of the {{Lower}} and {{Upper Probabilities}} of {{Failure Using Random Sets}} and {{Subset Simulation}}},
  author = {Alvarez, Diego A. and Hurtado, Jorge E. and Uribe, Felipe},
  year = {2014},
  month = jul,
  pages = {905--914},
  publisher = {{American Society of Civil Engineers}},
  doi = {10.1061/9780784413609.092},
  abstract = {The focus of the present paper is the determination of the lower and upper bounds of the probability of failure under uncertain inputs by means of random set theory. Under this general framework, it is possible to model uncertainty in the form of probability boxes, fuzzy sets, cumulative distribution functions, Dempster-Shafer structures or intervals. In addition, the dependence between the input variables can be expressed using copulas. In order to speed up the calculation, a very efficient probability-based reliability method - known as "subset simulation" - will be used. This method is specially suited for finding small probabilities of failure in both low- and high-dimensional spaces, disjoint failure regions and nonlinear limit state functions. The proposed method represents a drastic reduction of the computational labor implied by plain Monte Carlo simulation for problems defined with a mixture of representations for the input variables, while delivering similar results. A numerical example shows the usefulness of the proposed approach.},
  isbn = {9780784413609},
  langid = {english}
}

@inproceedings{amayuelas_neural_2022,
  title = {Neural {{Methods}} for {{Logical Reasoning}} over {{Knowledge Graphs}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Amayuelas, Alfonso and Zhang, Shuai and Rao, Xi Susie and Zhang, Ce},
  year = {2022},
  abstract = {Reasoning is a fundamental problem for computers and deeply studied in Artificial Intelligence. In this paper, we specifically focus on answering multi-hop logical queries on Knowledge Graphs...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/UF6HJNMZ/Amayuelas et al. - 2022 - Neural Methods for Logical Reasoning over Knowledg.pdf}
}

@article{andrychowicz_learning_2016,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and {de Freitas}, Nando},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.04474 [cs]},
  eprint = {1606.04474},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {17},
  abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00690},
  file = {/Users/fariedabuzaid/Zotero/storage/ABS58KT5/Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient .pdf}
}

@inproceedings{bai_don_2021,
  title = {Don't {{Just Blame Over-parametrization}} for {{Over-confidence}}: {{Theoretical Analysis}} of {{Calibration}} in {{Binary Classification}}},
  shorttitle = {Don't {{Just Blame Over-parametrization}} for {{Over-confidence}}},
  booktitle = {{{arXiv}}:2102.07856 [Cs, Math, Stat]},
  author = {Bai, Yu and Mei, Song and Wang, Huan and Xiong, Caiming},
  year = {2021},
  month = jul,
  eprint = {2102.07856},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  address = {{Virtual event}},
  abstract = {Modern machine learning models with high accuracy are often miscalibrated -- the predicted top probability does not reflect the actual accuracy, and tends to be over-confident. It is commonly believed that such over-confidence is mainly due to over-parametrization, in particular when the model is large enough to memorize the training data and maximize the confidence. In this paper, we show theoretically that over-parametrization is not the only reason for over-confidence. We prove that logistic regression is inherently over-confident, in the realizable, under-parametrized setting where the data is generated from the logistic model, and the sample size is much larger than the number of parameters. Further, this over-confidence happens for general well-specified binary classification problems as long as the activation is symmetric and concave on the positive part. Perhaps surprisingly, we also show that over-confidence is not always the case -- there exists another activation function (and a suitable loss function) under which the learned classifier is under-confident at some probability values. Overall, our theory provides a precise characterization of calibration in realizable binary classification, which we verify on simulations and real data experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/EWZCIJTX/Bai et al. - 2021 - Don't Just Blame Over-parametrization for Over-con.pdf}
}

@inproceedings{bai_recent_2021,
  title = {Recent {{Advances}} in {{Adversarial Training}} for {{Adversarial Robustness}}},
  booktitle = {{{arXiv}}:2102.01356 [Cs]},
  author = {Bai, Tao and Luo, Jinqi and Zhao, Jun and Wen, Bihan and Wang, Qian},
  year = {2021},
  month = apr,
  eprint = {2102.01356},
  eprinttype = {arxiv},
  primaryclass = {cs},
  address = {{Montreal, Canada}},
  abstract = {Adversarial training is one of the most effective approaches defending against adversarial examples for deep learning models. Unlike other defense strategies, adversarial training aims to promote the robustness of models intrinsically. During the last few years, adversarial training has been studied and discussed from various aspects. A variety of improvements and developments of adversarial training are proposed, which were, however, neglected in existing surveys. For the first time in this survey, we systematically review the recent progress on adversarial training for adversarial robustness with a novel taxonomy. Then we discuss the generalization problems in adversarial training from three perspectives. Finally, we highlight the challenges which are not fully tackled and present potential future directions.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/FR6WDI36/Bai et al. - 2021 - Recent Advances in Adversarial Training for Advers.pdf}
}

@inproceedings{bakker_dadi_2019,
  title = {{{DADI}}: {{Dynamic Discovery}} of {{Fair Information}} with {{Adversarial Reinforcement Learning}}},
  shorttitle = {{{DADI}}},
  booktitle = {{{ArXiv}}},
  author = {Bakker, M. and Tu, Duy Patrick and River{\'o}n Vald{\'e}s, Humberto and Gummadi, K. and Varshney, Kush R. and Weller, Adrian and Pentland, A.},
  year = {2019},
  abstract = {We introduce a framework for dynamic adversarial discovery of information (DADI), motivated by a scenario where information (a feature set) is used by third parties with unknown objectives. We train a reinforcement learning agent to sequentially acquire a subset of the information while balancing accuracy and fairness of predictors downstream. Based on the set of already acquired features, the agent decides dynamically to either collect more information from the set of available features or to stop and predict using the information that is currently available. Building on previous work exploring adversarial representation learning, we attain group fairness (demographic parity) by rewarding the agent with the adversary's loss, computed over the final feature set. Importantly, however, the framework provides a more general starting point for fair or private dynamic information discovery. Finally, we demonstrate empirically, using two real-world datasets, that we can trade-off fairness and predictive performance},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/Z5XJKLK6/Bakker et al. - 2019 - DADI Dynamic Discovery of Fair Information with A.pdf}
}

@inproceedings{bakker_fairness_2019,
  title = {On {{Fairness}} in {{Budget-Constrained Decision Making}}},
  booktitle = {{{KDD}} '19: {{Workshop}} on {{Explainable AI}}/{{ML}} ({{XAI}}) for {{Accountability}}, {{Fairness}}, and {{Transparency}}},
  author = {Bakker, Michiel A and {Noriega-Campero}, Alejandro and Tu, Duy Patrick and Sattigeri, Prasanna and Varshney, Kush R},
  year = {2019},
  month = aug,
  pages = {8},
  publisher = {{Association for Computing Machinery}},
  address = {{Anchorage, Alaska}},
  abstract = {The machine learning community and society at large have become increasingly concerned with discrimination and bias in data-driven decision making systems. This has led to a dramatic increase in academic and popular interest in algorithmic fairness. In this work, we focus on fairness in budget-constrained decision making, where the goal is to acquire information (features) one-by-one for each individual to achieve maximum classification performance in a cost-effective way. We provide a framework for choosing a set of stopping criteria that ensures that a probabilistic classifier achieves a single error parity (e.g. equal opportunity) and calibration. Our framework scales efficiently to multiple protected attributes and is not susceptible to intra-group unfairness. Finally, using one synthetic and two public datasets, we confirm the effectiveness of our framework and investigate its limitations.},
  langid = {english},
  annotation = {citecount: 00001},
  file = {/Users/fariedabuzaid/Zotero/storage/3N93CC23/Bakker et al. - 2019 - On Fairness in Budget-Constrained Decision Making.pdf}
}

@article{banino_pondernet_2021,
  title = {{{PonderNet}}: {{Learning}} to {{Ponder}}},
  shorttitle = {{{PonderNet}}},
  author = {Banino, Andrea and Balaguer, Jan and Blundell, Charles},
  year = {2021},
  month = sep,
  journal = {arXiv:2107.05407 [cs]},
  eprint = {2107.05407},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In standard neural networks the amount of computation used grows with the size of the inputs, but not with the complexity of the problem being learnt. To overcome this limitation we introduce PonderNet, a new algorithm that learns to adapt the amount of computation based on the complexity of the problem at hand. PonderNet learns end-to-end the number of computational steps to achieve an effective compromise between training prediction accuracy, computational cost and generalization. On a complex synthetic problem, PonderNet dramatically improves performance over previous adaptive computation methods and additionally succeeds at extrapolation tests where traditional neural networks fail. Also, our method matched the current state of the art results on a real world question and answering dataset, but using less compute. Finally, PonderNet reached state of the art results on a complex task designed to test the reasoning capabilities of neural networks.1},
  archiveprefix = {arXiv},
  annotation = {notion: https://www.notion.so/appliedaiinitiative/PonderNet-Learning-to-Ponder-8a6e9f639ff54c19ad8b3c4ecfb6cd9f post: https://community.appliedai.de/topics/27304/topic\_feed\_posts/1216268},
  file = {/Users/fariedabuzaid/Zotero/storage/W9X3W8LB/Banino et al. - 2021 - PonderNet Learning to Ponder.pdf}
}

@inproceedings{bao_analyticdpm_2022,
  title = {Analytic-{{DPM}}: An {{Analytic Estimate}} of the {{Optimal Reverse Variance}} in {{Diffusion Probabilistic Models}}},
  shorttitle = {Analytic-{{DPM}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR2022}})},
  author = {Bao, Fan and Li, Chongxuan and Zhu, Jun and Zhang, Bo},
  year = {2022},
  eprint = {2201.06503},
  eprinttype = {arxiv},
  address = {{|}},
  abstract = {Diffusion probabilistic models (DPMs) represent a class of powerful generative models. Despite their success, the inference of DPMs is expensive since it generally needs to iterate over thousands...},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {video:https://iclr.cc/virtual/2022/oral/7167},
  file = {/Users/fariedabuzaid/Zotero/storage/NADZ3KVY/Bao et al. - 2021 - Analytic-DPM an Analytic Estimate of the Optimal .pdf}
}

@article{bar-sinai_learning_2019,
  title = {Learning Data-Driven Discretizations for Partial Differential Equations},
  author = {{Bar-Sinai}, Yohai and Hoyer, Stephan and Hickey, Jason and Brenner, Michael P.},
  year = {2019},
  month = jul,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {31},
  eprint = {1808.04930},
  eprinttype = {arxiv},
  pages = {15344--15349},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1814058116},
  abstract = {The numerical solution of partial differential equations (PDEs) is challenging because of the need to resolve spatiotemporal features over wide length- and timescales. Often, it is computationally intractable to resolve the finest features in the solution. The only recourse is to use approximate coarse-grained representations, which aim to accurately represent long-wavelength dynamics while properly accounting for unresolved small-scale physics. Deriving such coarse-grained equations is notoriously difficult and often ad hoc. Here we introduce data-driven discretization, a method for learning optimized approximations to PDEs based on actual solutions to the known underlying equations. Our approach uses neural networks to estimate spatial derivatives, which are optimized end to end to best satisfy the equations on a low-resolution grid. The resulting numerical methods are remarkably accurate, allowing us to integrate in time a collection of nonlinear equations in 1 spatial dimension at resolutions 4\texttimes{} to 8\texttimes{} coarser than is possible with standard finite-difference methods.},
  archiveprefix = {arXiv},
  copyright = {Copyright \textcopyright{} 2019 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/97YCYZST/Bar-Sinai et al. - 2019 - Learning data-driven discretizations for partial d.pdf}
}

@inproceedings{basu_influence_2020,
  title = {Influence {{Functions}} in {{Deep Learning Are Fragile}}},
  booktitle = {Proceedings of the 9th {{International Conference}} on {{Learning Representations}}},
  author = {Basu, Samyadeep and Pope, Phil and Feizi, Soheil},
  year = {2020},
  month = sep,
  abstract = {Influence functions approximate the effect of training samples in test-time predictions and have a wide variety of applications in machine learning interpretability and uncertainty estimation. A...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/9TWFTUYJ/Basu et al. - 2020 - Influence Functions in Deep Learning Are Fragile.pdf;/Users/fariedabuzaid/Zotero/storage/MXIN9CRA/Influence Functions in Deep Learning Are Fragile - Appendix.pdf}
}

@inproceedings{basu_secondorder_2020,
  title = {On {{Second-Order Group Influence Functions}} for {{Black-Box Predictions}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Basu, Samyadeep and You, Xuchen and Feizi, Soheil},
  year = {2020},
  month = jul,
  volume = {119},
  eprint = {1911.00418},
  eprinttype = {arxiv},
  pages = {715:724},
  address = {{Vienna, Austria}},
  abstract = {With the rapid adoption of machine learning systems in sensitive applications, there is an increasing need to make black-box models explainable. Often we want to identify an influential group of training samples in a particular test prediction for a given machine learning model. Existing influence functions tackle this problem by using first-order approximations of the effect of removing a sample from the training set on model parameters. To compute the influence of a group of training samples (rather than an individual point) in model predictions, the change in optimal model parameters after removing that group from the training set can be large. Thus, in such cases, the first-order approximation can be loose. In this paper, we address this issue and propose second-order influence functions for identifying influential groups in test-time predictions. For linear models, across different sizes and types of groups, we show that using the proposed second-order influence function improves the correlation between the computed influence values and the ground truth ones. We also show that second-order influence functions could be used with optimization techniques to improve the selection of the most influential group for a test-sample.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/4X97N7FJ/Basu et al. - 2020 - On Second-Order Group Influence Functions for Blac.pdf}
}

@article{bates_crossvalidation_2021,
  title = {Cross-Validation: What Does It Estimate and How Well Does It Do It?},
  shorttitle = {Cross-Validation},
  author = {Bates, Stephen and Hastie, Trevor and Tibshirani, Robert},
  year = {2021},
  month = apr,
  eprint = {2104.00673v2},
  eprinttype = {arxiv},
  abstract = {Cross-validation is a widely-used technique to estimate prediction error, but its behavior is complex and not fully understood. Ideally, one would like to think that cross-validation estimates the prediction error for the model at hand, fit to the training data. We prove that this is not the case for the linear model fit by ordinary least squares; rather it estimates the average prediction error of models fit on other unseen training sets drawn from the same population. We further show that this phenomenon occurs for most popular estimates of prediction error, including data splitting, bootstrapping, and Mallow's Cp. Next, the standard confidence intervals for prediction error derived from cross-validation may have coverage far below the desired level. Because each data point is used for both training and testing, there are correlations among the measured accuracies for each fold, and so the usual estimate of variance is too small. We introduce a nested cross-validation scheme to estimate this variance more accurately, and show empirically that this modification leads to intervals with approximately correct coverage in many examples where traditional cross-validation intervals fail. Lastly, our analysis also shows that when producing confidence intervals for prediction accuracy with simple data splitting, one should not re-fit the model on the combined data, since this invalidates the confidence intervals.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/6DJT3HFI/KGX8234P.pdf}
}

@inproceedings{bengio_flow_2021,
  title = {Flow {{Network}} Based {{Generative Models}} for {{Non-Iterative Diverse Candidate Generation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Emmanuel and Jain, Moksh and Korablyov, Maksym and Precup, Doina and Bengio, Yoshua},
  year = {2021},
  volume = {34},
  pages = {27381--27394},
  publisher = {{Curran Associates, Inc.}},
  abstract = {This paper is about the problem of learning a stochastic policy for generating an object (like a molecular graph) from a sequence of actions, such that the probability of generating an object is proportional to a given positive reward for that object. Whereas standard return maximization tends to converge to a single return-maximizing sequence, there are cases where we would like to sample a diverse set of high-return solutions. These arise, for example, in black-box function optimization when few rounds are possible, each with large batches of queries, where the batches should be diverse, e.g., in the design of new molecules. One can also see this as a problem of approximately converting an energy function to a generative distribution. While MCMC methods can achieve that, they are expensive and generally only perform local exploration. Instead, training a generative policy amortizes the cost of search during training and yields to fast generation.  Using insights from Temporal Difference learning, we propose GFlowNet, based on a view of the generative process as a flow network, making it possible to handle the tricky case where different trajectories can yield the same final state, e.g., there are many ways to sequentially add atoms to generate some molecular graph. We cast the set of trajectories as a flow and convert the flow consistency equations into a learning objective, akin to the casting of the Bellman equations into Temporal Difference methods. We prove that any global minimum of the proposed objectives yields a policy which samples from the desired distribution, and demonstrate the improved performance and diversity of GFlowNet on a simple domain where there are many modes to the reward function, and on a molecule synthesis task.},
  file = {/Users/fariedabuzaid/Zotero/storage/89WVLQEJ/Bengio et al. - 2021 - Flow Network based Generative Models for Non-Itera.pdf}
}

@article{bengio_no_2004,
  title = {No {{Unbiased Estimator}} of the {{Variance}} of {{K-Fold Cross-Validation}}},
  author = {Bengio, Yoshua and Grandvalet, Yves},
  year = {2004},
  month = dec,
  journal = {The Journal of Machine Learning Research},
  volume = {5},
  pages = {1089--1105},
  issn = {1532-4435},
  doi = {10.5555/1005332.1044695},
  abstract = {Most machine learning researchers perform quantitative experiments to estimate generalization error and compare the performance of different algorithms (in particular, their proposed algorithm). In order to be able to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the very commonly used K-fold cross-validation estimator of generalization performance. The main theorem shows that there exists no universal (valid under all distributions) unbiased estimator of the variance of K-fold cross-validation. The analysis that accompanies this result is based on the eigen-decomposition of the covariance matrix of errors, which has only three different eigenvalues corresponding to three degrees of freedom of the matrix and three components of the total variance. This analysis helps to better understand the nature of the problem and how it can make naive estimators (that don't take into account the error correlations due to the overlap between training and test sets) grossly underestimate variance. This is confirmed by numerical experiments in which the three components of the variance are compared when the difficulty of the learning problem and the number of folds are varied.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/QVXH8EAS/2SGZSFMI.pdf}
}

@inproceedings{benton_calibration_2019,
  title = {Calibration for {{Anomaly Detection}}},
  booktitle = {25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}  - {{KDD}} '19: {{Workshop}} on {{Anomaly Detection}} in {{Finance}}},
  author = {Benton, Adrian},
  year = {2019},
  month = aug,
  address = {{Anchorage, Alaska}},
  abstract = {Recent work on model calibration found that a simple variant of Platt scaling, temperature scaling, is effective at calibrating modern neural networks across an array of classification tasks. However, when negative examples overwhelm the dataset, classifiers will often be biased to producing well-calibrated predictions for negative examples, but have trouble producing well-calibrated predictions for true anomalies. A well-calibrated model \textendash{} one whose scores accurately reflect the true probability of anomaly likelihood \textendash{} is an invaluable tool for decision makers.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/G67TNGSJ/Benton - Calibration for Anomaly Detection.pdf}
}

@article{berg_unified_2018,
  title = {A Unified Deep Artificial Neural Network Approach to Partial Differential Equations in Complex Geometries},
  author = {Berg, Jens and Nystr{\"o}m, Kaj},
  year = {2018},
  month = nov,
  journal = {Neurocomputing},
  volume = {317},
  pages = {28--41},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2018.06.056},
  abstract = {In this paper, we use deep feedforward artificial neural networks to approximate solutions to partial differential equations in complex geometries. We show how to modify the backpropagation algorithm to compute the partial derivatives of the network output with respect to the space variables which is needed to approximate the differential operator. The method is based on an ansatz for the solution which requires nothing but feedforward neural networks and an unconstrained gradient based optimization method such as gradient descent or a quasi-Newton method. We show an example where classical mesh based methods cannot be used and neural networks can be seen as an attractive alternative. Finally, we highlight the benefits of deep compared to shallow neural networks and device some other convergence enhancing techniques.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/U5R8284M/Berg and Nyström - 2018 - A unified deep artificial neural network approach .pdf}
}

@article{betancourt_conceptual_2018,
  title = {A {{Conceptual Introduction}} to {{Hamiltonian Monte Carlo}}},
  author = {Betancourt, Michael},
  year = {2018},
  month = jul,
  journal = {arXiv:1701.02434 [stat]},
  eprint = {1701.02434},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on difficult problems and how it is best applied in practice. Unfortunately, that understanding is confined within the mathematics of differential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important. In this review I provide a comprehensive conceptual account of these theoretical foundations, focusing on developing a principled intuition behind the method and its optimal implementations rather of any exhaustive rigor. Whether a practitioner or a statistician, the dedicated reader will acquire a solid grasp of how Hamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly, when it fails.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/FGUWKAYT/Betancourt - 2018 - A Conceptual Introduction to Hamiltonian Monte Car.pdf}
}

@inproceedings{bevilacqua_equivariant_2022,
  title = {Equivariant {{Subgraph Aggregation Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Bevilacqua, Beatrice and Frasca, Fabrizio and Lim, Derek and Srinivasan, Balasubramaniam and Cai, Chen and Balamurugan, Gopinath and Bronstein, Michael M. and Maron, Haggai},
  year = {2022},
  abstract = {Message-passing neural networks (MPNNs) are the leading architecture for deep learning on graph-structured data, in large part due to their simplicity and scalability. Unfortunately, it was shown...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/3WX9M4IW/Bevilacqua et al. - 2022 - Equivariant Subgraph Aggregation Networks.pdf}
}

@article{bhatt_cal_2021,
  title = {\$f\$-{{Cal}}: {{Calibrated}} Aleatoric Uncertainty Estimation from Neural Networks for Robot Perception},
  shorttitle = {\$f\$-{{Cal}}},
  author = {Bhatt, Dhaivat and Mani, Kaustubh and Bansal, Dishank and Murthy, Krishna and Lee, Hanju and Paull, Liam},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.13913 [cs]},
  eprint = {2109.13913},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {While modern deep neural networks are performant perception modules, performance (accuracy) alone is insufficient, particularly for safety-critical robotic applications such as self-driving vehicles. Robot autonomy stacks also require these otherwise blackbox models to produce reliable and calibrated measures of confidence on their predictions. Existing approaches estimate uncertainty from these neural network perception stacks by modifying network architectures, inference procedure, or loss functions. However, in general, these methods lack calibration, meaning that the predictive uncertainties do not faithfully represent the true underlying uncertainties (process noise). Our key insight is that calibration is only achieved by imposing constraints across multiple examples, such as those in a mini-batch; as opposed to existing approaches which only impose constraints per-sample, often leading to overconfident (thus miscalibrated) uncertainty estimates. By enforcing the distribution of outputs of a neural network to resemble a target distribution by minimizing an \$f\$-divergence, we obtain significantly better-calibrated models compared to prior approaches. Our approach, \$f\$-Cal, outperforms existing uncertainty calibration approaches on robot perception tasks such as object detection and monocular depth estimation over multiple real-world benchmarks.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/5YLQ2MLA/Bhatt et al. - 2021 - $f$-Cal Calibrated aleatoric uncertainty estimati.pdf}
}

@inproceedings{bian_energybased_2022,
  title = {Energy-{{Based Learning}} for {{Cooperative Games}}, with {{Applications}} to {{Valuation Problems}} in {{Machine Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Bian, Yatao and Rong, Yu and Xu, Tingyang and Wu, Jiaxiang and Krause, Andreas and Huang, Junzhou},
  year = {2022},
  abstract = {Valuation problems, such as feature interpretation, data valuation and model valuation for ensembles, become increasingly more important in many machine learning applications. Such problems are...},
  langid = {english},
  annotation = {video: https://iclr.cc/virtual/2022/poster/6807},
  file = {/Users/fariedabuzaid/Zotero/storage/YZUM4MQP/Bian et al. - 2021 - Energy-Based Learning for Cooperative Games, with .pdf}
}

@article{bishop_novelty_1994,
  title = {Novelty Detection and Neural Network Validation},
  author = {Bishop, C. M.},
  year = {1994},
  month = aug,
  journal = {IEE Proceedings - Vision, Image and Signal Processing},
  volume = {141},
  number = {4},
  pages = {217--222},
  publisher = {{IET Digital Library}},
  issn = {1359-7108},
  doi = {10.1049/ip-vis:19941330},
  abstract = {One of the key factors which limits the use of neural networks in many industrial applications has been the difficulty of demonstrating that a trained network will continue to generate reliable outputs once it is in routine use. An important potential source of errors is novel input data; that is, input data which differ significantly from the data used to train the network. The author investigates the relationship between the degree of novelty of input data and the corresponding reliability of the outputs from the network. He describes a quantitative procedure for assessing novelty, and demonstrates its performance by using an application which involves monitoring oil flow in multiphase pipelines.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/6XDXKHR3/ip-vis_19941330.html}
}

@book{bishop_pattern_2006,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  month = aug,
  series = {Information Science and Statistics},
  edition = {First},
  publisher = {{Springer}},
  abstract = {This is the first textbook on pattern recognition to present the Bayesian viewpoint. The book presents approximate inference algorithms that permit fast approximate answers in situations where exact answers are not feasible. It uses graphical models to describe probability distributions when no other books apply graphical models to machine learning. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory.},
  isbn = {978-0-387-31073-2},
  langid = {english},
  annotation = {citecount: 00184},
  file = {/Users/fariedabuzaid/Zotero/storage/BTRM9WTQ/Bishop - 2006 - Pattern recognition and machine learning.pdf;/Users/fariedabuzaid/Zotero/storage/MLRXCPTQ/prml-web-sol-2009-09-08.pdf;/Users/fariedabuzaid/Zotero/storage/ZZELHRIX/prml-errata-3rd-20110921.pdf}
}

@article{blechschmidt_three_2021,
  title = {Three {{Ways}} to {{Solve Partial Differential Equations}} with {{Neural Networks}} -- {{A Review}}},
  author = {Blechschmidt, Jan and Ernst, Oliver G.},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.11802 [cs, math]},
  eprint = {2102.11802},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {Neural networks are increasingly used to construct numerical solution methods for partial differential equations. In this expository review, we introduce and contrast three important recent approaches attractive in their simplicity and their suitability for high-dimensional problems: physics-informed neural networks, methods based on the Feynman-Kac formula and the Deep BSDE solver. The article is accompanied by a suite of expository software in the form of Jupyter notebooks in which each basic methodology is explained step by step, allowing for a quick assimilation and experimentation. An extensive bibliography summarizes the state of the art.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/XJ64HHI8/Blechschmidt and Ernst - 2021 - Three Ways to Solve Partial Differential Equations.pdf}
}

@article{blei_variational_2017,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  eprint = {1601.00670},
  eprinttype = {arxiv},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {citecount: 00205},
  file = {/Users/fariedabuzaid/Zotero/storage/HLAN47XE/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf}
}

@book{blum_foundations_2020,
  title = {Foundations of {{Data Science}}},
  author = {Blum, Avrim and Hopcroft, John and Kannan, Ravindran},
  year = {2020},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/9781108755528},
  abstract = {This book provides an introduction to the mathematical and algorithmic foundations of data science, including machine learning, high-dimensional geometry, and analysis of large networks. Topics include the counterintuitive nature of data in high dimensions, important linear algebraic techniques such as singular value decomposition, the theory of random walks and Markov chains, the fundamentals of and important algorithms for machine learning, algorithms and analysis for clustering, probabilistic models for large networks, representation learning including topic modelling and non-negative matrix factorization, wavelets and compressed sensing. Important probabilistic techniques are developed including the law of large numbers, tail inequalities, analysis of random projections, generalization guarantees in machine learning, and moment methods for analysis of phase transitions in large random graphs. Additionally, important structural and complexity measures are discussed such as matrix norms and VC-dimension. This book is suitable for both undergraduate and graduate courses in the design and analysis of algorithms for data.},
  isbn = {978-1-108-48506-7},
  file = {/Users/fariedabuzaid/Zotero/storage/WU5PNRLH/6A43CE830DE83BED6CC5171E62B0AA9E.html}
}

@article{bodria_benchmarking_2021,
  title = {Benchmarking and {{Survey}} of {{Explanation Methods}} for {{Black Box Models}}},
  author = {Bodria, Francesco and Giannotti, Fosca and Guidotti, Riccardo and Naretto, Francesca and Pedreschi, Dino and Rinzivillo, Salvatore},
  year = {2021},
  month = feb,
  abstract = {The widespread adoption of black-box models in Artificial Intelligence has enhanced the need for explanation methods to reveal how these obscure models reach specific decisions. Retrieving explanations is fundamental to unveil possible biases and to resolve practical or ethical issues. Nowadays, the literature is full of methods with different explanations. We provide a categorization of explanation methods based on the type of explanation returned. We present the most recent and widely used explainers, and we show a visual comparison among explanations and a quantitative benchmarking.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/7N8S537A/Bodria et al. - 2021 - Benchmarking and Survey of Explanation Methods for.pdf}
}

@inproceedings{bohdal_metacalibration_2021,
  title = {Meta-{{Calibration}}: {{Meta-Learning}} of {{Model Calibration Using Differentiable Expected Calibration Error}}},
  shorttitle = {Meta-{{Calibration}}},
  booktitle = {{{arXiv}}:2106.09613 [Cs, Stat]},
  author = {Bohdal, Ondrej and Yang, Yongxin and Hospedales, Timothy},
  year = {2021},
  month = jun,
  eprint = {2106.09613},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Calibration of neural networks is a topical problem that is becoming increasingly important for real-world use of neural networks. The problem is especially noticeable when using modern neural networks, for which there is significant difference between the model confidence and the confidence it should have. Various strategies have been successfully proposed, yet there is more space for improvements. We propose a novel approach that introduces a differentiable metric for expected calibration error and successfully uses it as an objective for meta-learning, achieving competitive results with state-of-the-art approaches. Our approach presents a new direction of using meta-learning to directly optimize model calibration, which we believe will inspire further work in this promising and new direction.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/LL7G3KYD/Bohdal et al. - 2021 - Meta-Calibration Meta-Learning of Model Calibrati.pdf}
}

@article{bottou_counterfactual_2013,
  title = {Counterfactual {{Reasoning}} and {{Learning Systems}}: {{The Example}} of {{Computational Advertising}}},
  shorttitle = {Counterfactual {{Reasoning}} and {{Learning Systems}}},
  author = {Bottou, L{\'e}on and Peters, Jonas and {Qui{\~n}onero-Candela}, Joaquin and Charles, Denis X. and Chickering, D. Max and Portugaly, Elon and Ray, Dipankar and Simard, Patrice and Snelson, Ed},
  year = {2013},
  journal = {Journal of Machine Learning Research},
  volume = {14},
  number = {65},
  pages = {3207--3260},
  issn = {1533-7928},
  abstract = {This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine.},
  file = {/Users/fariedabuzaid/Zotero/storage/XVTCMUT9/Bottou et al. - 2013 - Counterfactual Reasoning and Learning Systems The.pdf}
}

@article{brach_single_2020,
  title = {Single {{Shot MC Dropout Approximation}}},
  author = {Brach, Kai and Sick, Beate and D{\"u}rr, Oliver},
  year = {2020},
  month = jul,
  eprint = {2007.03293},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2007.03293},
  abstract = {Deep neural networks (DNNs) are known for their high prediction performance, especially in perceptual tasks such as object recognition or autonomous driving. Still, DNNs are prone to yield unreliable predictions when encountering completely new situations without indicating their uncertainty. Bayesian variants of DNNs (BDNNs), such as MC dropout BDNNs, do provide uncertainty measures. However, BDNNs are slow during test time because they rely on a sampling approach. Here we present a single shot MC dropout approximation that preserves the advantages of BDNNs without being slower than a DNN. Our approach is to analytically approximate for each layer in a fully connected network the expected value and the variance of the MC dropout signal. We evaluate our approach on different benchmark datasets and a simulated toy example. We demonstrate that our single shot MC dropout approximation resembles the point estimate and the uncertainty estimate of the predictive distribution that is achieved with an MC approach, while being fast enough for real-time deployments of BDNNs.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/SIITUGDG/Brach et al. - 2020 - Single Shot MC Dropout Approximation.pdf}
}

@inproceedings{brandstetter_message_2022,
  title = {Message {{Passing Neural PDE Solvers}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Brandstetter, Johannes and Worrall, Daniel E. and Welling, Max},
  year = {2022},
  abstract = {The numerical solution of partial differential equations (PDEs) is difficult, having led to a century of research so far. Recently, there have been pushes to build neural--numerical hybrid solvers,...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/B2EKTEJN/Brandstetter et al. - 2022 - Message Passing Neural PDE Solvers.pdf}
}

@article{breiman_heuristics_1996,
  title = {Heuristics of Instability and Stabilization in Model Selection},
  author = {Breiman, Leo},
  year = {1996},
  month = dec,
  journal = {The Annals of Statistics},
  volume = {24},
  number = {6},
  pages = {2350--2383},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1032181158},
  abstract = {In model selection, usually a "best" predictor is chosen from a collection \$\{\textbackslash hat\{\textbackslash mu\}(\textbackslash cdot, s)\}\$ of predictors where \$\textbackslash hat\{\textbackslash mu\}(\textbackslash cdot, s)\$ is the minimum least-squares predictor in a collection \$\textbackslash mathsf\{U\}\_s\$ of predictors. Here s is a complexity parameter; that is, the smaller s, the lower dimensional/smoother the models in \$\textbackslash mathsf\{U\}\_s\$. If \$\textbackslash mathsf\{L\}\$ is the data used to derive the sequence \$\{\textbackslash hat\{\textbackslash mu\}(\textbackslash cdot, s)\}\$, the procedure is called unstable if a small change in \$\textbackslash mathsf\{L\}\$ can cause large changes in \$\{\textbackslash hat\{\textbackslash mu\}(\textbackslash cdot, s)\}\$. With a crystal ball, one could pick the predictor in \$\{\textbackslash hat\{\textbackslash mu\}(\textbackslash cdot, s)\}\$ having minimum prediction error. Without prescience, one uses test sets, cross-validation and so forth. The difference in prediction error between the crystal ball selection and the statistician's choice we call predictive loss. For an unstable procedure the predictive loss is large. This is shown by some analytics in a simple case and by simulation results in a more complex comparison of four different linear regression methods. Unstable procedures can be stabilized by perturbing the data, getting a new predictor sequence \$\{\textbackslash hat\{\textbackslash mu'\}(\textbackslash cdot, s)\}\$ and then averaging over many such predictor sequences.},
  file = {/Users/fariedabuzaid/Zotero/storage/XGA32PRG/Breiman - 1996 - Heuristics of instability and stabilization in mod.pdf}
}

@article{breunig_lof_2000,
  title = {{{LOF}}: Identifying Density-Based Local Outliers},
  shorttitle = {{{LOF}}},
  author = {Breunig, Markus M. and Kriegel, Hans-Peter and Ng, Raymond T. and Sander, J{\"o}rg},
  year = {2000},
  month = may,
  journal = {ACM SIGMOD Record},
  volume = {29},
  number = {2},
  pages = {93--104},
  issn = {0163-5808},
  doi = {10.1145/335191.335388},
  abstract = {For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using real-world datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.},
  file = {/Users/fariedabuzaid/Zotero/storage/BN8DBWR6/Breunig et al. - 2000 - LOF identifying density-based local outliers.pdf}
}

@article{brocker_increasing_2007,
  title = {Increasing the {{Reliability}} of {{Reliability Diagrams}}},
  author = {Br{\"o}cker, Jochen and Smith, Leonard A.},
  year = {2007},
  month = jun,
  journal = {Weather and Forecasting},
  volume = {22},
  number = {3},
  pages = {651--661},
  issn = {1520-0434, 0882-8156},
  doi = {10.1175/WAF993.1},
  abstract = {Abstract             The reliability diagram is a common diagnostic graph used to summarize and evaluate probabilistic forecasts. Its strengths lie in the ease with which it is produced and the transparency of its definition. While visually appealing, major long-noted shortcomings lie in the difficulty of interpreting the graph visually; for the most part, ambiguities arise from variations in the distributions of forecast probabilities and from various binning procedures. A resampling method for assigning consistency bars to the observed frequencies is introduced that allows for immediate visual evaluation as to just how likely the observed relative frequencies are under the assumption that the predicted probabilities are reliable. Further, an alternative presentation of the same information on probability paper eases quantitative evaluation and comparison. Both presentations can easily be employed for any method of binning.},
  langid = {english}
}

@article{broderick_automatic_2021,
  title = {An {{Automatic Finite-Sample Robustness Metric}}: {{When Can Dropping}} a {{Little Data Make}} a {{Big Difference}}?},
  shorttitle = {An {{Automatic Finite-Sample Robustness Metric}}},
  author = {Broderick, Tamara and Giordano, Ryan and Meager, Rachael},
  year = {2021},
  month = nov,
  eprint = {2011.14999},
  eprinttype = {arxiv},
  abstract = {We propose a method to assess the sensitivity of econometric analyses to the removal of a small fraction of the data. Manually checking the influence of all possible small subsets is computationally infeasible, so we provide an approximation to find the most influential subset. Our metric, the "Approximate Maximum Influence Perturbation," is automatically computable for common methods including (but not limited to) OLS, IV, MLE, GMM, and variational Bayes. We provide finite-sample error bounds on approximation performance. At minimal extra cost, we provide an exact finite-sample lower bound on sensitivity. We find that sensitivity is driven by a signal-to-noise ratio in the inference problem, is not reflected in standard errors, does not disappear asymptotically, and is not due to misspecification. While some empirical applications are robust, results of several economics papers can be overturned by removing less than 1\% of the sample.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/A4U5GT7U/Broderick et al. - 2021 - An Automatic Finite-Sample Robustness Metric When.pdf;/Users/fariedabuzaid/Zotero/storage/BM97YF6E/DAQAKJLT.pdf}
}

@inproceedings{brody_how_2022,
  title = {How {{Attentive}} Are {{Graph Attention Networks}}?},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Brody, Shaked and Alon, Uri and Yahav, Eran},
  year = {2022},
  abstract = {Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/7G3UL44H/Brody et al. - 2022 - How Attentive are Graph Attention Networks.pdf}
}

@article{brunton_discovering_2016,
  title = {Discovering Governing Equations from Data by Sparse Identification of Nonlinear Dynamical Systems},
  author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
  year = {2016},
  month = apr,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {113},
  number = {15},
  pages = {3932--3937},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1517384113},
  abstract = {Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
  chapter = {Physical Sciences},
  copyright = {\textcopyright{}  . Freely available online through the PNAS open access option.},
  langid = {english},
  pmid = {27035946},
  file = {/Users/fariedabuzaid/Zotero/storage/VUSE8FKL/Brunton et al. - 2016 - Discovering governing equations from data by spars.pdf}
}

@inproceedings{cao_relational_2022,
  title = {Relational {{Multi-Task Learning}}: {{Modeling Relations}} between {{Data}} and {{Tasks}}},
  shorttitle = {Relational {{Multi-Task Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Cao, Kaidi and You, Jiaxuan and Leskovec, Jure},
  year = {2022},
  abstract = {A key assumption in multi-task learning is that at the inference time the multi-task model only has access to a given data point but not to the data point's labels from other tasks. This presents...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/H9DZMHW7/Cao et al. - 2022 - Relational Multi-Task Learning Modeling Relations.pdf}
}

@inproceedings{carmon_unlabeled_2019,
  title = {Unlabeled {{Data Improves Adversarial Robustness}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Carmon, Yair and Raghunathan, Aditi and Schmidt, Ludwig and Duchi, John C and Liang, Percy S},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We demonstrate, theoretically and empirically, that adversarial robustness can significantly benefit from semisupervised learning. Theoretically, we revisit the simple Gaussian model of Schmidt et al. that shows a sample complexity gap between standard and robust classification. We prove that unlabeled data bridges this gap: a simple semisupervised learning procedure (self-training) achieves high robust accuracy using the same number of labels required for achieving high standard accuracy. Empirically, we augment CIFAR-10 with 500K unlabeled images sourced from 80 Million Tiny Images and use robust self-training to outperform state-of-the-art robust accuracies by over 5 points in (i) \$\textbackslash ell\_\textbackslash infty\$ robustness against several strong attacks via adversarial training and (ii) certified \$\textbackslash ell\_2\$ and \$\textbackslash ell\_\textbackslash infty\$ robustness via randomized smoothing. On SVHN, adding the dataset's own extra training set with the labels removed provides gains of 4 to 10 points, within 1 point of the gain from using the extra labels.},
  keywords = {notion},
  file = {/Users/fariedabuzaid/Zotero/storage/UGBXMPBT/Carmon et al. - 2019 - Unlabeled Data Improves Adversarial Robustness.pdf}
}

@article{castillo_fitting_1997,
  title = {Fitting the {{Generalized Pareto Distribution}} to {{Data}}},
  author = {Castillo, Enrique and Hadi, Ali S.},
  year = {1997},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {92},
  number = {440},
  pages = {1609--1620},
  issn = {0162-1459},
  doi = {10.1080/01621459.1997.10473683},
  abstract = {The generalized Pareto distribution (GPD) was introduced by Pickands to model exceedances over a threshold. It has since been used by many authors to model data in several fields. The GPD has a scale parameter ([sgrave] {$>$} 0) and a shape parameter (-{$\infty$} {$<$} k {$<$} {$\infty$}). The estimation of these parameters is not generally an easy problem. When k {$>$} 1, the maximum likelihood estimates do not exist, and when k is between 1/2 and 1, they may have problems. Furthermore, for k {$\leq$} -1/2, second and higher moments do not exist, and hence both the method-of-moments (MOM) and the probability-weighted moments (PWM) estimates do not exist. Another and perhaps more serious problem with the MOM and PWM methods is that they can produce nonsensical estimates (i.e., estimates inconsistent with the observed data). In this article we propose a method for estimating the parameters and quantiles of the GPD. The estimators are well defined for all parameter values. They are also easy to compute. Some asymptotic results are provided. A simulation study is carried out to evaluate the performance of the proposed methods and to compare them with other methods suggested in the literature. The simulation results indicate that although no method is uniformly best for all the parameter values, the proposed method performs well compared to existing methods. The methods are applied to real-life data. Specific recommendations are also given.}
}

@article{cawley_overfitting_2010,
  title = {On {{Over-fitting}} in {{Model Selection}} and {{Subsequent Selection Bias}} in {{Performance Evaluation}}},
  author = {Cawley, Gavin C. and Talbot, Nicola L. C.},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {Jul},
  pages = {2079--2107},
  issn = {ISSN 1533-7928},
  annotation = {citecount: 00341},
  file = {/Users/fariedabuzaid/Zotero/storage/DXN8ANRM/Cawley and Talbot - 2010 - On Over-fitting in Model Selection and Subsequent .pdf}
}

@article{chalapathy_deep_2019,
  title = {Deep {{Learning}} for {{Anomaly Detection}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} for {{Anomaly Detection}}},
  author = {Chalapathy, Raghavendra and Chawla, Sanjay},
  year = {2019},
  month = jan,
  journal = {arXiv:1901.03407 [cs, stat]},
  eprint = {1901.03407},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Anomaly detection is an important problem that has been well-studied within diverse research areas and application domains. The aim of this survey is two-fold, firstly we present a structured and comprehensive overview of research methods in deep learning-based anomaly detection. Furthermore, we review the adoption of these methods for anomaly across various application domains and assess their effectiveness. We have grouped state-of-the-art research techniques into different categories based on the underlying assumptions and approach adopted. Within each category we outline the basic anomaly detection technique, along with its variants and present key assumptions, to differentiate between normal and anomalous behavior. For each category, we present we also present the advantages and limitations and discuss the computational complexity of the techniques in real application domains. Finally, we outline open issues in research and challenges faced while adopting these techniques.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/297DFHH6/Chalapathy and Chawla - 2019 - Deep Learning for Anomaly Detection A Survey.pdf}
}

@article{chandola_anomaly_2009,
  title = {Anomaly Detection: {{A}} Survey},
  shorttitle = {Anomaly Detection},
  author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  year = {2009},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {41},
  number = {3},
  pages = {15:1--15:58},
  issn = {0360-0300},
  doi = {10.1145/1541880.1541882},
  abstract = {Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.},
  file = {/Users/fariedabuzaid/Zotero/storage/A3JTTUDX/Chandola et al. - 2009 - Anomaly detection A survey.pdf}
}

@inproceedings{charpentier_natural_2022,
  title = {Natural {{Posterior Network}}: {{Deep Bayesian Predictive Uncertainty}} for {{Exponential Family Distributions}}},
  shorttitle = {Natural {{Posterior Network}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Charpentier, Bertrand and Borchert, Oliver and Z{\"u}gner, Daniel and Geisler, Simon and G{\"u}nnemann, Stephan},
  year = {2022},
  abstract = {Uncertainty awareness is crucial to develop reliable machine learning models. In this work, we propose the Natural Posterior Network (NatPN) for fast and high-quality uncertainty estimation for any...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/EJ3WJVXK/Charpentier et al. - 2022 - Natural Posterior Network Deep Bayesian Predictiv.pdf}
}

@article{chen_causal_2019,
  title = {On Causal Discovery with an Equal-Variance Assumption},
  author = {Chen, Wenyu and Drton, Mathias and Wang, Y. Samuel},
  year = {2019},
  month = dec,
  journal = {Biometrika},
  volume = {106},
  number = {4},
  pages = {973--980},
  publisher = {{Oxford Academic}},
  issn = {0006-3444},
  doi = {10.1093/biomet/asz049},
  abstract = {Summary.  Prior work has shown that causal structure can be uniquely identified from observational data when these follow a structural equation model whose erro},
  langid = {english},
  annotation = {citecount: 00002},
  file = {/Users/fariedabuzaid/Zotero/storage/Z3GGMRSP/Chen et al. - 2019 - On causal discovery with an equal-variance assumpt.pdf}
}

@inproceedings{chen_does_2022,
  title = {Does Your Graph Need a Confidence Boost? {{Convergent}} Boosted Smoothing on Graphs with Tabular Node Features},
  shorttitle = {Does Your Graph Need a Confidence Boost?},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Chen, Jiuhai and Mueller, Jonas and Ioannidis, Vassilis N. and Adeshina, Soji and Wang, Yangkun and Goldstein, Tom and Wipf, David},
  year = {2022},
  abstract = {Many practical modeling tasks require making predictions using tabular data composed of heterogeneous feature types (e.g., text-based, categorical, continuous, etc.).  In this setting boosted...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/3UBQIY3X/Chen et al. - 2022 - Does your graph need a confidence boost Convergen.pdf}
}

@article{chen_hydra_2021,
  title = {{{HYDRA}}: {{Hypergradient Data Relevance Analysis}} for {{Interpreting Deep Neural Networks}}},
  shorttitle = {{{HYDRA}}},
  author = {Chen, Yuanyuan and Li, Boyang and Yu, Han and Wu, Pengcheng and Miao, Chunyan},
  year = {2021},
  month = mar,
  journal = {arXiv:2102.02515 [cs]},
  eprint = {2102.02515},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The behaviors of deep neural networks (DNNs) are notoriously resistant to human interpretations. In this paper, we propose Hypergradient Data Relevance Analysis, or HYDRA, which interprets the predictions made by DNNs as effects of their training data. Existing approaches generally estimate data contributions around the final model parameters and ignore how the training data shape the optimization trajectory. By unrolling the hypergradient of test loss w.r.t. the weights of training data, HYDRA assesses the contribution of training data toward test data points throughout the training trajectory. In order to accelerate computation, we remove the Hessian from the calculation and prove that, under moderate conditions, the approximation error is bounded. Corroborating this theoretical claim, empirical results indicate the error is indeed small. In addition, we quantitatively demonstrate that HYDRA outperforms influence functions in accurately estimating data contribution and detecting noisy data labels. The source code is available at https://github.com/cyyever/aaai\_hydra\_8686.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/DRYFU363/Chen et al. - 2021 - HYDRA Hypergradient Data Relevance Analysis for I.pdf}
}

@inproceedings{chen_more_2020,
  title = {More {{Data Can Expand The Generalization Gap Between Adversarially Robust}} and {{Standard Models}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Chen, Lin and Min, Yifei and Zhang, Mingrui and Karbasi, Amin},
  year = {2020},
  month = nov,
  eprint = {2002.04725},
  eprinttype = {arxiv},
  pages = {1670--1680},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Despite remarkable success in practice, modern machine learning models have been found to be susceptible to adversarial attacks that make human-imperceptible perturbations to the data, but result in serious and potentially dangerous prediction errors. To address this issue, practitioners often use adversarial training to learn models that are robust against such attacks at the cost of higher generalization error on unperturbed test sets. The conventional wisdom is that more training data should shrink the gap between the generalization error of adversarially-trained models and standard models. However, we study the training of robust classifiers for both Gaussian and Bernoulli models under \$\textbackslash ell\_\textbackslash infty\$ attacks, and we prove that more data may actually increase this gap. Furthermore, our theoretical results identify if and when additional data will finally begin to shrink the gap. Lastly, we experimentally demonstrate that our results also hold for linear regression models, which may indicate that this phenomenon occurs more broadly.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/MEC2LFNL/Chen et al. - 2020 - More Data Can Expand The Generalization Gap Betwee.pdf}
}

@inproceedings{chen_tamps2gcnets_2022,
  title = {{{TAMP-S2GCNets}}: {{Coupling Time-Aware Multipersistence Knowledge Representation}} with {{Spatio-Supra Graph Convolutional Networks}} for {{Time-Series Forecasting}}},
  shorttitle = {{{TAMP-S2GCNets}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Chen, Yuzhou and {Segovia-Dominguez}, Ignacio and Coskunuzer, Baris and Gel, Yulia},
  year = {2022},
  abstract = {Graph Neural Networks (GNNs) are proven to be a powerful machinery for learning complex dependencies in multivariate spatio-temporal processes. However, most existing GNNs have inherently static...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/YCJP2YWT/Chen et al. - 2021 - TAMP-S2GCNets Coupling Time-Aware Multipersistenc.pdf;/Users/fariedabuzaid/Zotero/storage/RPGK7SH9/forum.html}
}

@inproceedings{chen_understanding_2022,
  title = {Understanding and {{Improving Graph Injection Attack}} by {{Promoting Unnoticeability}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Chen, Yongqiang and Yang, Han and Zhang, Yonggang and Kaili, Ma and Liu, Tongliang and Han, Bo and Cheng, James},
  year = {2022},
  abstract = {Recently Graph Injection Attack (GIA) emerges as a practical attack scenario on Graph Neural Networks (GNNs), where the adversary can merely inject few malicious nodes instead of modifying existing...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/MAU4LF2J/Chen et al. - 2021 - Understanding and Improving Graph Injection Attack.pdf;/Users/fariedabuzaid/Zotero/storage/46FUYQJK/forum.html}
}

@inproceedings{chen_unrestricted_2021,
  title = {Unrestricted {{Adversarial Attacks}} on {{ImageNet Competition}}},
  booktitle = {{{CVPR-2021}}},
  author = {Chen, Yuefeng and Mao, Xiaofeng and He, Yuan and Xue, Hui and Li, Chao and Dong, Yinpeng and Fu, Qi-An and Yang, Xiao and Xiang, Wenzhao and Pang, Tianyu and Su, Hang and Zhu, Jun and Liu, Fangcheng and Zhang, Chao and Zhang, Hongyang and Zhang, Yichi and Liu, Shilong and Liu, Chang and Xiang, Wenzhao and Wang, Yajie and Zhou, Huipeng and Lyu, Haoran and Xu, Yidan and Xu, Zixuan and Zhu, Taoyu and Li, Wenjun and Gao, Xianfeng and Wang, Guoqiu and Yan, Huanqian and Guo, Ying and Zhang, Chaoning and Fang, Zheng and Wang, Yang and Fu, Bingyang and Zheng, Yunfei and Wang, Yekui and Luo, Haorong and Yang, Zhen},
  year = {2021},
  month = oct,
  eprint = {2110.09903},
  eprinttype = {arxiv},
  abstract = {Many works have investigated the adversarial attacks or defenses under the settings where a bounded and imperceptible perturbation can be added to the input. However in the real-world, the attacker does not need to comply with this restriction. In fact, more threats to the deep model come from unrestricted adversarial examples, that is, the attacker makes large and visible modifications on the image, which causes the model classifying mistakenly, but does not affect the normal observation in human perspective. Unrestricted adversarial attack is a popular and practical direction but has not been studied thoroughly. We organize this competition with the purpose of exploring more effective unrestricted adversarial attack algorithm, so as to accelerate the academical research on the model robustness under stronger unbounded attacks. The competition is held on the TianChi platform (\textbackslash url\{https://tianchi.aliyun.com/competition/entrance/531853/introduction\}) as one of the series of AI Security Challengers Program.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/NJ84SV8L/Chen et al. - 2021 - Unrestricted Adversarial Attacks on ImageNet Compe.pdf}
}

@article{cheridito_secondorder_2007,
  title = {Second-Order Backward Stochastic Differential Equations and Fully Nonlinear Parabolic {{PDEs}}},
  author = {Cheridito, Patrick and Soner, H. Mete and Touzi, Nizar and Victoir, Nicolas},
  year = {2007},
  journal = {Communications on Pure and Applied Mathematics},
  volume = {60},
  number = {7},
  pages = {1081--1110},
  issn = {1097-0312},
  doi = {10.1002/cpa.20168},
  abstract = {For a d-dimensional diffusion of the form dXt = {$\mu$}(Xt)dt + {$\sigma$}(Xt)dWt and continuous functions f and g, we study the existence and uniqueness of adapted processes Y, Z, {$\Gamma$}, and A solving the second-order backward stochastic differential equation (2BSDE) \$\$dY\_t = f(t,X\_t, Y\_t, Z\_t, \textbackslash Gamma\_t) dt + Z\_t'\textbackslash circ dX\_t, \textbackslash quad t \i n [0,T),\$\$ \$\$dZ\_t = A\_t dt + \textbackslash Gamma\_tdX\_t, \textbackslash quad t \i n [0,T),\$\$ \$\$Y\_T = g(X\_T).\$\$ If the associated PDE \$\$- v\_t(t,x) + f(t,x,v(t,x), Dv(t,x), D\^2v(t,x)) = 0,\$\$ \$\$(t,x) \i n [0,T) \textbackslash times \textbackslash cal R\^d, \textbackslash quad v(T,x) = g(x),\$\$ has a sufficiently regular solution, then it follows directly from It\^o's formula that the processes \$\$v(t,X\_t), Dv(t,X\_t), D\^2v(t,X\_t), \textbackslash cal L Dv(t,X\_t), \textbackslash quad t \i n [0,T],\$\$ solve the 2BSDE, where {$\mathscr{l}$} is the Dynkin operator of X without the drift term. The main result of the paper shows that if f is Lipschitz in Y as well as decreasing in {$\Gamma$} and the PDE satisfies a comparison principle as in the theory of viscosity solutions, then the existence of a solution (Y, Z,{$\Gamma$}, A) to the 2BSDE implies that the associated PDE has a unique continuous viscosity solution v and the process Y is of the form Yt = v(t, Xt), t {$\in$} [0, T]. In particular, the 2BSDE has at most one solution. This provides a stochastic representation for solutions of fully nonlinear parabolic PDEs. As a consequence, the numerical treatment of such PDEs can now be approached by Monte Carlo methods. \textcopyright{} 2006 Wiley Periodicals, Inc.},
  copyright = {Copyright \textcopyright{} 2006 Wiley Periodicals, Inc.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.20168},
  file = {/Users/fariedabuzaid/Zotero/storage/EPLE6XMZ/Cheridito et al. - 2007 - Second-order backward stochastic differential equa.pdf}
}

@article{chickering_optimal_2002,
  title = {Optimal {{Structure Identification With Greedy Search}}},
  author = {Chickering, David Maxwell},
  year = {2002},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  number = {Nov},
  pages = {507--554},
  issn = {ISSN 1533-7928},
  abstract = {In this paper we prove the so-called "Meek Conjecture". In particular, we show that if a DAG H is an independence map of another DAG G, then there exists a finite sequence of edge additions and covered edge reversals in G such that (1) after each edge modification H remains an independence map of G and (2) after all modifications G =H. As shown by Meek (1997), this result has an important consequence for Bayesian approaches to learning Bayesian networks from data: in the limit of large sample size, there exists a two-phase greedy search algorithm that---when applied to a particular sparsely-connected search space---provably identifies a perfect map of the generative distribution if that perfect map is a DAG. We provide a new implementation of the search space, using equivalence classes as states, for which all operators used in the greedy search can be scored efficiently using local functions of the nodes in the domain. Finally, using both synthetic and real-world datasets, we demonstrate that the two-phase greedy approach leads to good solutions when learning with finite sample sizes.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/EB59B2ZV/Chickering - Erratum Optimal Structure Identiﬁcation With Gree.pdf;/Users/fariedabuzaid/Zotero/storage/XGHCMFM6/Chickering - 2002 - Optimal Structure Identification With Greedy Searc.pdf}
}

@inproceedings{chien_node_2022,
  title = {Node {{Feature Extraction}} by {{Self-Supervised Multi-scale Neighborhood Prediction}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Chien, Eli and Chang, Wei-Cheng and Hsieh, Cho-Jui and Yu, Hsiang-Fu and Zhang, Jiong and Milenkovic, Olgica and Dhillon, Inderjit S.},
  year = {2022},
  abstract = {Learning on graphs has attracted significant attention in the learning community due to numerous real-world applications. In particular, graph neural networks (GNNs), which take \textbackslash emph\{numerical\}...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/37PLN8NL/Chien et al. - 2022 - Node Feature Extraction by Self-Supervised Multi-s.pdf}
}

@inproceedings{chien_you_2022,
  title = {You Are {{AllSet}}: {{A Multiset Function Framework}} for {{Hypergraph Neural Networks}}},
  shorttitle = {You Are {{AllSet}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Chien, Eli and Pan, Chao and Peng, Jianhao and Milenkovic, Olgica},
  year = {2022},
  abstract = {Hypergraphs are used to model higher-order interactions amongst agents and there exist many practically relevant instances of hypergraph datasets. To enable the efficient processing of hypergraph...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/PQQG5JAZ/Chien et al. - 2022 - You are AllSet A Multiset Function Framework for .pdf}
}

@article{choi_use_2021,
  title = {On the Use of Simulation in Robotics: {{Opportunities}}, Challenges, and Suggestions for Moving Forward},
  shorttitle = {On the Use of Simulation in Robotics},
  author = {Choi, HeeSun and Crump, Cindy and Duriez, Christian and Elmquist, Asher and Hager, Gregory and Han, David and Hearl, Frank and Hodgins, Jessica and Jain, Abhinandan and Leve, Frederick and Li, Chen and Meier, Franziska and Negrut, Dan and Righetti, Ludovic and Rodriguez, Alberto and Tan, Jie and Trinkle, Jeff},
  year = {2021},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {1},
  pages = {e1907856118},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1907856118},
  file = {/Users/fariedabuzaid/Zotero/storage/P2ZNTP7Q/Choi et al. - 2021 - On the use of simulation in robotics Opportunitie.pdf}
}

@article{clifton_novelty_2011,
  title = {Novelty {{Detection}} with {{Multivariate Extreme Value Statistics}}},
  author = {Clifton, David Andrew and Hugueny, Samuel and Tarassenko, Lionel},
  year = {2011},
  month = dec,
  journal = {Journal of Signal Processing Systems},
  volume = {65},
  number = {3},
  pages = {371--389},
  issn = {1939-8018, 1939-8115},
  doi = {10.1007/s11265-010-0513-6},
  abstract = {Novelty detection, or one-class classification, aims to determine if data are ``normal'' with respect to some model of normality constructed using examples of normal system behaviour. If that model is composed of generative probability distributions, the extent of ``normality'' in the data space can be described using Extreme Value Theory (EVT), a branch of statistics concerned with describing the tails of distributions. This paper demonstrates that existing approaches to the use of EVT for novelty detection are appropriate only for univariate, unimodal problems. We generalise the use of EVT for novelty detection to the analysis of data with multivariate, multimodal distributions, allowing a principled approach to the analysis of high-dimensional data to be taken. Examples are provided using vital-sign data obtained from a large clinical study of patients in a high-dependency hospital ward.},
  langid = {english}
}

@inproceedings{coggins_wattle_1994,
  title = {{{WATTLE}}: {{A Trainable Gain Analogue VLSI Neural Network}}},
  shorttitle = {{{WATTLE}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Coggins, Richard and Jabri, Marwan},
  editor = {Cowan, J. and Tesauro, G. and Alspector, J.},
  year = {1994},
  volume = {6},
  publisher = {{Morgan-Kaufmann}}
}

@article{cohen_feature_2007,
  title = {Feature {{Selection}} via {{Coalitional Game Theory}}},
  author = {Cohen, Shay and Dror, Gideor and Ruppin, Eytan},
  year = {2007},
  month = jul,
  journal = {Neural computation},
  volume = {19},
  number = {7},
  pages = {1939--1961},
  doi = {10.1162/neco.2007.19.7.1939},
  abstract = {We present and study the contribution-selection algorithm (CSA), a novel algorithm for feature selection. The algorithm is based on the multiperturbation shapley analysis (MSA), a framework that relies on game theory to estimate usefulness. The algorithm iteratively estimates the usefulness of features and selects them accordingly, using either forward selection or backward elimination. It can optimize various performance measures over unseen data such as accuracy, balanced error rate, and area under receiver-operator-characteristic curve. Empirical comparison with several other existing feature selection methods shows that the backward elimination variant of CSA leads to the most accurate classification results on an array of data sets.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/YKELFDRT/Cohen et al. - 2007 - Feature Selection via Coalitional Game Theory.pdf}
}

@book{coles_introduction_2013,
  title = {An {{Introduction}} to {{Statistical Modeling}} of {{Extreme Values}}.},
  author = {Coles, Stuart},
  year = {2013},
  publisher = {{Springer London, Limited}},
  address = {{London}},
  abstract = {Directly oriented towards real practical application, this book develops the basic theoretical framework of extreme value models and the statistical inference techniques for using these models in practice.  Intended for statisticians and non-statisticians alike, the theoretical treatment is elementary, with heuristics often replacing detailed mathematical proof. Most aspects of extreme modeling techniques are covered, including historical techniques (still widely used) and contemporary techniques based on point process models. A wide range of worked examples, using genuine datasets, illustrate the various modeling procedures and a concluding chapter provides a brief intorduction to a number of more advanced topics, including Bayesian inference and spatial extremes.  All the computations are carried out using S-PLUS, and the corresponding datasets and functions are available via the internet for readers to recreate examples for themselves. An essential reference for students and researchers in statistics and disciplines such as engineering, finance and environmental science, this book will also appeal to practicioners looking for practical help in solving real problems.  Stuart Coles is Reader in Statistics at the University of Bristol, U.K., having previously lectured at the universities of Nottingham and Lancaster. In 1992 he was the first recipient of the Royal Statistical Society's research prize. He has published widely in the statistical literature, principally in the area of extreme value modeling.},
  isbn = {978-1-4471-3675-0},
  langid = {english},
  annotation = {OCLC: 1066184729},
  file = {/Users/fariedabuzaid/Zotero/storage/EDRTCQYC/Coles - 2013 - An Introduction to Statistical Modeling of Extreme.pdf}
}

@article{colombo_learning_2012,
  title = {Learning High-Dimensional Directed Acyclic Graphs with Latent and Selection Variables},
  author = {Colombo, Diego and Maathuis, Marloes H. and Kalisch, Markus and Richardson, Thomas S.},
  year = {2012},
  month = feb,
  journal = {The Annals of Statistics},
  volume = {40},
  number = {1},
  pages = {294--321},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/11-AOS940},
  abstract = {We consider the problem of learning causal information between random variables in directed acyclic graphs (DAGs) when allowing arbitrarily many latent and selection variables. The FCI (Fast Causal Inference) algorithm has been explicitly designed to infer conditional independence and causal information in such settings. However, FCI is computationally infeasible for large graphs. We therefore propose the new RFCI algorithm, which is much faster than FCI. In some situations the output of RFCI is slightly less informative, in particular with respect to conditional independence information. However, we prove that any causal information in the output of RFCI is correct in the asymptotic limit. We also define a class of graphs on which the outputs of FCI and RFCI are identical. We prove consistency of FCI and RFCI in sparse high-dimensional settings, and demonstrate in simulations that the estimation performances of the algorithms are very similar. All software is implemented in the R-package pcalg.},
  file = {/Users/fariedabuzaid/Zotero/storage/P67IKKKF/Colombo et al. - 2012 - Learning high-dimensional directed acyclic graphs .pdf}
}

@book{conn_trust_2000,
  title = {Trust {{Region Methods}}},
  author = {Conn, Andrew R. and Gould, Nicholas I. M. and Toint, Philippe L.},
  year = {2000},
  month = jan,
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898719857},
  abstract = {This is the first comprehensive reference on trust-region methods, a class of numerical algorithms for the solution of nonlinear convex optimization methods. Its unified treatment covers both unconstrained and constrained problems and reviews a large part of the specialized literature on the subject. It also provides an up-to-date view of numerical optimization. Written primarily for postgraduates and researchers, the book features an extensive commented bibliography, which contains more than 1000 references by over 750 authors. The book also contains several practical comments and an entire chapter devoted to software and implementation issues. Its many illustrations, including nearly 100 figures, balance the formal and intuitive treatment of the presented topics.},
  isbn = {978-0-89871-460-9 978-0-89871-985-7},
  langid = {english}
}

@article{courbariaux_binarized_2016,
  title = {Binarized {{Neural Networks}}: {{Training Deep Neural Networks}} with {{Weights}} and {{Activations Constrained}} to +1 or -1},
  shorttitle = {Binarized {{Neural Networks}}},
  author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and {El-Yaniv}, Ran and Bengio, Yoshua},
  year = {2016},
  month = mar,
  journal = {arXiv:1602.02830 [cs]},
  eprint = {1602.02830},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/9N37JAIF/Courbariaux et al. - 2016 - Binarized Neural Networks Training Deep Neural Ne.pdf}
}

@book{cover_elements_2006,
  title = {Elements of {{Information Theory}}},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  year = {2006},
  series = {Wiley {{Series}} in {{Telecommunications}} and {{Signal Processing}}},
  publisher = {{Wiley-Interscience}},
  isbn = {0-471-24195-4}
}

@article{croux_limit_1998,
  title = {Limit Behavior of the Empirical Influence Function of the Median},
  author = {Croux, Christophe},
  year = {1998},
  month = mar,
  journal = {Statistics \& Probability Letters},
  volume = {37},
  number = {4},
  pages = {331--340},
  issn = {0167-7152},
  doi = {10.1016/S0167-7152(97)00135-1},
  abstract = {The empirical influence function EIF(x, Tn; X) measures the influence of an observation x on the estimator Tn at a sample X of size n. In this note we show that the empirical influence function of the median is not a consistent estimator of the corresponding influence function. This observation leads to a reconsideration of the most -robustness property of the median. We will prove and show by simulations that the median is less robust to single outliers than commonly believed.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/HXN42NMT/Croux - 1998 - Limit behavior of the empirical influence function.pdf}
}

@inproceedings{cubuk_autoaugment_2019,
  title = {{{AutoAugment}}: {{Learning Augmentation Policies}} from {{Data}}},
  shorttitle = {{{AutoAugment}}},
  booktitle = {{{CVPR}} 2019},
  author = {Cubuk, Ekin Dogus and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
  year = {2019},
  eprint = {1805.09501v3},
  eprinttype = {arxiv},
  address = {{Long Beach, CA, USA}},
  abstract = {Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many sub-policies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5\% which is 0.4\% better than the previous record of 83.1\%. On CIFAR-10, we achieve an error rate of 1.5\%, which is 0.6\% better than the previous state-of-the-art. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/EIYBTKAM/Cubuk et al. - 2019 - AutoAugment Learning Augmentation Policies from D.pdf}
}

@inproceedings{cucala_explainable_2022,
  title = {Explainable {{GNN-Based Models}} over {{Knowledge Graphs}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022))},
  author = {Cucala, David Jaime Tena and Grau, Bernardo Cuenca and Kostylev, Egor V. and Motik, Boris},
  year = {2022},
  abstract = {Graph Neural Networks (GNNs) are often used to learn transformations of graph data. While effective in practice, such approaches make predictions via numeric manipulations so their output cannot be...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/ZIG3AXEW/Cucala et al. - 2021 - Explainable GNN-Based Models over Knowledge Graphs.pdf;/Users/fariedabuzaid/Zotero/storage/H6YQYTKP/forum.html}
}

@inproceedings{dai_graphaugmented_2022,
  title = {Graph-{{Augmented Normalizing Flows}} for {{Anomaly Detection}} of {{Multiple Time Series}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Dai, Enyan and Chen, Jie},
  year = {2022},
  abstract = {Anomaly detection is a widely studied task for a broad variety of data types; among them, multiple time series appear frequently in applications, including for example, power grids and traffic...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/8P6UD5PP/Dai und Chen - 2021 - Graph-Augmented Normalizing Flows for Anomaly Dete.pdf;/Users/fariedabuzaid/Zotero/storage/5CN6UM2H/forum.html}
}

@inproceedings{dathathri_enabling_2020,
  title = {Enabling Certification of Verification-Agnostic Networks via Memory-Efficient Semidefinite Programming},
  booktitle = {Advances in {{Neural Information Processing Systems}} 33},
  author = {Dathathri, Sumanth and Dvijotham, Krishnamurthy and Kurakin, Alexey and Raghunathan, Aditi and Uesato, Jonathan and Bunel, Rudy and Shankar, Shreya and Steinhardt, Jacob and Goodfellow, Ian and Liang, Percy and Kohli, Pushmeet},
  year = {2020},
  month = nov,
  eprint = {2010.11645},
  eprinttype = {arxiv},
  abstract = {Convex relaxations have emerged as a promising approach for verifying desirable properties of neural networks like robustness to adversarial perturbations. Widely used Linear Programming (LP) relaxations only work well when networks are trained to facilitate verification. This precludes applications that involve verification-agnostic networks, i.e., networks not specially trained for verification. On the other hand, semidefinite programming (SDP) relaxations have successfully be applied to verification-agnostic networks, but do not currently scale beyond small networks due to poor time and space asymptotics. In this work, we propose a first-order dual SDP algorithm that (1) requires memory only linear in the total number of network activations, (2) only requires a fixed number of forward/backward passes through the network per iteration. By exploiting iterative eigenvector methods, we express all solver operations in terms of forward and backward passes through the network, enabling efficient use of hardware like GPUs/TPUs. For two verification-agnostic networks on MNIST and CIFAR-10, we significantly improve L-inf verified robust accuracy from 1\% to 88\% and 6\% to 40\% respectively. We also demonstrate tight verification of a quadratic stability specification for the decoder of a variational autoencoder.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/C5NS5ET9/Dathathri et al. - 2020 - Enabling certification of verification-agnostic ne.pdf}
}

@techreport{dau_ucr_2018,
  title = {The {{UCR}} Time Series Classification Archive},
  author = {{Dau} and Anh, Hoang and Keogh, Eamonn and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Ann and {Yanping} and Hu, Bing and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo and {Hexagon-ML}},
  year = {2018},
  month = oct,
  file = {/Users/fariedabuzaid/Zotero/storage/IL6ZD4XZ/UCR_TimeSeriesAnomalyDatasets2021.zip;/Users/fariedabuzaid/Zotero/storage/RJ9EJHSM/DataSummary.csv}
}

@article{dawid_wellcalibrated_1982,
  title = {The {{Well-Calibrated Bayesian}}},
  author = {Dawid, A. P.},
  year = {1982},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {77},
  number = {379},
  pages = {605--610},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1982.10477856},
  abstract = {Suppose that a forecaster sequentially assigns probabilities to events. He is well calibrated if, for example, of those events to which he assigns a probability 30 percent, the long-run proportion that actually occurs turns out to be 30 percent. We prove a theorem to the effect that a coherent Bayesian expects to be well calibrated, and consider its destructive implications for the theory of coherence.},
  annotation = {citecount: 00000  \_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1982.10477856},
  file = {/Users/fariedabuzaid/Zotero/storage/WN5HPUPR/Dawid - 1982 - The Well-Calibrated Bayesian.pdf}
}

@article{deavilabelbute-peres_endtoend_2018,
  title = {End-to-{{End Differentiable Physics}} for {{Learning}} and {{Control}}},
  author = {{de Avila Belbute-Peres}, Filipe and Smith, Kevin and Allen, Kelsey and Tenenbaum, Josh and Kolter, J. Zico},
  year = {2018},
  journal = {Advances in Neural Information Processing Systems},
  volume = {31},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/HTLUXNFS/de Avila Belbute-Peres et al. - 2018 - End-to-End Differentiable Physics for Learning and.pdf}
}

@book{degroot_optimal_2005,
  title = {Optimal {{Statistical Decisions}}},
  author = {DeGroot, Morris H.},
  year = {2005},
  month = jan,
  publisher = {{John Wiley \& Sons}},
  abstract = {The Wiley Classics Library consists of selected books that have become recognized classics in their respective fields. With these new unabridged and inexpensive editions, Wiley hopes to extend the life of these important works by making them available to future generations of mathematicians and scientists.},
  googlebooks = {dtVieJ245z0C},
  isbn = {978-0-471-72614-2},
  langid = {english}
}

@article{degroot_uncertainty_1962,
  title = {Uncertainty, {{Information}}, and {{Sequential Experiments}}},
  author = {DeGroot, M. H.},
  year = {1962},
  journal = {The Annals of Mathematical Statistics},
  volume = {33},
  number = {2},
  pages = {404--419},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  abstract = {Consider a situation in which it is desired to gain knowledge about the true value of some parameter (or about the true state of the world) by means of experimentation. Let {$\Omega$} denote the set of all possible values of the parameter \texttheta, and suppose that the experimenter's knowledge about the true value of \texttheta{} can be expressed, at each stage of experimentation, in terms of a probability distribution {$\xi$} over {$\Omega$}. Each distribution {$\xi$} indicates a certain amount of uncertainty on the part of the experimenter about the true value of \texttheta, and it is assumed that for each {$\xi$} this uncertainty can be characterized by a non-negative number. The information in an experiment is then defined as the expected difference between the uncertainty of the prior distribution over {$\Omega$} and the uncertainty of the posterior distribution. In any particular situation, the selection of an appropriate uncertainty function would typically be based on the use to which the experimenter's knowledge about \texttheta{} is to be put. If, for example, the actions available to the experimenter and the losses associated with these actions can be specified as in a statistical decision problem, then presumably the uncertainty function would be determined from the loss function. In Section 2 some properties of uncertainty and information functions, and their relation to statistical decision problems and loss functions, are considered. In Section 3 the sequential sampling rule whereby experiments are performed until the uncertainty is reduced to a preassigned level is studied for various uncertainty functions and experiments. This rule has been previously studied by Lindley, [8], [9], in special cases where the uncertainty function is the Shannon entropy function. In Sections 4 and 5 the problem of optimally choosing the experiments to be performed sequentially from a class of available experiments is considered when the goal is either to minimize the expected uncertainty after a fixed number of experiments or to minimize the expected number of experiments needed to reduce the uncertainty to a fixed level. Particular problems of this nature have been treated by Bradt and Karlin [6]. The recent work of Chernoff [7] and Albert [1] on the sequential design of experiments is also of interest in relation to these problems.}
}

@inproceedings{deng_imagenet_2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  month = jun,
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500\textendash 1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  file = {/Users/fariedabuzaid/Zotero/storage/SY8SBXB7/5206848.html}
}

@inproceedings{deutch_explanations_2021,
  title = {Explanations for {{Data Repair Through Shapley Values}}},
  booktitle = {Proceedings of the 30th {{ACM International Conference}} on {{Information}} \& {{Knowledge Management}}},
  author = {Deutch, Daniel and Frost, Nave and Gilad, Amir and Sheffer, Oren},
  year = {2021},
  month = oct,
  series = {{{CIKM}} '21},
  pages = {362--371},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3459637.3482341},
  abstract = {Data repair, i.e., the identification and fix of errors in the data, is a central component of the Data Science cycle. As such, significant research effort has been devoted to automate the repair process. Yet it still requires significant manual labor by the Data Scientists, tweaking and optimizing repair modules (up to 80\% of their time, according to surveys). To this end, we propose in this paper a novel framework for explaining the results of any data repair module. Explanations involve identifying the table cells and database constraints having the strongest influence on the process. Influence, in turn, is quantified through the game-theoretic notion of Shapley values, commonly used for explaining Machine Learning classifier results. The main technical challenge is that exact computation of Shapley values incurs exponential time. We consequently devise and optimize novel approximation algorithms, and analyze them both theoretically and empirically. Our results show the efficiency of our approach when compared to the alternative of adapting existing Shapley value computation techniques to the data repair settings.},
  isbn = {978-1-4503-8446-9},
  file = {/Users/fariedabuzaid/Zotero/storage/PSRYXXYB/Deutch et al. - 2021 - Explanations for Data Repair Through Shapley Value.pdf}
}

@article{diffenderfer_multiprize_2021,
  title = {Multi-{{Prize Lottery Ticket Hypothesis}}: {{Finding Accurate Binary Neural Networks}} by {{Pruning A Randomly Weighted Network}}},
  shorttitle = {Multi-{{Prize Lottery Ticket Hypothesis}}},
  author = {Diffenderfer, James and Kailkhura, Bhavya},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.09377 [cs]},
  eprint = {2103.09377},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Recently, Frankle \& Carbin (2019) demonstrated that randomly-initialized dense networks contain subnetworks that once found can be trained to reach test accuracy comparable to the trained dense network. However, finding these high performing trainable subnetworks is expensive, requiring iterative process of training and pruning weights. In this paper, we propose (and prove) a stronger Multi-Prize Lottery Ticket Hypothesis: A sufficiently over-parameterized neural network with random weights contains several subnetworks (winning tickets) that (a) have comparable accuracy to a dense target network with learned weights (prize 1), (b) do not require any further training to achieve prize 1 (prize 2), and (c) is robust to extreme forms of quantization (i.e., binary weights and/or activation) (prize 3).},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/G8RIZNY9/Diffenderfer and Kailkhura - 2021 - Multi-Prize Lottery Ticket Hypothesis Finding Acc.pdf}
}

@inproceedings{ding_revisiting_2020,
  title = {Revisiting the {{Evaluation}} of {{Uncertainty Estimation}} and {{Its Application}} to {{Explore Model Complexity-Uncertainty Trade-Off}}},
  booktitle = {{{arXiv}}:1903.02050 [Cs, Stat]},
  author = {Ding, Yukun and Liu, Jinglan and Xiong, Jinjun and Shi, Yiyu},
  year = {2020},
  month = jul,
  eprint = {1903.02050},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Accurately estimating uncertainties in neural network predictions is of great importance in building trusted DNNs-based models, and there is an increasing interest in providing accurate uncertainty estimation on many tasks, such as security cameras and autonomous driving vehicles. In this paper, we focus on the two main use cases of uncertainty estimation, i.e. selective prediction and confidence calibration. We first reveal potential issues of commonly used quality metrics for uncertainty estimation in both use cases, and propose our new metrics to mitigate them. We then apply these new metrics to explore the trade-off between model complexity and uncertainty estimation quality, a critically missing work in the literature. Our empirical experiment results validate the superiority of the proposed metrics, and some interesting trends about the complexity-uncertainty trade-off are observed.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/VFQUJJBR/Ding et al. - 2020 - Revisiting the Evaluation of Uncertainty Estimatio.pdf}
}

@article{dissanayake_neuralnetworkbased_1994,
  title = {Neural-Network-Based Approximations for Solving Partial Differential Equations},
  author = {Dissanayake, M. W. M. G. and Phan-Thien, N.},
  year = {1994},
  journal = {Communications in Numerical Methods in Engineering},
  volume = {10},
  number = {3},
  pages = {195--201},
  issn = {1099-0887},
  doi = {10.1002/cnm.1640100303},
  abstract = {A numerical method, based on neural-network-based functions, for solving partial differential equations is reported in the paper. Using a `universal approximator' based on a neural network and point collocation, the numerical problem of solving the partial differential equation is transformed to an unconstrained minimization problem. The method is extremely easy to implement and is suitable for obtaining an approximate solution in a short period of time. The technique is illustrated with the aid of two numerical examples.},
  copyright = {Copyright \textcopyright{} 1994 John Wiley \& Sons, Ltd},
  langid = {english},
  annotation = {citecount: 00066},
  file = {/Users/fariedabuzaid/Zotero/storage/T5784IBS/Dissanayake and Phan‐Thien - 1994 - Neural-network-based approximations for solving pa.pdf}
}

@article{dockhorn_discussion_2019,
  title = {A {{Discussion}} on {{Solving Partial Differential Equations}} Using {{Neural Networks}}},
  author = {Dockhorn, Tim},
  year = {2019},
  month = apr,
  journal = {arXiv:1904.07200 [cs, math, stat]},
  eprint = {1904.07200},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Can neural networks learn to solve partial differential equations (PDEs)? We investigate this question for two (systems of) PDEs, namely, the Poisson equation and the steady Navier--Stokes equations. The contributions of this paper are five-fold. (1) Numerical experiments show that small neural networks ({$<$} 500 learnable parameters) are able to accurately learn complex solutions for systems of partial differential equations. (2) It investigates the influence of random weight initialization on the quality of the neural network approximate solution and demonstrates how one can take advantage of this non-determinism using ensemble learning. (3) It investigates the suitability of the loss function used in this work. (4) It studies the benefits and drawbacks of solving (systems of) PDEs with neural networks compared to classical numerical methods. (5) It proposes an exhaustive list of possible directions of future work.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/N37BHSEX/Dockhorn - 2019 - A Discussion on Solving Partial Differential Equat.pdf}
}

@inproceedings{dockhorn_scorebased_2022,
  title = {Score-{{Based Generative Modeling}} with {{Critically-Damped Langevin Diffusion}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR2022}})},
  author = {Dockhorn, Tim and Vahdat, Arash and Kreis, Karsten},
  year = {2022},
  abstract = {Score-based generative models (SGMs) have demonstrated remarkable synthesis quality. SGMs rely on a diffusion process that gradually perturbs the data towards a tractable distribution, while the...},
  langid = {english},
  annotation = {video: https://iclr.cc/virtual/2022/poster/6687},
  file = {/Users/fariedabuzaid/Zotero/storage/JY8NVVFJ/Dockhorn et al. - 2021 - Score-Based Generative Modeling with Critically-Da.pdf}
}

@inproceedings{domke_importance_2018,
  title = {Importance {{Weighting}} and {{Variational Inference}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Domke, Justin and Sheldon, Daniel R},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  address = {{Montreal, Canada}},
  abstract = {Recent work used importance sampling ideas for better variational bounds on likelihoods. We clarify the applicability of these ideas to pure probabilistic inference, by showing the resulting Importance Weighted Variational Inference (IWVI) technique is an instance of augmented variational inference, thus identifying the looseness in previous work. Experiments confirm IWVI's practicality for probabilistic inference. As a second contribution, we investigate inference with elliptical distributions, which improves accuracy in low dimensions, and convergence in high dimensions.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/GIXMUS6P/Domke and Sheldon - 2018 - Importance Weighting and Variational Inference.pdf}
}

@article{dosovitskiy_image_2021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2021},
  month = jun,
  journal = {arXiv:2010.11929 [cs]},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/XQUSQHBH/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf}
}

@article{drton_computation_2019,
  title = {Computation of Maximum Likelihood Estimates in Cyclic Structural Equation Models},
  author = {Drton, Mathias and Fox, Christopher and Wang, Y. Samuel},
  year = {2019},
  month = apr,
  journal = {The Annals of Statistics},
  volume = {47},
  number = {2},
  pages = {663--690},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/17-AOS1602},
  abstract = {Software for computation of maximum likelihood estimates in linear structural equation models typically employs general techniques from nonlinear optimization, such as quasi-Newton methods. In practice, careful tuning of initial values is often required to avoid convergence issues. As an alternative approach, we propose a block-coordinate descent method that cycles through the considered variables, updating only the parameters related to a given variable in each step. We show that the resulting block update problems can be solved in closed form even when the structural equation model comprises feedback cycles. Furthermore, we give a characterization of the models for which the block-coordinate descent algorithm is well defined, meaning that for generic data and starting values all block optimization problems admit a unique solution. For the characterization, we represent each model by its mixed graph (also known as path diagram), which leads to criteria that can be checked in time that is polynomial in the number of considered variables.},
  langid = {english},
  mrnumber = {MR3909946},
  zmnumber = {07033147},
  annotation = {citecount: 00003},
  file = {/Users/fariedabuzaid/Zotero/storage/EEDT6IWF/Drton et al. - 2019 - Computation of maximum likelihood estimates in cyc.pdf}
}

@inproceedings{dupty_pfgnn_2022,
  title = {{{PF-GNN}}: {{Differentiable}} Particle Filtering Based Approximation of Universal Graph Representations},
  shorttitle = {{{PF-GNN}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Dupty, Mohammed Haroon and Dong, Yanfei and Lee, Wee Sun},
  year = {2022},
  abstract = {Message passing Graph Neural Networks (GNNs) are known to be limited in expressive power by the 1-WL color-refinement test for graph isomorphism. Other more expressive models either are...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/6RUTBI2R/Dupty et al. - 2021 - PF-GNN Differentiable particle filtering based ap.pdf;/Users/fariedabuzaid/Zotero/storage/A2S95VWQ/forum.html}
}

@inproceedings{dusell_learning_2022,
  title = {Learning {{Hierarchical Structures}} with {{Differentiable Nondeterministic Stacks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {DuSell, Brian and Chiang, David},
  year = {2022},
  abstract = {Learning hierarchical structures in sequential data -- from simple algorithmic patterns to natural language -- in a reliable, generalizable way remains a challenging problem for neural language...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/VDIF42KI/DuSell and Chiang - 2022 - Learning Hierarchical Structures with Differentiab.pdf}
}

@inproceedings{dwivedi_graph_2021,
  title = {Graph {{Neural Networks}} with {{Learnable Structural}} and {{Positional Representations}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Dwivedi, Vijay Prakash and Luu, Anh Tuan and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  year = {2021},
  abstract = {Graph neural networks (GNNs) have become the standard learning architectures for graphs. GNNs have been applied to numerous domains ranging from quantum chemistry, recommender systems to knowledge...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/TETPXJI8/Dwivedi et al. - 2022 - Graph Neural Networks with Learnable Structural an.pdf}
}

@article{e_deep_2017,
  title = {Deep {{Learning-Based Numerical Methods}} for {{High-Dimensional Parabolic Partial Differential Equations}} and {{Backward Stochastic Differential Equations}}},
  author = {E, Weinan and Han, Jiequn and Jentzen, Arnulf},
  year = {2017},
  month = dec,
  journal = {Communications in Mathematics and Statistics},
  volume = {5},
  number = {4},
  pages = {349--380},
  issn = {2194-671X},
  doi = {10.1007/s40304-017-0117-6},
  abstract = {We study a new algorithm for solving parabolic partial differential equations (PDEs) and backward stochastic differential equations (BSDEs) in high dimension, which is based on an analogy between the BSDE and reinforcement learning with the gradient of the solution playing the role of the policy function, and the loss function given by the error between the prescribed terminal condition and the solution of the BSDE. The policy function is then approximated by a neural network, as is done in deep reinforcement learning. Numerical results using TensorFlow illustrate the efficiency and accuracy of the studied algorithm for several 100-dimensional nonlinear PDEs from physics and finance such as the Allen\textendash Cahn equation, the Hamilton\textendash Jacobi\textendash Bellman equation, and a nonlinear pricing model for financial derivatives.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/TWZZ8I6K/E et al. - 2017 - Deep Learning-Based Numerical Methods for High-Dim.pdf}
}

@article{e_deep_2018,
  ids = {e_deep_2017a},
  title = {The {{Deep Ritz}} Method: {{A}} Deep Learning-Based Numerical Algorithm for Solving Variational Problems},
  shorttitle = {The {{Deep Ritz}} Method},
  author = {E, Weinan and Yu, Bing},
  year = {2018},
  month = mar,
  journal = {Communications in Mathematics and Statistics},
  volume = {6},
  number = {1},
  eprint = {1710.00211},
  eprinttype = {arxiv},
  pages = {1--12},
  issn = {2194-671X},
  doi = {10.1007/s40304-018-0127-z},
  abstract = {We propose a deep learning based method, the Deep Ritz Method, for numerically solving variational problems, particularly the ones that arise from partial differential equations. The Deep Ritz method is naturally nonlinear, naturally adaptive and has the potential to work in rather high dimensions. The framework is quite simple and fits well with the stochastic gradient descent method used in deep learning. We illustrate the method on several problems including some eigenvalue problems.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/55GL6TGH/E and Yu - 2018 - The Deep Ritz method A deep learning-based numeri.pdf}
}

@misc{eamonn_ucr_2016,
  type = {{{UCR}} Staff Website},
  title = {The {{UCR Matrix Profile Page}}},
  author = {Eamonn, Keogh},
  year = {2016},
  abstract = {The Matrix Profile (and the algorithms to compute it: STAMP, STAMPI, STOMP, SCRIMP, SCRIMP++, SWAMP  and GPU-STOMP), has the potential to revolutionize time series data mining because of its generality, versatility, simplicity and scalability.  In particular it has implications for time series motif discovery, time series joins, shapelet discovery (classification), density estimation, semantic segmentation, visualization, rule discovery, clustering etc (note, for pure similarity search, we suggest you see MASS for Euclidean Distance, and the UCR Suite for DTW)},
  howpublished = {https://www.cs.ucr.edu/\textasciitilde eamonn/MatrixProfile.html},
  langid = {english}
}

@inproceedings{ehlers_formal_2017,
  title = {Formal {{Verification}} of {{Piece-Wise Linear Feed-Forward Neural Networks}}},
  booktitle = {Automated {{Technology}} for {{Verification}} and {{Analysis}}},
  author = {Ehlers, R{\"u}diger},
  editor = {D'Souza, Deepak and Narayan Kumar, K.},
  year = {2017},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  eprint = {1705.01320},
  eprinttype = {arxiv},
  pages = {269--286},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-68167-2_19},
  abstract = {We present an approach for the verification of feed-forward neural networks in which all nodes have a piece-wise linear activation function. Such networks are often used in deep learning and have been shown to be hard to verify for modern satisfiability modulo theory (SMT) and integer linear programming (ILP) solvers.The starting point of our approach is the addition of a global linear approximation of the overall network behavior to the verification problem that helps with SMT-like reasoning over the network behavior. We present a specialized verification algorithm that employs this approximation in a search process in which it infers additional node phases for the non-linear nodes in the network from partial node phase assignments, similar to unit propagation in classical SAT solving. We also show how to infer additional conflict clauses and safe node fixtures from the results of the analysis steps performed during the search. The resulting approach is evaluated on collision avoidance and handwritten digit recognition case studies.},
  archiveprefix = {arXiv},
  isbn = {978-3-319-68167-2},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/D2CQTEJ9/Ehlers - 2017 - Formal Verification of Piece-Wise Linear Feed-Forw.pdf}
}

@phdthesis{eigenmann_improving_2020,
  type = {Doctoral {{Thesis}}},
  title = {Improving and Assessing Causal Inference Algorithms for {{DAGs}}},
  author = {Eigenmann, Marco F.},
  year = {2020},
  doi = {10.3929/ethz-b-000428225},
  copyright = {http://rightsstatements.org/page/InC-NC/1.0/},
  langid = {english},
  school = {ETH Zurich},
  annotation = {Accepted: 2020-07-23T12:51:50Z},
  file = {/Users/fariedabuzaid/Zotero/storage/PER5D8AE/Eigenmann - 2020 - Improving and assessing causal inference algorithm.pdf}
}

@book{facchinei_finitedimensional_2003,
  title = {{Finite-Dimensional Variational Inequalities and Complementarity Problems}},
  author = {Facchinei, Francisco and Pang, Jong-Shi},
  year = {2003},
  month = feb,
  edition = {Two thousand, third},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-95580-3},
  langid = {Englisch},
  annotation = {pdf:https://www.researchgate.net/publication/220588703\_Finite-Dimensional\_Variational\_Inequality\_and\_Nonlinear\_Complementarity\_Problems\_A\_Survey\_of\_Theory\_Algorithms\_and\_Applications}
}

@article{fang_exploring_2021,
  title = {Exploring {{Deep Neural Networks}} via {{Layer-Peeled Model}}: {{Minority Collapse}} in {{Imbalanced Training}}},
  shorttitle = {Exploring {{Deep Neural Networks}} via {{Layer-Peeled Model}}},
  author = {Fang, Cong and He, Hangfeng and Long, Qi and Su, Weijie J.},
  year = {2021},
  month = sep,
  journal = {arXiv:2101.12699 [cs, math, stat]},
  eprint = {2101.12699},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {In this paper, we introduce the \textbackslash textit\{Layer-Peeled Model\}, a nonconvex yet analytically tractable optimization program, in a quest to better understand deep neural networks that are trained for a sufficiently long time. As the name suggests, this new model is derived by isolating the topmost layer from the remainder of the neural network, followed by imposing certain constraints separately on the two parts of the network. We demonstrate that the Layer-Peeled Model, albeit simple, inherits many characteristics of well-trained neural networks, thereby offering an effective tool for explaining and predicting common empirical patterns of deep learning training. First, when working on class-balanced datasets, we prove that any solution to this model forms a simplex equiangular tight frame, which in part explains the recently discovered phenomenon of neural collapse \textbackslash cite\{papyan2020prevalence\}. More importantly, when moving to the imbalanced case, our analysis of the Layer-Peeled Model reveals a hitherto unknown phenomenon that we term \textbackslash textit\{Minority Collapse\}, which fundamentally limits the performance of deep learning models on the minority classes. In addition, we use the Layer-Peeled Model to gain insights into how to mitigate Minority Collapse. Interestingly, this phenomenon is first predicted by the Layer-Peeled Model before being confirmed by our computational experiments.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/FJEM9MKX/Fang et al. - 2021 - Exploring Deep Neural Networks via Layer-Peeled Mo.pdf}
}

@article{fazlyab_safety_2020,
  title = {Safety {{Verification}} and {{Robustness Analysis}} of {{Neural Networks}} via {{Quadratic Constraints}} and {{Semidefinite Programming}}},
  author = {Fazlyab, Mahyar and Morari, Manfred and Pappas, George J.},
  year = {2020},
  month = dec,
  journal = {IEEE Transactions on Automatic Control},
  eprint = {1903.01287},
  eprinttype = {arxiv},
  pages = {16},
  issn = {1558-2523},
  doi = {10.1109/TAC.2020.3046193},
  abstract = {Certifying the safety or robustness of neural networks against input uncertainties and adversarial attacks is an emerging challenge in the area of safe machine learning and control. To provide such a guarantee, one must be able to bound the output of neural networks when their input changes within a bounded set. In this paper, we propose a semidefinite programming (SDP) framework to address this problem for feed-forward neural networks with general activation functions and input uncertainty sets. Our main idea is to abstract various properties of activation functions (e.g., monotonicity, bounded slope, bounded values, and repetition across layers) with the formalism of quadratic constraints. We then analyze the safety properties of the abstracted network via the S-procedure and semidefinite programming. Our framework spans the trade-off between conservatism and computational efficiency and applies to problems beyond safety verification. We evaluate the performance of our approach via numerical problem instances of various sizes.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/6KYQ5Z48/Fazlyab et al. - 2020 - Safety Verification and Robustness Analysis of Neu.pdf}
}

@article{fedus_switch_2021,
  title = {Switch {{Transformers}}: {{Scaling}} to {{Trillion Parameter Models}} with {{Simple}} and {{Efficient Sparsity}}},
  shorttitle = {Switch {{Transformers}}},
  author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
  year = {2021},
  month = jan,
  journal = {arXiv:2101.03961 [cs]},
  eprint = {2101.03961},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the "Colossal Clean Crawled Corpus" and achieve a 4x speedup over the T5-XXL model.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/276C59TA/Fedus et al. - 2021 - Switch Transformers Scaling to Trillion Parameter.pdf}
}

@inproceedings{feng_degree_2022,
  title = {{{DEGREE}}: {{Decomposition Based Explanation}} for {{Graph Neural Networks}}},
  shorttitle = {{{DEGREE}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Feng, Qizhang and Liu, Ninghao and Yang, Fan and Tang, Ruixiang and Du, Mengnan and Hu, Xia},
  year = {2022},
  abstract = {Graph Neural Networks (GNNs) are gaining extensive attention for their application in graph data. However, the black-box nature of GNNs prevents users from understanding and trusting the models...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/BU6ZTEEF/Feng et al. - 2021 - DEGREE Decomposition Based Explanation for Graph .pdf;/Users/fariedabuzaid/Zotero/storage/9Q8XPCRN/forum.html}
}

@article{ferri_setting_2019,
  title = {Setting Decision Thresholds When Operating Conditions Are Uncertain},
  author = {Ferri, C{\`e}sar and {Hern{\'a}ndez-Orallo}, Jos{\'e} and Flach, Peter},
  year = {2019},
  month = jul,
  journal = {Data Mining and Knowledge Discovery},
  volume = {33},
  number = {4},
  pages = {805--847},
  issn = {1384-5810, 1573-756X},
  doi = {10.1007/s10618-019-00613-7},
  abstract = {The quality of the decisions made by a machine learning model depends on the data and the operating conditions during deployment. Often, operating conditions such as class distribution and misclassification costs have changed during the time since the model was trained and evaluated. When deploying a binary classifier that outputs scores, once we know the new class distribution and the new cost ratio between false positives and false negatives, there are several methods in the literature to help us choose an appropriate threshold for the classifier's scores. However, on many occasions, the information that we have about this operating condition is uncertain. Previous work has considered ranges or distributions of operating conditions during deployment, with expected costs being calculated for ranges or intervals, but still the decision for each point is made as if the operating condition were certain. The implications of this assumption have received limited attention: a threshold choice that is best suited without uncertainty may be suboptimal under uncertainty. In this paper we analyse the effect of operating condition uncertainty on the expected loss for different threshold choice methods, both theoretically and experimentally. We model uncertainty as a second conditional distribution over the actual operation condition and study it theoretically in such a way that minimum and maximum uncertainty are both seen as special cases of this general formulation. This is complemented by a thorough experimental analysis investigating how different learning algorithms behave for a range of datasets according to the threshold choice method and the uncertainty level.},
  langid = {english},
  annotation = {citecount: 00000},
  file = {/Users/fariedabuzaid/Zotero/storage/C8JAESGA/Ferri et al. - 2019 - Setting decision thresholds when operating conditi.pdf}
}

@inproceedings{fezza_perceptual_2019,
  title = {Perceptual {{Evaluation}} of {{Adversarial Attacks}} for {{CNN-based Image Classification}}},
  booktitle = {2019 {{Eleventh International Conference}} on {{Quality}} of {{Multimedia Experience}} ({{QoMEX}})},
  author = {Fezza, Sid Ahmed and Bakhti, Yassine and Hamidouche, Wassim and D{\'e}forges, Olivier},
  year = {2019},
  month = jun,
  eprint = {1906.00204},
  eprinttype = {arxiv},
  doi = {10.1109/QoMEX.2019.8743213},
  abstract = {Deep neural networks (DNNs) have recently achieved state-of-the-art performance and provide significant progress in many machine learning tasks, such as image classification, speech processing, natural language processing, etc. However, recent studies have shown that DNNs are vulnerable to adversarial attacks. For instance, in the image classification domain, adding small imperceptible perturbations to the input image is sufficient to fool the DNN and to cause misclassification. The perturbed image, called \textbackslash textit\{adversarial example\}, should be visually as close as possible to the original image. However, all the works proposed in the literature for generating adversarial examples have used the \$L\_\{p\}\$ norms (\$L\_\{0\}\$, \$L\_\{2\}\$ and \$L\_\{\textbackslash infty\}\$) as distance metrics to quantify the similarity between the original image and the adversarial example. Nonetheless, the \$L\_\{p\}\$ norms do not correlate with human judgment, making them not suitable to reliably assess the perceptual similarity/fidelity of adversarial examples. In this paper, we present a database for visual fidelity assessment of adversarial examples. We describe the creation of the database and evaluate the performance of fifteen state-of-the-art full-reference (FR) image fidelity assessment metrics that could substitute \$L\_\{p\}\$ norms. The database as well as subjective scores are publicly available to help designing new metrics for adversarial examples and to facilitate future research works.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/IDRD75IB/Fezza et al. - 2019 - Perceptual Evaluation of Adversarial Attacks for C.pdf}
}

@inproceedings{flennerhag_bootstrapped_2022,
  title = {Bootstrapped {{Meta-Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR2022}})},
  author = {Flennerhag, Sebastian and Schroecker, Yannick and Zahavy, Tom and {van Hasselt}, Hado and Silver, David and Singh, Satinder},
  year = {2022},
  month = mar,
  eprint = {2109.04504},
  eprinttype = {arxiv},
  pages = {37},
  address = {{Virtual event}},
  abstract = {Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging meta-optimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the meta-learner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that the metric can control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an epsilon-greedy Q-learning agent, without backpropagating through the update rule.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {notion: https://www.notion.so/appliedaiinitiative/Bootstrapped-Meta-Learning-dd2b49b762974b15b6b422752a932aac},
  file = {/Users/fariedabuzaid/Zotero/storage/83A4BXJA/Flennerhag et al. - 2022 - Bootstrapped Meta-Learning.pdf}
}

@techreport{fletcher_modified_1971,
  title = {A Modified {{Marquardt}} Subroutine for Nonlinear Least Squares},
  author = {Fletcher, R},
  year = {1971},
  address = {{Harwell, Berskshire}},
  institution = {{Atomic Energy Research Establishment}},
  abstract = {A FORTRAN subroutine is described for minimizing a sum of squares of functions of many variables. Such problems arise in non-linear data fitting, and in the solution of non-linear algebraic equations. The subroutine is based on an algorithn due to Marquardt, but with modifications which improve the performance of the method in certain circumstances, yet which require negligible extra computer time and storage.},
  file = {/Users/fariedabuzaid/Zotero/storage/66BJMG7A/Fletcher - 1971 - A modified Marquardt subroutine for nonlinear leas.pdf}
}

@techreport{fonseca_calibration_2017,
  title = {Calibration of {{Machine Learning Classifiers}} for {{Probability}} of {{Default Modelling}}},
  author = {Fonseca, Pedro G. and Lopes, Hugo D.},
  year = {2017},
  month = oct,
  eprint = {1710.08901},
  eprinttype = {arxiv},
  pages = {23},
  institution = {{James\hspace{0pt} \hspace{0pt}Finance\hspace{0pt} \hspace{0pt}(CrowdProcess\hspace{0pt} \hspace{0pt}Inc.)}},
  abstract = {Binary classification is highly used in credit scoring in the estimation of probability of default. The validation of such predictive models is based both on rank ability, and also on calibration (i.e. how accurately the probabilities output by the model map to the observed probabilities). In this study we cover the current best practices regarding calibration for binary classification, and explore how different approaches yield different results on real world credit scoring data. The limitations of evaluating credit scoring models using only rank ability metrics are explored. A benchmark is run on 18 real world datasets, and results compared. The calibration techniques used are Platt Scaling and Isotonic Regression. Also, different machine learning models are used: Logistic Regression, Random Forest Classifiers, and Gradient Boosting Classifiers. Results show that when the dataset is treated as a time series, the use of re-calibration with Isotonic Regression is able to improve the long term calibration better than the alternative methods. Using re-calibration, the non-parametric models are able to outperform the Logistic Regression on Brier Score Loss.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/FYF54H46/Fonseca and Lopes - 2017 - Calibration of Machine Learning Classifiers for Pr.pdf}
}

@article{forman_bochner_2003,
  title = {Bochner's {{Method}} for {{Cell Complexes}} and {{Combinatorial Ricci Curvature}}},
  author = {Forman, Robin},
  year = {2003},
  month = feb,
  journal = {Discrete and Computational Geometry},
  volume = {29},
  number = {3},
  pages = {323--374},
  issn = {0179-5376, 1432-0444},
  doi = {10.1007/s00454-002-0743-x},
  abstract = {In this paper we present a new notion of curvature for cell complexes. For each p, we define a pth combinatorial curvature function, which assigns a number to each p-cell of the complex. The curvature of a p-cell depends only on the relationships between the cell and its neighbors. In the case that p = 1, the curvature function appears to play the role for cell complexes that Ricci curvature plays for Riemannian manifolds. We begin by deriving a combinatorial analogue of Bochner's theorems, which demonstrate that there are topological restrictions to a space having a cell decomposition with everywhere positive curvature. Much of the rest of this paper is devoted to comparing the properties of the combinatorial Ricci curvature with those of its Riemannian avatar.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/49USNRSE/Forman - 2003 - Bochner's Method for Cell Complexes and Combinator.pdf}
}

@inproceedings{fortuin_bayesian_2022,
  title = {Bayesian {{Neural Network Priors Revisited}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Fortuin, Vincent and {Garriga-Alonso}, Adri{\`a} and Ober, Sebastian W. and Wenzel, Florian and Ratsch, Gunnar and Turner, Richard E. and van der Wilk, Mark and Aitchison, Laurence},
  year = {2022},
  abstract = {Isotropic Gaussian priors are the de facto standard for modern Bayesian neural network inference. However, it is unclear whether these priors accurately reflect our true beliefs about the weight...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/22BX3C79/Fortuin et al. - 2021 - Bayesian Neural Network Priors Revisited.pdf;/Users/fariedabuzaid/Zotero/storage/TYU9REKU/forum.html}
}

@inproceedings{fujimoto_offpolicy_2019,
  title = {Off-{{Policy Deep Reinforcement Learning}} without {{Exploration}}},
  booktitle = {{{arXiv}}:1812.02900 [Cs, Stat]},
  author = {Fujimoto, Scott and Meger, David and Precup, Doina},
  year = {2019},
  month = aug,
  eprint = {1812.02900},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Many practical applications of reinforcement learning constrain agents to learn from a fixed batch of data which has already been gathered, without offering further possibility for data collection. In this paper, we demonstrate that due to errors introduced by extrapolation, standard off-policy deep reinforcement learning algorithms, such as DQN and DDPG, are incapable of learning with data uncorrelated to the distribution under the current policy, making them ineffective for this fixed batch setting. We introduce a novel class of off-policy algorithms, batch-constrained reinforcement learning, which restricts the action space in order to force the agent towards behaving close to on-policy with respect to a subset of the given data. We present the first continuous control deep reinforcement learning algorithm which can learn effectively from arbitrary, fixed batch data, and empirically demonstrate the quality of its behavior in several tasks.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/WLLYKUJY/Fujimoto et al. - 2019 - Off-Policy Deep Reinforcement Learning without Exp.pdf}
}

@article{fuks_limitations_2020,
  title = {Limitations of Physics Informed Machine Learning for Nonlinear Two-Phase Transport Inn Porous Media},
  author = {Fuks, Olga and Tchelepi, Hamdi A.},
  year = {2020},
  journal = {Journal of Machine Learning for Modeling and Computing},
  volume = {1},
  number = {1},
  publisher = {{Begel House Inc.}},
  issn = {2689-3967, 2689-3975},
  doi = {10.1615/.2020033905},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/R2S6U6G9/Fuks and Tchelepi - 2020 - Limitations of physics informed machine learning f.pdf}
}

@article{gal_dropout_2016,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = oct,
  eprint = {1506.02142},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/ARXIV.1506.02142},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/N9FDT5D3/Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf}
}

@inproceedings{galkin_nodepiece_2022,
  title = {{{NodePiece}}: {{Compositional}} and {{Parameter-Efficient Representations}} of {{Large Knowledge Graphs}}},
  shorttitle = {{{NodePiece}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Galkin, Mikhail and Denis, Etienne and Wu, Jiapeng and Hamilton, William L.},
  year = {2022},
  abstract = {Conventional representation learning algorithms for knowledge graphs (KG) map each entity to a unique embedding vector.  Such a shallow lookup results in a linear growth of memory consumption for...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/A894B5MP/Galkin et al. - 2021 - NodePiece Compositional and Parameter-Efficient R.pdf;/Users/fariedabuzaid/Zotero/storage/STFP6FMM/forum.html}
}

@inproceedings{gao_abinitio_2022,
  title = {Ab-{{Initio Potential Energy Surfaces}} by {{Pairing GNNs}} with {{Neural Wave Functions}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Gao, Nicholas and G{\"u}nnemann, Stephan},
  year = {2022},
  abstract = {Solving the Schr\"odinger equation is key to many quantum mechanical properties. However, an analytical solution is only tractable for single-electron systems. Recently, neural networks succeeded at...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/R5T72D5I/Gao and Günnemann - 2022 - Ab-Initio Potential Energy Surfaces by Pairing GNN.pdf}
}

@inproceedings{gao_amortized_2022,
  title = {Amortized {{Tree Generation}} for {{Bottom-up Synthesis Planning}} and {{Synthesizable Molecular Design}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Gao, Wenhao and Mercado, Roc{\'i}o and Coley, Connor W.},
  year = {2022},
  abstract = {Molecular design and synthesis planning are two critical steps in the process of molecular discovery that we propose to formulate as a single shared task of conditional synthetic pathway...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/9F4AWBHX/Gao et al. - 2022 - Amortized Tree Generation for Bottom-up Synthesis .pdf}
}

@inproceedings{gao_gradient_2022,
  title = {Gradient {{Importance Learning}} for {{Incomplete Observations}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR2022}})},
  author = {Gao, Qitong and Wang, Dong and Amason, Joshua David and Yuan, Siyang and Tao, Chenyang and Henao, Ricardo and Hadziahmetovic, Majda and Carin, Lawrence and Pajic, Miroslav},
  year = {2022},
  abstract = {Though recent works have developed methods that can generate estimates (or imputations) of the missing entries in a dataset to facilitate downstream analysis, most depend on assumptions that may...},
  langid = {english},
  annotation = {video: https://iclr.cc/virtual/2022/poster/6859 slides: https://iclr.cc/media/iclr-2022/Slides/6859.pdf},
  file = {/Users/fariedabuzaid/Zotero/storage/Z7NVIJXE/Gao et al. - 2021 - Gradient Importance Learning for Incomplete Observ.pdf}
}

@inproceedings{gasteiger_diffusion_2019,
  title = {Diffusion {{Improves Graph Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gasteiger, Johannes and {Wei{\ss} enberger}, Stefan and G{\"u}nnemann, Stephan},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Graph convolution is the core of most Graph Neural Networks (GNNs) and usually approximated by message passing between direct (one-hop) neighbors. In this work, we remove the restriction of using only the direct neighbors by introducing a powerful, yet spatially localized graph convolution: Graph diffusion convolution (GDC). GDC leverages generalized graph diffusion, examples of which are the heat kernel and personalized PageRank. It alleviates the problem of noisy and often arbitrarily defined edges in real graphs. We show that GDC is closely related to spectral-based models and thus combines the strengths of both spatial (message passing) and spectral methods. We demonstrate that replacing message passing with graph diffusion convolution consistently leads to significant performance improvements across a wide range of models on both supervised and unsupervised tasks and a variety of datasets. Furthermore, GDC is not limited to GNNs but can trivially be combined with any graph-based model or algorithm (e.g. spectral clustering) without requiring any changes to the latter or affecting its computational complexity. Our implementation is available online.},
  file = {/Users/fariedabuzaid/Zotero/storage/JS8A995K/Gasteiger et al. - 2019 - Diffusion Improves Graph Learning.pdf}
}

@article{gebru_datasheets_2021,
  title = {Datasheets for Datasets},
  author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Vaughan, Jennifer Wortman and Wallach, Hanna and III, Hal Daum{\'e} and Crawford, Kate},
  year = {2021},
  month = nov,
  journal = {Communications of the ACM},
  volume = {64},
  number = {12},
  eprint = {1803.09010},
  eprinttype = {arxiv},
  pages = {86--92},
  issn = {0001-0782},
  doi = {10.1145/3458723},
  abstract = {Documentation to facilitate communication between dataset creators and consumers.},
  archiveprefix = {arXiv},
  annotation = {video: https://youtu.be/R7s7\_T4yXak notion: https://www.notion.so/appliedaiinitiative/Data-sheets-for-datasets-5a36d58691794cf6b49241d75426f97b},
  file = {/Users/fariedabuzaid/Zotero/storage/6WHWBWYK/Print version - Gebru et al. - 2021 - Datasheets for datasets.pdf;/Users/fariedabuzaid/Zotero/storage/RYJVMEIX/Gebru et al. - 2021 - Datasheets for datasets.pdf}
}

@inproceedings{geerts_expressiveness_2022,
  title = {Expressiveness and {{Approximation Properties}} of {{Graph Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Geerts, Floris and Reutter, Juan L.},
  year = {2022},
  abstract = {Characterizing the separation power of graph neural networks (GNNs) provides an understanding of their limitations for graph learning tasks. Results regarding separation power are, however, usually...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/ZYANVNXQ/Geerts und Reutter - 2021 - Expressiveness and Approximation Properties of Gra.pdf;/Users/fariedabuzaid/Zotero/storage/MH7AB5I5/forum.html}
}

@misc{ghildyal_attacking_2021,
  title = {Attacking {{Perceptual Similarity Metrics}}},
  author = {Ghildyal, Abhijay and Liu, Feng},
  year = {2021},
  month = sep,
  abstract = {Perceptual similarity metrics have progressively become more correlated with human judgments on perceptual similarity; however, despite recent advances, the addition of an imperceptible distortion...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/PFEXMSSH/Ghildyal and Liu - 2021 - Attacking Perceptual Similarity Metrics.pdf}
}

@inproceedings{ghorbani_data_2019,
  title = {Data {{Shapley}}: {{Equitable Valuation}} of {{Data}} for {{Machine Learning}}},
  shorttitle = {Data {{Shapley}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Ghorbani, Amirata and Zou, James},
  year = {2019},
  month = may,
  eprint = {1904.02868},
  eprinttype = {arxiv},
  pages = {2242--2251},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare...},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/IHVQ8D9P/Suppl. - Ghorbani and Zou - 2019 - Data Shapley Equitable Valuation of Data for Mach.pdf;/Users/fariedabuzaid/Zotero/storage/QIJURDZ6/Ghorbani and Zou - 2019 - Data Shapley Equitable Valuation of Data for Mach.pdf}
}

@article{ghorbani_data_2021,
  title = {Data {{Shapley Valuation}} for {{Efficient Batch Active Learning}}},
  author = {Ghorbani, Amirata and Zou, James and Esteva, Andre},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.08312 [cs, stat]},
  eprint = {2104.08312},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Annotating the right set of data amongst all available data points is a key challenge in many machine learning applications. Batch active learning is a popular approach to address this, in which batches of unlabeled data points are selected for annotation, while an underlying learning algorithm gets subsequently updated. Increasingly larger batches are particularly appealing in settings where data can be annotated in parallel, and model training is computationally expensive. A key challenge here is scale - typical active learning methods rely on diversity techniques, which select a diverse set of data points to annotate, from an unlabeled pool. In this work, we introduce Active Data Shapley (ADS) -- a filtering layer for batch active learning that significantly increases the efficiency of active learning by pre-selecting, using a linear time computation, the highest-value points from an unlabeled dataset. Using the notion of the Shapley value of data, our method estimates the value of unlabeled data points with regards to the prediction task at hand. We show that ADS is particularly effective when the pool of unlabeled data exhibits real-world caveats: noise, heterogeneity, and domain shift. We run experiments demonstrating that when ADS is used to pre-select the highest-ranking portion of an unlabeled dataset, the efficiency of state-of-the-art batch active learning methods increases by an average factor of 6x, while preserving performance effectiveness.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/3T76LBBA/Ghorbani et al. - 2021 - Data Shapley Valuation for Efficient Batch Active .pdf}
}

@inproceedings{ghorbani_distributional_2020,
  title = {A {{Distributional Framework For Data Valuation}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Ghorbani, Amirata and Kim, Michael and Zou, James},
  year = {2020},
  month = nov,
  pages = {3535--3544},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Shapley value is a classic notion from game theory, historically used to quantify the contributions of individuals within groups, and more recently applied to assign values to data points when trai...},
  langid = {english},
  annotation = {video: https://icml.cc/virtual/2020/poster/6637},
  file = {/Users/fariedabuzaid/Zotero/storage/233H8ECY/Ghorbani et al. - A Distributional Framework For Data Valuation Supp.pdf;/Users/fariedabuzaid/Zotero/storage/IJBREC4B/Ghorbani et al. - 2020 - A Distributional Framework For Data Valuation.pdf}
}

@inproceedings{glockler_variational_2022,
  title = {Variational Methods for Simulation-Based Inference},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Gl{\"o}ckler, Manuel and Deistler, Michael and Macke, Jakob H.},
  year = {2022},
  abstract = {We present Sequential Neural Variational Inference (SNVI), an approach to perform Bayesian inference in models with intractable likelihoods. SNVI combines likelihood-estimation (or...},
  langid = {english},
  annotation = {video: https://iclr.cc/virtual/2022/spotlight/6749},
  file = {/Users/fariedabuzaid/Zotero/storage/IHT345ET/Glöckler et al. - 2021 - Variational methods for simulation-based inference.pdf}
}

@article{gneiting_regression_2021,
  title = {Regression {{Diagnostics}} Meets {{Forecast Evaluation}}: {{Conditional Calibration}}, {{Reliability Diagrams}}, and {{Coefficient}} of {{Determination}}},
  shorttitle = {Regression {{Diagnostics}} Meets {{Forecast Evaluation}}},
  author = {Gneiting, Tilmann and Resin, Johannes},
  year = {2021},
  month = aug,
  eprint = {2108.03210},
  eprinttype = {arxiv},
  abstract = {Model diagnostics and forecast evaluation are two sides of the same coin. A common principle is that fitted or predicted distributions ought to be calibrated or reliable, ideally in the sense of auto-calibration, where the outcome is a random draw from the posited distribution. For binary outcomes, this is a universal concept of reliability. For general real-valued outcomes, practitioners and theoreticians have relied on weaker, unconditional notions, most prominently probabilistic calibration, which corresponds to the uniformity of the probability integral transform. Conditional concepts give rise to hierarchies of calibration. In a nutshell, a predictive distribution is conditionally T-calibrated if it can be taken at face value in terms of the functional T. Whenever T is defined via an identification function \textemdash{} as in the cases of threshold (non) exceedance probabilities, quantiles, expectiles, and moments \textemdash{} auto-calibration implies T-calibration. However, the notion of T-calibration also applies to stand-alone point forecasts or regression output in terms of the functional T. We introduce population versions of T-reliability diagrams and revisit a score decomposition into measures of miscalibration (MCB), discrimination (DSC), and uncertainty (UNC). In empirical settings, stable and efficient estimators of T-reliability diagrams and score components arise via nonparametric isotonic regression and the pool-adjacent-violators algorithm. For in-sample model diagnostics, we propose a universal coefficient of determination,},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/Z829D4BU/Gneiting and Resin - 2021 - Regression Diagnostics meets Forecast Evaluation .pdf}
}

@inproceedings{godwin_simple_2022,
  title = {Simple {{GNN Regularisation}} for {{3D Molecular Property Prediction}} and {{Beyond}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} (2022)},
  author = {Godwin, Jonathan and Schaarschmidt, Michael and Gaunt, Alexander L. and {Sanchez-Gonzalez}, Alvaro and Rubanova, Yulia and Veli{\v c}kovi{\'c}, Petar and Kirkpatrick, James and Battaglia, Peter},
  year = {2022},
  abstract = {In this paper we show that simple noisy regularisation can be an effective way to address oversmoothing. We first argue that regularisers ad-dressing oversmoothing should both penalise node latent...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/A36GUUTG/Godwin et al. - 2021 - Simple GNN Regularisation for 3D Molecular Propert.pdf;/Users/fariedabuzaid/Zotero/storage/QUZN7J5R/forum.html}
}

@article{gokcesu_optimally_2021,
  title = {Optimally {{Efficient Sequential Calibration}} of {{Binary Classifiers}} to {{Minimize Classification Error}}},
  author = {Gokcesu, Kaan and Gokcesu, Hakan},
  year = {2021},
  month = aug,
  journal = {arXiv:2108.08780 [cs]},
  eprint = {2108.08780},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this work, we aim to calibrate the score outputs of an estimator for the binary classification problem by finding an 'optimal' mapping to class probabilities, where the 'optimal' mapping is in the sense that minimizes the classification error (or equivalently, maximizes the accuracy). We show that for the given target variables and the score outputs of an estimator, an 'optimal' soft mapping, which monotonically maps the score values to probabilities, is a hard mapping that maps the score values to \$0\$ and \$1\$. We show that for class weighted (where the accuracy for one class is more important) and sample weighted (where the samples' accurate classifications are not equally important) errors, or even general linear losses; this hard mapping characteristic is preserved. We propose a sequential recursive merger approach, which produces an 'optimal' hard mapping (for the observed samples so far) sequentially with each incoming new sample. Our approach has a logarithmic in sample size time complexity, which is optimally efficient.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/IK9SEK8K/Gokcesu and Gokcesu - 2021 - Optimally Efficient Sequential Calibration of Bina.pdf}
}

@article{goodfellow_explaining_2014,
  title = {Explaining and Harnessing Adversarial Examples},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  year = {2014},
  month = dec,
  journal = {arXiv:1412.6572 [cs, stat]},
  eprint = {1412.6572},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {11},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00269},
  file = {/Users/fariedabuzaid/Zotero/storage/H8AZTFWU/Goodfellow et al. - 2014 - Explaining and Harnessing Adversarial Examples.pdf}
}

@misc{goodfellow_explaining_2015,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  year = {2015},
  month = mar,
  number = {arXiv:1412.6572},
  eprint = {1412.6572},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/fariedabuzaid/Zotero/storage/N99JWAQG/Goodfellow et al. - 2015 - Explaining and Harnessing Adversarial Examples.pdf;/Users/fariedabuzaid/Zotero/storage/QQHTFNNU/1412.html}
}

@article{gosiewska_not_2019,
  title = {Do {{Not Trust Additive Explanations}}},
  author = {Gosiewska, Alicja and Biecek, Przemyslaw},
  year = {2019},
  month = mar,
  abstract = {Explainable Artificial Intelligence (XAI)has received a great deal of attention recently. Explainability is being presented as a remedy for the distrust of complex and opaque models. Model agnostic methods such as LIME, SHAP, or Break Down promise instance-level interpretability for any complex machine learning model. But how faithful are these additive explanations? Can we rely on additive explanations for non-additive models? In this paper, we (1) examine the behavior of the most popular instance-level explanations under the presence of interactions, (2) introduce a new method that detects interactions for instance-level explanations, (3) perform a large scale benchmark to see how frequently additive explanations may be misleading.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/AXYCGYIJ/Gosiewska and Biecek - 2019 - Do Not Trust Additive Explanations.pdf}
}

@article{green_macest_2021,
  title = {{{MACEst}}: {{The}} Reliable and Trustworthy {{Model Agnostic Confidence Estimator}}},
  shorttitle = {{{MACEst}}},
  author = {Green, Rhys and Rowe, Matthew and Polleri, Alberto},
  year = {2021},
  month = sep,
  journal = {arXiv:2109.01531 [cs]},
  eprint = {2109.01531},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Reliable Confidence Estimates are hugely important for any machine learning model to be truly useful. In this paper, we argue that any confidence estimates based upon standard machine learning point prediction algorithms are fundamentally flawed and under situations with a large amount of epistemic uncertainty are likely to be untrustworthy. To address these issues, we present MACEst, a Model Agnostic Confidence Estimator, which provides reliable and trustworthy confidence estimates. The algorithm differs from current methods by estimating confidence independently as a local quantity which explicitly accounts for both aleatoric and epistemic uncertainty. This approach differs from standard calibration methods that use a global point prediction model as a starting point for the confidence estimate.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/A38SEPZM/Green et al. - 2021 - MACEst The reliable and trustworthy Model Agnosti.pdf}
}

@article{griffiths_mere_2007,
  title = {From Mere Coincidences to Meaningful Discoveries},
  author = {Griffiths, Thomas L. and Tenenbaum, Joshua B.},
  year = {2007},
  month = may,
  journal = {Cognition},
  volume = {103},
  number = {2},
  pages = {180--226},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2006.03.004},
  abstract = {People's reactions to coincidences are often cited as an illustration of the irrationality of human reasoning about chance. We argue that coincidences may be better understood in terms of rational statistical inference, based on their functional role in processes of causal discovery and theory revision. We present a formal definition of coincidences in the context of a Bayesian framework for causal induction: a coincidence is an event that provides support for an alternative to a currently favored causal theory, but not necessarily enough support to accept that alternative in light of its low prior probability. We test the qualitative and quantitative predictions of this account through a series of experiments that examine the transition from coincidence to evidence, the correspondence between the strength of coincidences and the statistical support for causal structure, and the relationship between causes and coincidences. Our results indicate that people can accurately assess the strength of coincidences, suggesting that irrational conclusions drawn from coincidences are the consequence of overestimation of the plausibility of novel causal forces. We discuss the implications of our account for understanding the role of coincidences in theory change.},
  langid = {english},
  pmid = {16678145}
}

@inproceedings{grill_bootstrap_2020,
  title = {Bootstrap Your Own Latent: {{A}} New Approach to Self-Supervised {{Learning}}},
  shorttitle = {Bootstrap Your Own Latent},
  booktitle = {Advances in {{Neural Information Processing Systems}} 33},
  author = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, R{\'e}mi and Valko, Michal},
  year = {2020},
  month = sep,
  eprint = {2006.07733},
  eprinttype = {arxiv},
  address = {{Virtual event}},
  abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3\textbackslash\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6\textbackslash\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {notion: https://www.notion.so/appliedaiinitiative/Representation-learning-with-BYOL-and-SimSiam-77d9edf4170747908bbec4697566c771 post: https://community.appliedai.de/topics/27304/topic\_feed\_posts/1222266},
  file = {/Users/fariedabuzaid/Zotero/storage/UHVQF9IA/Grill et al. - 2020 - Bootstrap your own latent A new approach to self-.pdf}
}

@article{grohs_proof_2018,
  title = {A Proof That Artificial Neural Networks Overcome the Curse of Dimensionality in the Numerical Approximation of {{Black-Scholes}} Partial Differential Equations},
  author = {Grohs, Philipp and Hornung, Fabian and Jentzen, Arnulf and {von Wurstemberger}, Philippe},
  year = {2018},
  month = sep,
  journal = {arXiv:1809.02362 [cs, math, q-fin]},
  eprint = {1809.02362},
  eprinttype = {arxiv},
  primaryclass = {cs, math, q-fin},
  abstract = {Artificial neural networks (ANNs) have very successfully been used in numerical simulations for a series of computational problems ranging from image classification/image recognition, speech recognition, time series analysis, game intelligence, and computational advertising to numerical approximations of partial differential equations (PDEs). Such numerical simulations suggest that ANNs have the capacity to very efficiently approximate high-dimensional functions and, especially, such numerical simulations indicate that ANNs seem to admit the fundamental power to overcome the curse of dimensionality when approximating the high-dimensional functions appearing in the above named computational problems. There are also a series of rigorous mathematical approximation results for ANNs in the scientific literature. Some of these mathematical results prove convergence without convergence rates and some of these mathematical results even rigorously establish convergence rates but there are only a few special cases where mathematical results can rigorously explain the empirical success of ANNs when approximating high-dimensional functions. The key contribution of this article is to disclose that ANNs can efficiently approximate high-dimensional functions in the case of numerical approximations of Black-Scholes PDEs. More precisely, this work reveals that the number of required parameters of an ANN to approximate the solution of the Black-Scholes PDE grows at most polynomially in both the reciprocal of the prescribed approximation accuracy \$\textbackslash varepsilon {$>$} 0\$ and the PDE dimension \$d \textbackslash in \textbackslash mathbb\{N\}\$ and we thereby prove, for the first time, that ANNs do indeed overcome the curse of dimensionality in the numerical approximation of Black-Scholes PDEs.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/R65TX2L3/Grohs et al. - 2018 - A proof that artificial neural networks overcome t.pdf}
}

@inproceedings{gu_levenshtein_2019,
  title = {Levenshtein {{Transformer}}},
  booktitle = {{{NeurIPS}}},
  author = {Gu, Jiatao and Wang, Changhan and Zhao, Jake},
  year = {2019},
  abstract = {Modern neural sequence generation models are built to either generate tokens step-by-step from scratch or (iteratively) modify a sequence of tokens bounded by a fixed length. In this work, we develop Levenshtein Transformer, a new partially autoregressive model devised for more flexible and amenable sequence generation. Unlike previous approaches, the atomic operations of our model are insertion and deletion. The combination of them facilitates not only generation but also sequence refinement allowing dynamic length changes. We also propose a set of new training techniques dedicated at them, effectively exploiting one as the other's learning signal thanks to their complementary nature. Experiments applying the proposed model achieve comparable performance but much-improved efficiency on both generation (e.g. machine translation, text summarization) and refinement tasks (e.g. automatic post-editing). We further confirm the flexibility of our model by showing a Levenshtein Transformer trained by machine translation can straightforwardly be used for automatic post-editing.},
  file = {/Users/fariedabuzaid/Zotero/storage/CURXLF84/Gu et al. - 2019 - Levenshtein Transformer.pdf}
}

@article{guan_test_2018,
  title = {Test {{Error Estimation}} after {{Model Selection Using Validation Error}}},
  author = {Guan, Leying},
  year = {2018},
  month = feb,
  journal = {arXiv:1801.02817 [stat]},
  eprint = {1801.02817},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {When performing supervised learning with the model selected using validation error from sample splitting and cross validation, the minimum value of the validation error can be biased downward. We propose two simple methods that use the errors produced in the validating step to estimate the test error after model selection, and we focus on the situations where we select the model by minimizing the validation error and the randomized validation error. Our methods do not require model refitting, and the additional computational cost is negligible. In the setting of sample splitting, we show that, the proposed test error estimates have biases of size \$o(1/\textbackslash sqrt\{n\})\$ under suitable assumptions. We also propose to use the bootstrap to construct confidence intervals for the test error based on this result. We apply our proposed methods to a number of simulations and examine their performance.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/P2PWUKZ6/DVL2GTZS.pdf}
}

@inproceedings{guha_robust_2016,
  title = {Robust {{Random Cut Forest Based Anomaly Detection}} on {{Streams}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Guha, Sudipto and Mishra, Nina and Roy, Gourav and Schrijvers, Okke},
  year = {2016},
  month = jun,
  pages = {2712--2721},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {In this paper we focus on the anomaly detection problem for dynamic data streams through the lens of random cut forests. We investigate a robust random cut data structure that can be used as a sket...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/HAET4KDR/Guha et al. - 2016 - Robust Random Cut Forest Based Anomaly Detection o.pdf}
}

@inproceedings{guo_calibration_2017,
  title = {On {{Calibration}} of {{Modern Neural Networks}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  year = {2017},
  month = jul,
  pages = {1321--1330},
  address = {{Sydney, Australia}},
  abstract = {Confidence calibration \textendash{} the problem of predicting probability estimates representative of the true correctness likelihood \textendash{} is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-ofthe-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling \textendash{} a singleparameter variant of Platt Scaling \textendash{} is surprisingly effective at calibrating predictions.},
  langid = {english},
  annotation = {citecount: 00253},
  file = {/Users/fariedabuzaid/Zotero/storage/ZHHMTDUS/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf}
}

@inproceedings{guo_dataefficient_2022,
  title = {Data-{{Efficient Graph Grammar Learning}} for {{Molecular Generation}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Guo, Minghao and Thost, Veronika and Li, Beichen and Das, Payel and Chen, Jie and Matusik, Wojciech},
  year = {2022},
  abstract = {The problem of molecular generation has received significant attention recently. Existing methods are typically based on deep neural networks and require training on large datasets with tens of...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/PWMZZM5K/Guo et al. - 2021 - Data-Efficient Graph Grammar Learning for Molecula.pdf;/Users/fariedabuzaid/Zotero/storage/CGDIRNQ2/forum.html}
}

@article{guo_fastif_2020,
  title = {{{FastIF}}: {{Scalable Influence Functions}} for {{Efficient Model Interpretation}} and {{Debugging}}},
  shorttitle = {{{FastIF}}},
  author = {Guo, Han and Rajani, Nazneen Fatema and Hase, Peter and Bansal, Mohit and Xiong, Caiming},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.15781 [cs]},
  eprint = {2012.15781},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Influence functions approximate the 'influences' of training data-points for test predictions and have a wide variety of applications. Despite the popularity, their computational cost does not scale well with model and training data size. We present FastIF, a set of simple modifications to influence functions that significantly improves their run-time. We use k-Nearest Neighbors (kNN) to narrow the search space down to a subset of good candidate data points, identify the configurations that best balance the speed-quality trade-off in estimating the inverse Hessian-vector product, and introduce a fast parallel variant. Our proposed method achieves about 80x speedup while being highly correlated with the original influence values. With the availability of the fast influence functions, we demonstrate their usefulness in four applications. First, we examine whether influential data-points can 'explain' test time behavior using the framework of simulatability. Second, we visualize the influence interactions between training and test data-points. Third, we show that we can correct model errors by additional fine-tuning on certain influential data-points, improving the accuracy of a trained MNLI model by 2.6\% on the HANS challenge set using a small number of gradient updates. Finally, we experiment with a data-augmentation setup where we use influence functions to search for new data-points unseen during training to improve model performance. Overall, our fast influence functions can be efficiently applied to large models and datasets, and our experiments demonstrate the potential of influence functions in model interpretation and correcting model errors. Code is available at https://github.com/salesforce/fast-influence-functions},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/35GRAIPA/Guo et al. - 2020 - FastIF Scalable Influence Functions for Efficient.pdf}
}

@article{guo_survey_2020,
  title = {A {{Survey}} of {{Learning Causality}} with {{Data}}: {{Problems}} and {{Methods}}},
  shorttitle = {A {{Survey}} of {{Learning Causality}} with {{Data}}},
  author = {Guo, Ruocheng and Cheng, Lu and Li, Jundong and Hahn, P. Richard and Liu, Huan},
  year = {2020},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {53},
  number = {4},
  pages = {75:1--75:37},
  issn = {0360-0300},
  doi = {10.1145/3397269},
  abstract = {This work considers the question of how convenient access to copious data impacts our ability to learn causal effects and relations. In what ways is learning causality in the era of big data different from\textemdash or the same as\textemdash the traditional one? To answer this question, this survey provides a comprehensive and structured review of both traditional and frontier methods in learning causality and relations along with the connections between causality and machine learning. This work points out on a case-by-case basis how big data facilitates, complicates, or motivates each approach.},
  file = {/Users/fariedabuzaid/Zotero/storage/IPBUMM97/YIK9LXA6.pdf}
}

@inproceedings{gupta_toplabel_2022,
  title = {Top-Label Calibration and Multiclass-to-Binary Reductions},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Gupta, Chirag and Ramdas, Aaditya},
  year = {2022},
  abstract = {We investigate the relationship between commonly considered notions of multiclass calibration and the calibration algorithms used to achieve these notions, leading to two broad contributions. First...},
  langid = {english},
  annotation = {video: https://iclr.cc/virtual/2022/poster/6278},
  file = {/Users/fariedabuzaid/Zotero/storage/P8QMUBWW/Gupta and Ramdas - 2021 - Top-label calibration and multiclass-to-binary red.pdf}
}

@article{haarnoja_soft_2018,
  title = {Soft {{Actor-Critic Algorithms}} and {{Applications}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.05905 [cs, stat]},
  eprint = {1812.05905},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00006},
  file = {/Users/fariedabuzaid/Zotero/storage/VM9E8LTW/Haarnoja et al. - 2018 - Soft Actor-Critic Algorithms and Applications.pdf}
}

@article{haarnoja_soft_2018a,
  title = {Soft {{Actor-Critic}}: {{Off-Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor-Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = jan,
  journal = {arXiv:1801.01290 [cs, stat]},
  eprint = {1801.01290},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy - that is, succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00032},
  file = {/Users/fariedabuzaid/Zotero/storage/3SJEV6BQ/Haarnoja et al. - 2018 - Soft Actor-Critic Off-Policy Maximum Entropy Deep.pdf}
}

@article{hampel_influence_1974,
  title = {The {{Influence Curve}} and {{Its Role}} in {{Robust Estimation}}},
  author = {Hampel, Frank R.},
  year = {1974},
  journal = {Journal of the American Statistical Association},
  volume = {69},
  number = {346},
  pages = {383--393},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2285666},
  abstract = {This paper treats essentially the first derivative of an estimator viewed as functional and the ways in which it can be used to study local robustness properties. A theory of robust estimation "near" strict parametric models is briefly sketched and applied to some classical situations. Relations between von Mises functionals, the jackknife and U-statistics are indicated. A number of classical and new estimators are discussed, including trimmed and Winsorized means, Huber-estimators, and more generally maximum likelihood and M-estimators. Finally, a table with some numerical robustness properties is given.}
}

@book{hampel_robust_2005,
  title = {Robust {{Statistics}}: {{The Approach Based}} on {{Influence Functions}}},
  shorttitle = {Robust {{Statistics}}},
  author = {Hampel, Frank R. and Ronchetti, Elvezio M. and Rousseeuw, Peter J. and Stahel, Werner A.},
  year = {2005},
  month = apr,
  series = {Wiley {{Series}} in Probability and Statistics},
  edition = {1st edition},
  publisher = {{Wiley-Interscience}},
  address = {{New York}},
  doi = {10.1002/9781118186435},
  abstract = {Introducing concepts, theory, and applications, Robust Statistics is accessible to a broad audience, avoiding allusions to high-powered mathematics while emphasizing ideas, heuristics, and background. The text covers the approach based on the influence function (the effect of an outlier on an estimater, for example) and related notions such as the breakdown point. It also treats the change-of-variance function, fundamental concepts and results in the framework of estimation of a single parameter, and applications to estimation of covariance matrices and regression parameters.},
  isbn = {978-0-471-73577-9},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/2F4DQJAH/Hampel et al. - 2005 - Robust Statistics The Approach Based on Influence.pdf}
}

@inproceedings{han_neural_2022,
  title = {Neural {{Collapse Under MSE Loss}}: {{Proximity}} to and {{Dynamics}} on the {{Central Path}}},
  shorttitle = {Neural {{Collapse Under MSE Loss}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR2022}})},
  author = {Han, X. Y. and Papyan, Vardan and Donoho, David L.},
  year = {2022},
  address = {{Virtual event}},
  abstract = {The recently discovered Neural Collapse (NC) phenomenon occurs pervasively in today's deep net training paradigm of driving cross-entropy (CE) loss towards zero. During NC, last-layer features...},
  langid = {english},
  annotation = {video: https://iclr.cc/virtual/2022/oral/6353},
  file = {/Users/fariedabuzaid/Zotero/storage/XPJ9MAAV/Han et al. - 2021 - Neural Collapse Under MSE Loss Proximity to and D.pdf}
}

@article{han_solving_2018,
  title = {Solving High-Dimensional Partial Differential Equations Using Deep Learning},
  author = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
  year = {2018},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {34},
  eprint = {1707.02568},
  eprinttype = {arxiv},
  pages = {8505--8510},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1718942115},
  abstract = {Developing algorithms for solving high-dimensional partial differential equations (PDEs) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the "curse of dimensionality". This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic PDEs. To this end, the PDEs are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black-Scholes equation, the Hamilton-Jacobi-Bellman equation, and the Allen-Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up new possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their inter-relationships.},
  archiveprefix = {arXiv},
  copyright = {\textcopyright{} 2018 . http://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/D78XUY29/Han et al. - 2018 - Solving high-dimensional partial differential equa.pdf;/Users/fariedabuzaid/Zotero/storage/XZ56VT8X/Han et al. - 2018 - Solving high-dimensional partial differential equa.pdf}
}

@misc{harang_sorel20m_2020,
  type = {[Cs]},
  title = {{{SOREL-20M}}: {{A Large Scale Benchmark Dataset}} for {{Malicious PE Detection}}},
  shorttitle = {{{SOREL-20M}}},
  author = {Harang, Richard and Rudd, Ethan M.},
  year = {2020},
  month = dec,
  number = {2012.07634},
  eprint = {2012.07634},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {In this paper we describe the SOREL-20M (Sophos/ReversingLabs-20 Million) dataset: a large-scale dataset consisting of nearly 20 million files with pre-extracted features and metadata, high-quality labels derived from multiple sources, information about vendor detections of the malware samples at the time of collection, and additional ``tags'' related to each malware sample to serve as additional targets. In addition to features and metadata, we also provide approximately 10 million ``disarmed'' malware samples -- samples with both the optional\textbackslash\_headers.subsystem and file\textbackslash\_header.machine flags set to zero -- that may be used for further exploration of features and detection strategies. We also provide Python code to interact with the data and features, as well as baseline neural network and gradient boosted decision tree models and their results, with full training and evaluation code, to serve as a starting point for further experimentation.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/J5XY544V/Harang and Rudd - 2020 - SOREL-20M A Large Scale Benchmark Dataset for Mal.pdf}
}

@article{hariri_extended_2019,
  title = {Extended {{Isolation Forest}}},
  author = {Hariri, Sahand and Kind, Matias Carrasco and Brunner, Robert J.},
  year = {2019},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  eprint = {1811.02141},
  eprinttype = {arxiv},
  pages = {1--1},
  issn = {1041-4347, 1558-2191, 2326-3865},
  doi = {10.1109/TKDE.2019.2947676},
  abstract = {We present an extension to the model-free anomaly detection algorithm, Isolation Forest. This extension, named Extended Isolation Forest (EIF), resolves issues with assignment of anomaly score to given data points. We motivate the problem using heat maps for anomaly scores. These maps suffer from artifacts generated by the criteria for branching operation of the binary tree. We explain this problem in detail and demonstrate the mechanism by which it occurs visually. We then propose two different approaches for improving the situation. First we propose transforming the data randomly before creation of each tree, which results in averaging out the bias. Second, which is the preferred way, is to allow the slicing of the data to use hyperplanes with random slopes. This approach results in remedying the artifact seen in the anomaly score heat maps. We show that the robustness of the algorithm is much improved using this method by looking at the variance of scores of data points distributed along constant level sets. We report AUROC and AUPRC for our synthetic datasets, along with real-world benchmark datasets. We find no appreciable difference in the rate of convergence nor in computation time between the standard Isolation Forest and EIF.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00000},
  file = {/Users/fariedabuzaid/Zotero/storage/C6XZNMRC/Hariri et al. - 2019 - Extended Isolation Forest.pdf}
}

@inproceedings{harley_evaluation_2015,
  title = {Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval},
  booktitle = {2015 13th {{International Conference}} on {{Document Analysis}} and {{Recognition}} ({{ICDAR}})},
  author = {Harley, Adam W. and Ufkes, Alex and Derpanis, Konstantinos G.},
  year = {2015},
  month = aug,
  pages = {991--995},
  publisher = {{IEEE}},
  address = {{Tunis, Tunisia}},
  doi = {10.1109/ICDAR.2015.7333910},
  abstract = {This paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular handcrafted alternatives. Extensive experiments show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories.},
  isbn = {978-1-4799-1805-8},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/CCJ7JY3T/Harley et al. - 2015 - Evaluation of deep convolutional nets for document.pdf}
}

@inproceedings{harutyunyan_estimating_2020,
  title = {Estimating Informativeness of Samples with {{Smooth Unique Information}}},
  booktitle = {Proceedings of the {{Ninth International Conference}} on {{Learning Representations}}},
  author = {Harutyunyan, Hrayr and Achille, Alessandro and Paolini, Giovanni and Majumder, Orchid and Ravichandran, Avinash and Bhotika, Rahul and Soatto, Stefano},
  year = {2020},
  month = sep,
  address = {{Virtual conference}},
  abstract = {We define a notion of information that an individual sample provides to the training of a neural network, and we specialize it to measure both how much a sample informs the final weights and how...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/Z5HRHTR9/Harutyunyan et al. - 2020 - Estimating informativeness of samples with Smooth .pdf}
}

@inproceedings{hataya_faster_2020,
  title = {Faster {{AutoAugment}}: {{Learning Augmentation Strategies Using Backpropagation}}},
  shorttitle = {Faster {{AutoAugment}}},
  booktitle = {16th {{European}} Conference on Computer Vision ({{ECCV}} 2020)},
  author = {Hataya, Ryuichiro and Zdenek, Jan and Yoshizoe, Kazuki and Nakayama, Hideki},
  editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {XXV},
  eprint = {1911.06987},
  eprinttype = {arxiv},
  pages = {1--16},
  publisher = {{Springer International Publishing}},
  address = {{Glasgow}},
  doi = {10.1007/978-3-030-58595-2_1},
  abstract = {Data augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in image recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms outperform hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper, we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as a differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented and original data, which can be differentiated. We show that our method, Faster AutoAugment, achieves significantly faster searching than prior methods without a performance drop.},
  archiveprefix = {arXiv},
  isbn = {978-3-030-58595-2},
  langid = {english},
  annotation = {notion: https://www.notion.so/appliedaiinitiative/Faster-AutoAugment-Learning-Augmentation-Strategies-using-Backpropagation-7ec15a4b08534d4f99284385febac105 post: https://community.appliedai.de/topics/27304/topic\_feed\_posts/1221678},
  file = {/Users/fariedabuzaid/Zotero/storage/8ZW2RX52/Hataya et al. - 2020 - Faster AutoAugment Learning Augmentation Strategi.pdf}
}

@techreport{he_deep_2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  eprint = {1512.03385},
  eprinttype = {arxiv},
  institution = {{Microsoft}},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/LWDDXXTX/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf}
}

@inproceedings{hebert-johnson_multicalibration_2018,
  title = {Multicalibration: {{Calibration}} for the ({{Computationally-Identifiable}}) {{Masses}}},
  shorttitle = {Multicalibration},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {{Hebert-Johnson}, Ursula and Kim, Michael and Reingold, Omer and Rothblum, Guy},
  year = {2018},
  month = jul,
  pages = {1939--1948},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We develop and study multicalibration as a new measure of fairness in machine learning that aims to mitigate inadvertent or malicious discrimination that is introduced at training time (even from ground truth data). Multicalibration guarantees meaningful (calibrated) predictions for every subpopulation that can be identified within a specified class of computations. The specified class can be quite rich; in particular, it can contain many overlapping subgroups of a protected group. We demonstrate that in many settings this strong notion of protection from discrimination is provably attainable and aligned with the goal of obtaining accurate predictions. Along the way, we present algorithms for learning a multicalibrated predictor, study the computational complexity of this task, and illustrate tight connections to the agnostic learning model.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/CDN9YTIP/Hebert-Johnson et al. - 2018 - Multicalibration Calibration for the (Computation.pdf;/Users/fariedabuzaid/Zotero/storage/NAKAYS9V/Hebert-Johnson et al. - 2018 - Multicalibration Calibration for the (Computation.pdf}
}

@book{hernan_causal_2019,
  title = {Causal {{Inference}}: {{What}} If},
  author = {Hernan, Miquel A. and Robins, James M.},
  year = {2019},
  month = dec,
  publisher = {{Taylor \& Francis}},
  abstract = {Causal inference is a complex scientific task that relies on combining evidence from multiple sources, and on the application of a variety of methodological approaches. Causal Inference: What If is an introduction to causal inference when data are collected on each individual in a population. The book is divided into three parts of increasing difficulty: causal inference without models, causal inference with models, and causal inference from complex longitudinal data. The book helps scientists to generate and analyze data for causal inferences that are explicit about both the causal question and the assumptions underlying the data analysis.  Features:    Provides a cohesive presentation of concepts and methods for causal inference that are currently scattered across journals in several disciplines   Emphasizes the need to take the causal question seriously enough to articulate it with sufficient precision   Shows that causal inference from observational data cannot be reduced to a collection of recipes for data analysis, as subject-matter knowledge is required to justify the necessary assumptions   Describes causal diagrams, both directed acyclic graphs and single-world intervention graphs, to represent causal inference problems   Describes various data analysis approaches to estimate the causal effect of interest, including the g-formula, inverse probability weighting, g-estimation, instrumental variable estimation, and propensity score adjustment   Includes 'Fine Points' and 'Technical Points' throughout to elaborate on certain key topics, as well as software and real data examples   Causal Inference: What If has been written to be accessible to all professionals that make causal inferences, including epidemiologists, statisticians, psychologists, economists, sociologists, political scientists, computer scientists, and more. It can be used to teach an introductory course on causal inference at graduate and advanced undergraduate level.},
  googlebooks = {\_KnHIAAACAAJ},
  isbn = {978-1-4200-7616-5},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/SXTFFG72/Hernan and Robins - 2019 - Causal Inference What if.pdf}
}

@inproceedings{heusel_gans_2018,
  title = {{{GANs Trained}} by a {{Two Time-Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  year = {2018},
  month = jan,
  volume = {30},
  eprint = {1706.08500},
  eprinttype = {arxiv},
  address = {{Long Beach, CA, USA}},
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the "Fr\textbackslash 'echet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/8UZ9U8EE/Heusel et al. - 2018 - GANs Trained by a Two Time-Scale Update Rule Conve.pdf}
}

@inproceedings{hirsch_exponential_1987,
  title = {Exponential Lower Bounds for Finding {{Brouwer}} Fixed Points},
  booktitle = {28th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} (Sfcs 1987)},
  author = {Hirsch, Michael D. and Vavasis, Stephen},
  year = {1987},
  month = oct,
  pages = {401--410},
  issn = {0272-5428},
  doi = {10.1109/SFCS.1987.24},
  abstract = {The Brouwer fixed point theorem has become a major tool for modeling economic systems during the 20th century. It was intractable to use the theorem in a computational manner until 1965 when Scarf provided the first practical algorithm for finding a fixed point of a Brouwer map. Scarf's work left open the question of worstcase complexity, although he hypothesized that his algorithm had "typical" behavior of polynomial time in the number of variables of the problem. Here we show that any algorithm for fixed points based on function evaluation (which includes all general purpose fixed-point algorithrna) must in the worst case take a number of steps which is exponential both in the number of digits of accuracy and in the number of variables.}
}

@inproceedings{ho_denoising_2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = dec,
  volume = {33},
  eprint = {2006.11239},
  eprinttype = {arxiv},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/9XMBW4UM/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@article{hoffman_nouturn_2014,
  title = {The {{No-U-Turn Sampler}}: {{Adaptively Setting Path Lengths}} in {{Hamiltonian Monte Carlo}}},
  shorttitle = {The {{No-U-Turn Sampler}}},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {47},
  pages = {1593--1623},
  issn = {1533-7928},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/24JTAAG6/Hoffman and Gelman - 2014 - The No-U-Turn Sampler Adaptively Setting Path Len.pdf}
}

@inproceedings{holl_learning_2020,
  title = {Learning to {{Control PDEs}} with {{Differentiable Physics}}},
  booktitle = {{{arXiv}}:2001.07457 [Physics, Stat]},
  author = {Holl, Philipp and Koltun, Vladlen and Thuerey, Nils},
  year = {2020},
  month = jan,
  eprint = {2001.07457},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  abstract = {Predicting outcomes and planning interactions with the physical world are long-standing goals for machine learning. A variety of such tasks involves continuous physical systems, which can be described by partial differential equations (PDEs) with many degrees of freedom. Existing methods that aim to control the dynamics of such systems are typically limited to relatively short time frames or a small number of interaction parameters. We present a novel hierarchical predictor-corrector scheme which enables neural networks to learn to understand and control complex nonlinear physical systems over long time frames. We propose to split the problem into two distinct tasks: planning and control. To this end, we introduce a predictor network that plans optimal trajectories and a control network that infers the corresponding control parameters. Both stages are trained end-to-end using a differentiable PDE solver. We demonstrate that our method successfully develops an understanding of complex physical systems and learns to control them for tasks involving PDEs such as the incompressible Navier-Stokes equations.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/Y68SQIQX/Holl et al. - 2020 - Learning to Control PDEs with Differentiable Physi.pdf}
}

@inproceedings{horn_topological_2022,
  title = {Topological {{Graph Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Horn, Max and Brouwer, Edward De and Moor, Michael and Moreau, Yves and Rieck, Bastian and Borgwardt, Karsten},
  year = {2022},
  abstract = {Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/C9VQ3YDZ/Horn et al. - 2022 - Topological Graph Neural Networks.pdf}
}

@misc{hu_improved_2020,
  title = {Improved {{Image Wasserstein Attacks}} and {{Defenses}}},
  author = {Hu, J. Edward and Swaminathan, Adith and Salman, Hadi and Yang, Greg},
  year = {2020},
  month = apr,
  number = {arXiv:2004.12478},
  eprint = {2004.12478},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2004.12478},
  abstract = {Robustness against image perturbations bounded by a \$\textbackslash ell\_p\$ ball have been well-studied in recent literature. Perturbations in the real-world, however, rarely exhibit the pixel independence that \$\textbackslash ell\_p\$ threat models assume. A recently proposed Wasserstein distance-bounded threat model is a promising alternative that limits the perturbation to pixel mass movements. We point out and rectify flaws in previous definition of the Wasserstein threat model and explore stronger attacks and defenses under our better-defined framework. Lastly, we discuss the inability of current Wasserstein-robust models in defending against perturbations seen in the real world. Our code and trained models are available at https://github.com/edwardjhu/improved\_wasserstein .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/fariedabuzaid/Zotero/storage/NWBKJJ22/Hu et al. - 2020 - Improved Image Wasserstein Attacks and Defenses.pdf;/Users/fariedabuzaid/Zotero/storage/EYU3KG9E/2004.html}
}

@inproceedings{huang_deepening_2022,
  title = {Towards {{Deepening Graph Neural Networks}}: {{A GNTK-based Optimization Perspective}}},
  shorttitle = {Towards {{Deepening Graph Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Huang, Wei and Li, Yayong and Du, Weitao and Xu, Richard and Yin, Jie and Chen, Ling and Zhang, Miao},
  year = {2022},
  abstract = {Graph convolutional networks (GCNs) and their variants have achieved great success in dealing with graph-structured data. Nevertheless, it is well known that deep GCNs suffer from the...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/IYFJGEZE/Huang et al. - 2021 - Towards Deepening Graph Neural Networks A GNTK-ba.pdf;/Users/fariedabuzaid/Zotero/storage/7CZN9AXY/forum.html}
}

@article{huber_robust_1964,
  title = {Robust {{Estimation}} of a {{Location Parameter}}},
  author = {Huber, Peter J.},
  year = {1964},
  journal = {The Annals of Mathematical Statistics},
  volume = {35},
  number = {1},
  pages = {73--101},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  abstract = {This paper contains a new approach toward a theory of robust estimation; it treats in detail the asymptotic theory of estimating a location parameter for contaminated normal distributions, and exhibits estimators--intermediaries between sample mean and sample median--that are asymptotically most robust (in a sense to be specified) among all translation invariant estimators. For the general background, see Tukey (1960) (p. 448 ff.) Let x1, {$\cdots$}, xn be independent random variables with common distribution function F(t - {$\xi$}). The problem is to estimate the location parameter {$\xi$}, but with the complication that the prototype distribution F(t) is only approximately known. I shall primarily be concerned with the model of indeterminacy F = (1 - {$\epsilon$}){$\Phi$} + {$\epsilon$} H, where \$0 \textbackslash leqq \textbackslash epsilon {$<$} 1\$ is a known number, {$\Phi$}(t) = (2{$\pi$})-1/2 {$\int$}t -{$\infty$} exp(-1/2s2) ds is the standard normal cumulative and H is an unknown contaminating distribution. This model arises for instance if the observations are assumed to be normal with variance 1, but a fraction {$\epsilon$} of them is affected by gross errors. Later on, I shall also consider other models of indeterminacy, e.g., \$\textbackslash sup\_t |F(t) - \textbackslash Phi(t)| \textbackslash leqq \textbackslash epsilon\$. Some inconvenience is caused by the fact that location and scale parameters are not uniquely determined: in general, for fixed {$\epsilon$}, there will be several values of {$\xi$} and {$\sigma$} such that \$\textbackslash sup\_t|F(t) - \textbackslash Phi((t - \textbackslash xi)/\textbackslash sigma)| \textbackslash leqq \textbackslash epsilon\$, and similarly for the contaminated case. Although this inherent and unavoidable indeterminacy is small if {$\epsilon$} is small and is rather irrelevant for practical purposes, it poses awkward problems for the theory, especially for optimality questions. To remove this difficulty, one may either (i) restrict attention to symmetric distributions, and estimate the location of the center of symmetry (this works for {$\xi$} but not for {$\sigma$}); or (ii) one may define the parameter to be estimated in terms of the estimator itself, namely by its asymptotic value for sample size n \textrightarrow{} {$\infty$}; or (iii) one may define the parameters by arbitrarily chosen functionals of the distribution (e.g., by the expectation, or the median of F). All three possibilities have unsatisfactory aspects, and I shall usually choose the variant which is mathematically most convenient. It is interesting to look back to the very origin of the theory of estimation, namely to Gauss and his theory of least squares. Gauss was fully aware that his main reason for assuming an underlying normal distribution and a quadratic loss function was mathematical, i.e., computational, convenience. In later times, this was often forgotten, partly because of the central limit theorem. However, if one wants to be honest, the central limit theorem can at most explain why many distributions occurring in practice are approximately normal. The stress is on the word "approximately." This raises a question which could have been asked already by Gauss, but which was, as far as I know, only raised a few years ago (notably by Tukey): What happens if the true distribution deviates slightly from the assumed normal one? As is now well known, the sample mean then may have a catastrophically bad performance: seemingly quite mild deviations may already explode its variance. Tukey and others proposed several more robust substitutes--trimmed means, Winsorized means, etc.--and explored their performance for a few typical violations of normality. A general theory of robust estimation is still lacking; it is hoped that the present paper will furnish the first few steps toward such a theory. At the core of the method of least squares lies the idea to minimize the sum of the squared "errors," that is, to adjust the unknown parameters such that the sum of the squares of the differences between observed and computed values is minimized. In the simplest case, with which we are concerned here, namely the estimation of a location parameter, one has to minimize the expression {$\sum$}i (xi - T)2; this is of course achieved by the sample mean T = {$\sum$}i xi/n. I should like to emphasize that no loss function is involved here; I am only describing how the least squares estimator is defined, and neither the underlying family of distributions nor the true value of the parameter to be estimated enters so far. It is quite natural to ask whether one can obtain more robustness by minimizing another function of the errors than the sum of their squares. We shall therefore concentrate our attention to estimators that can be defined by a minimum principle of the form (for a location parameter): T = Tn(x1, {$\cdots$}, xn) minimizes {$\sum$}i {$\rho$}(xi - T), \textbackslash begin\{equation*\} \textbackslash tag\{M\} where \textbackslash rho is a non-constant function. \textbackslash end\{equation*\} Of course, this definition generalizes at once to more general least squares type problems, where several parameters have to be determined. This class of estimators contains in particular (i) the sample mean ({$\rho$}(t) = t2), (ii) the sample median ({$\rho$}(t) = |t|), and more generally, (iii) all maximum likelihood estimators ({$\rho$}(t) = -log f(t), where f is the assumed density of the untranslated distribution). These (M)-estimators, as I shall call them for short, have rather pleasant asymptotic properties; sufficient conditions for asymptotic normality and an explicit expression for their asymptotic variance will be given. How should one judge the robustness of an estimator Tn(x) = Tn(x1, {$\cdots$}, xn)? Since ill effects from contamination are mainly felt for large sample sizes, it seems that one should primarily optimize large sample robustness properties. Therefore, a convenient measure of robustness for asymptotically normal estimators seems to be the supremum of the asymptotic variance (n \textrightarrow{} {$\infty$}) when F ranges over some suitable set of underlying distributions, in particular over the set of all F = (1 - {$\epsilon$}){$\Phi$} + {$\epsilon$} H for fixed {$\epsilon$} and symmetric H. On second thought, it turns out that the asymptotic variance is not only easier to handle, but that even for moderate values of n it is a better measure of performance than the actual variance, because (i) the actual variance of an estimator depends very much on the behavior of the tails of H, and the supremum of the actual variance is infinite for any estimator whose value is always contained in the convex hull of the observations. (ii) If an estimator is asymptotically normal, then the important central part of its distribution and confidence intervals for moderate confidence levels can better be approximated in terms of the asymptotic variance than in terms of the actual variance. If we adopt this measure of robustness, and if we restrict attention to (M)-estimators, then it will be shown that the most robust estimator is uniquely determined and corresponds to the following {$\rho$}:{$\rho$}(t) = 1/2t2 for \$|t| {$<$} k, \textbackslash rho(t) = k|t| - \textbackslash frac\{1\}\{2\}k\^2\$ for |t| {$\geq$} k, with k depending on {$\epsilon$}. This estimator is most robust even among all translation invariant estimators. Sample mean (k = {$\infty$}) and sample median (k = 0) are limiting cases corresponding to {$\epsilon$} = 0 and {$\epsilon$} = 1, respectively, and the estimator is closely related and asymptotically equivalent to Winsorizing. I recall the definition of Winsorizing: assume that the observations have been ordered, x1 {$\leq$} x2 {$\leq$} {$\cdots$} {$\leq$} xn, then the statistic T = n-1(gxg + 1 + xg + 1 + xg + 2 + {$\cdots$} + xn - h + hxn - h) is called the Winsorized mean, obtained by Winsorizing the g leftmost and the h rightmost observations. The above most robust (M)-estimators can be described by the same formula, except that in the first and in the last summand, the factors xg + 1 and xn - h have to be replaced by some numbers u, v satisfying xg {$\leq$} u {$\leq$} xg + 1 and xn - h {$\leq$} v {$\leq$} xn - h + 1, respectively; g, h, u and v depend on the sample. In fact, this (M)-estimator is the maximum likelihood estimator corresponding to a unique least favorable distribution F0 with density f0(t) = (1 - {$\epsilon$})(2{$\pi$})-1/2e-{$\rho$}(t). This f0 behaves like a normal density for small t, like an exponential density for large t. At least for me, this was rather surprising--I would have expected an f0 with much heavier tails. This result is a particular case of a more general one that can be stated roughly as follows: Assume that F belongs to some convex set C of distribution functions. Then the most robust (M)-estimator for the set C coincides with the maximum likelihood estimator for the unique F0 {$\epsilon$} C which has the smallest Fisher information number I(F) = {$\int$} (f'/f)2f dt among all F {$\epsilon$} C. Miscellaneous related problems will also be treated: the case of non-symmetric contaminating distributions; the most robust estimator for the model of indeterminacy \$\textbackslash sup\_t|F(t) - \textbackslash Phi(t)| \textbackslash leqq \textbackslash epsilon\$; robust estimation of a scale parameter; how to estimate location, if scale and {$\epsilon$} are unknown; numerical computation of the estimators; more general estimators, e.g., minimizing \$\textbackslash sum\_\{i {$<$} j\} \textbackslash rho(x\_i - T, x\_j - T)\$, where {$\rho$} is a function of two arguments. Questions of small sample size theory will not be touched in this paper.}
}

@article{hullermeier_aleatoric_2021,
  title = {Aleatoric and {{Epistemic Uncertainty}} in {{Machine Learning}}: {{An Introduction}} to {{Concepts}} and {{Methods}}},
  shorttitle = {Aleatoric and {{Epistemic Uncertainty}} in {{Machine Learning}}},
  author = {H{\"u}llermeier, Eyke and Waegeman, Willem},
  year = {2021},
  month = mar,
  journal = {Machine Learning},
  volume = {110},
  number = {3},
  eprint = {1910.09457},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {457--506},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-021-05946-3},
  abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
  archiveprefix = {arXiv},
  langid = {english}
}

@techreport{ichimura_influence_2015,
  type = {{{CeMMAP}} Working Papers},
  title = {The Influence Function of Semiparametric Estimators},
  author = {Ichimura, Hidehiko and Newey, Whitney K.},
  year = {2015},
  month = jul,
  number = {CWP44/15},
  pages = {46},
  institution = {{Centre for Microdata Methods and Practice, Institute for Fiscal Studies}},
  abstract = {Often semiparametric estimators are asymptotically equivalent to a sample average. The object being averaged is referred to as the in?uence function. The in?uence function is useful in formulating primitive regularity conditions for asymptotic normality, in efficiency comparions, for bias reduction, and for analyzing robustness. We show that the in?uence function of a semiparametric estimator can be calculated as the limit of the Gateaux derivative of a parameter with respect to a smooth deviation as the deviation approaches a point mass. We also consider high level and primitive regularity conditions for validity of the in?uence function calculation. The conditions involve Frechet differentiability, nonparametric convergence rates, stochastic equicontinuity, and small bias conditions. We apply these results to examples.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/I46QQYLF/Ichimura and Newey - The Inﬂuence Function of Semiparametric Estimators.pdf}
}

@misc{icml_,
  title = {{{ICML}} 2018 {{Schedule}}},
  howpublished = {https://icml.cc/Conferences/2018/Schedule?showEvent=3139}
}

@inproceedings{igl_generalization_2019,
  title = {Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Igl, Maximilian and Ciosek, Kamil and Li, Yingzhen and Tschiatschek, Sebastian and Zhang, Cheng and Devlin, Sam and Hofmann, Katja},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  volume = {32},
  pages = {13978--13990},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/fariedabuzaid/Zotero/storage/SHZVTTQH/Igl et al. - 2019 - Generalization in reinforcement learning with sele.pdf}
}

@misc{imagenette_2022,
  title = {Imagenette},
  year = {2022},
  month = jun,
  abstract = {A smaller subset of 10 easily classified classes from Imagenet, and a little more French},
  copyright = {Apache-2.0},
  howpublished = {fast.ai}
}

@inproceedings{islam_classdistributionaware_2021,
  title = {Class-{{Distribution-Aware Calibration}} for {{Long-Tailed Visual Recognition}}},
  booktitle = {{{arXiv}}:2109.05263 [Cs]},
  author = {Islam, Mobarakol and Seenivasan, Lalithkumar and Ren, Hongliang and Glocker, Ben},
  year = {2021},
  month = sep,
  eprint = {2109.05263},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Despite impressive accuracy, deep neural networks are often miscalibrated and tend to overly confident predictions. Recent techniques like temperature scaling (TS) and label smoothing (LS) show effectiveness in obtaining a well-calibrated model by smoothing logits and hard labels with scalar factors, respectively. However, the use of uniform TS or LS factor may not be optimal for calibrating models trained on a long-tailed dataset where the model produces overly confident probabilities for high-frequency classes. In this study, we propose class-distribution-aware TS (CDA-TS) and LS (CDA-LS) by incorporating class frequency information in model calibration in the context of long-tailed distribution. In CDA-TS, the scalar temperature value is replaced with the CDA temperature vector encoded with class frequency to compensate for the over-confidence. Similarly, CDA-LS uses a vector smoothing factor and flattens the hard labels according to their corresponding class distribution. We also integrate CDA optimal temperature vector with distillation loss, which reduces miscalibration in self-distillation (SD). We empirically show that class-distribution-aware TS and LS can accommodate the imbalanced data distribution yielding superior performance in both calibration error and predictive accuracy. We also observe that SD with an extremely imbalanced dataset is less effective in terms of calibration performance. Code is available in https://github.com/mobarakol/Class-Distribution-Aware-TS-LS.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/Q7VKY5UW/Islam et al. - 2021 - Class-Distribution-Aware Calibration for Long-Tail.pdf}
}

@article{jacobs_how_2021,
  title = {How Machine-Learning Recommendations Influence Clinician Treatment Selections: The Example of Antidepressant Selection},
  shorttitle = {How Machine-Learning Recommendations Influence Clinician Treatment Selections},
  author = {Jacobs, Maia and Pradier, Melanie F. and McCoy, Thomas H. and Perlis, Roy H. and {Doshi-Velez}, Finale and Gajos, Krzysztof Z.},
  year = {2021},
  month = feb,
  journal = {Translational Psychiatry},
  volume = {11},
  number = {1},
  pages = {1--9},
  publisher = {{Nature Publishing Group}},
  issn = {2158-3188},
  doi = {10.1038/s41398-021-01224-x},
  abstract = {Decision support systems embodying machine learning models offer the promise of an improved standard of care for major depressive disorder, but little is known about how clinicians' treatment decisions will be influenced by machine learning recommendations and explanations. We used a within-subject factorial experiment to present 220 clinicians with patient vignettes, each with or without a machine-learning (ML) recommendation and one of the multiple forms of explanation. We found that interacting with ML recommendations did not significantly improve clinicians' treatment selection accuracy, assessed as concordance with expert psychopharmacologist consensus, compared to baseline scenarios in which clinicians made treatment decisions independently. Interacting with incorrect recommendations paired with explanations that included limited but easily interpretable information did lead to a significant reduction in treatment selection accuracy compared to baseline questions. These results suggest that incorrect ML recommendations may adversely impact clinician treatment selections and that explanations are insufficient for addressing overreliance on imperfect ML algorithms. More generally, our findings challenge the common assumption that clinicians interacting with ML tools will perform better than either clinicians or ML algorithms individually.},
  copyright = {2021 The Author(s)},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cc\_license\_type: cc\_by Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Depression;Scientific community Subject\_term\_id: depression;scientific-community},
  file = {/Users/fariedabuzaid/Zotero/storage/CRSA4Y3L/Jacobs et al. - 2021 - How machine-learning recommendations influence cli.pdf}
}

@inproceedings{jahanian_generative_2022,
  title = {Generative {{Models}} as a {{Data Source}} for {{Multiview Representation Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR2022}})},
  author = {Jahanian, Ali and Puig, Xavier and Tian, Yonglong and Isola, Phillip},
  year = {2022},
  abstract = {Generative models are now capable of producing highly realistic images that look nearly indistinguishable from the data on which they are trained. This raises the question: if we have good enough...},
  langid = {english},
  annotation = {video:https://iclr.cc/virtual/2022/poster/6339},
  file = {/Users/fariedabuzaid/Zotero/storage/DIPVQYQV/Jahanian et al. - 2021 - Generative Models as a Data Source for Multiview R.pdf}
}

@article{jain_maximizing_2020,
  title = {Maximizing {{Overall Diversity}} for {{Improved Uncertainty Estimates}} in {{Deep Ensembles}}},
  author = {Jain, Siddhartha and Liu, Ge and Mueller, Jonas and Gifford, David},
  year = {2020},
  month = feb,
  journal = {arXiv [cs.LG]},
  eprint = {1906.07380},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  doi = {10.48550/ARXIV.1906.07380},
  abstract = {The inaccuracy of neural network models on inputs that do not stem from the training data distribution is both problematic and at times unrecognized. Model uncertainty estimation can address this issue, where uncertainty estimates are often based on the variation in predictions produced by a diverse ensemble of models applied to the same input. Here we describe Maximize Overall Diversity (MOD), a straightforward approach to improve ensemble-based uncertainty estimates by encouraging larger overall diversity in ensemble predictions across all possible inputs that might be encountered in the future. When applied to various neural network ensembles, MOD significantly improves predictive performance for out-of-distribution test examples without sacrificing in-distribution performance on 38 Protein-DNA binding regression datasets, 9 UCI datasets, and the IMDB-Wiki image dataset. Across many Bayesian optimization tasks, the performance of UCB acquisition is also greatly improved by leveraging MOD uncertainty estimates.},
  archiveprefix = {arXiv},
  langid = {english}
}

@inproceedings{jang_categorical_2017,
  title = {Categorical {{Reparameterization}} with {{Gumbel-Softmax}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2017)},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2017},
  month = aug,
  eprint = {1611.01144},
  eprinttype = {arxiv},
  doi = {10.48550/arXiv.1611.01144},
  abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/UF6TL3K7/Jang et al. - 2017 - Categorical Reparameterization with Gumbel-Softmax.pdf}
}

@inproceedings{jeon_neural_2022,
  title = {Neural {{Variational Dropout Processes}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Jeon, Insu and Park, Youngjin and Kim, Gunhee},
  year = {2022},
  abstract = {Learning to infer the conditional posterior model is a key step for robust meta-learning. This paper presents a new Bayesian meta-learning approach called Neural Variational Dropout Processes...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/5KP8FHZ5/Jeon et al. - 2022 - Neural Variational Dropout Processes.pdf}
}

@inproceedings{jethani_fastshap_2022,
  title = {{{FastSHAP}}: {{Real-Time Shapley Value Estimation}}},
  shorttitle = {{{FastSHAP}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Jethani, Neil and Sudarshan, Mukund and Covert, Ian Connick and Lee, Su-In and Ranganath, Rajesh},
  year = {2022},
  abstract = {Although Shapley values are theoretically appealing for explaining black-box models, they are costly to calculate and thus impractical in settings that involve large, high-dimensional models. To...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/B6NWW9BZ/FastSHAP_with_supplement.pdf;/Users/fariedabuzaid/Zotero/storage/G66CDRLZ/Jethani et al. - 2021 - FastSHAP Real-Time Shapley Value Estimation.pdf}
}

@article{jia_efficient_2019,
  title = {Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms},
  shorttitle = {{{VLDB}} 2019},
  author = {Jia, Ruoxi and Dao, David and Wang, Boxin and Hubis, Frances Ann and Gurel, Nezihe Merve and Li, Bo and Zhang, Ce and Spanos, Costas and Song, Dawn},
  year = {2019},
  month = jul,
  journal = {Proceedings of the VLDB Endowment},
  volume = {12},
  number = {11},
  pages = {1610--1623},
  issn = {2150-8097},
  doi = {10.14778/3342263.3342637},
  abstract = {Given a data set D containing millions of data points and a data consumer who is willing to pay for \$X to train a machine learning (ML) model over D, how should we distribute this \$X to each data point to reflect its "value"? In this paper, we define the "relative value of data" via the Shapley value, as it uniquely possesses properties with appealing real-world interpretations, such as fairness, rationality and decentralizability. For general, bounded utility functions, the Shapley value is known to be challenging to compute: to get Shapley values for all N data points, it requires O(2N) model evaluations for exact computation and O(N log N) for ({$\epsilon$}, {$\delta$})-approximation. In this paper, we focus on one popular family of ML models relying on K-nearest neighbors (KNN). The most surprising result is that for unweighted KNN classifiers and regressors, the Shapley value of all N data points can be computed, exactly, in O(N log N) time - an exponential improvement on computational complexity! Moreover, for ({$\epsilon$}, {$\delta$})-approximation, we are able to develop an algorithm based on Locality Sensitive Hashing (LSH) with only sublinear complexity O(Nh({$\epsilon$}, K) log N) when {$\epsilon$} is not too small and K is not too large. We empirically evaluate our algorithms on up to 10 million data points and even our exact algorithm is up to three orders of magnitude faster than the baseline approximation algorithm. The LSH-based approximation algorithm can accelerate the value calculation process even further. We then extend our algorithm to other scenarios such as (1) weighed KNN classifiers, (2) different data points are clustered by different data curators, and (3) there are data analysts providing computation who also requires proper valuation. Some of these extensions, although also being improved exponentially, are less practical for exact computation (e.g., O(NK) complexity for weigthed KNN). We thus propose an Monte Carlo approximation algorithm, which is O(N(log N)2/(log K)2) times more efficient than the baseline approximation algorithm.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/6LGCAXI6/Jia et al. - 2019 - Efficient task-specific data valuation for nearest.pdf}
}

@inproceedings{jia_efficient_2019a,
  title = {Towards {{Efficient Data Valuation Based}} on the {{Shapley Value}}},
  booktitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Jia, Ruoxi and Dao, David and Wang, Boxin and Hubis, Frances Ann and Hynes, Nick and G{\"u}rel, Nezihe Merve and Li, Bo and Zhang, Ce and Song, Dawn and Spanos, Costas J.},
  year = {2019},
  month = apr,
  pages = {1167--1176},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {``How much is my data worth?'' is an increasingly common question posed by organizations and individuals alike. An answer to this question could allow, for instance, fairly distributing profits...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/3DBXRJAY/Jia et al. - 2019 - Towards Efficient Data Valuation Based on the Shap.pdf;/Users/fariedabuzaid/Zotero/storage/AH9SKPMA/Jia et al. - Supplementary Material Towards Eﬃcient Data Valua.pdf}
}

@article{jia_empirical_2019,
  title = {An {{Empirical}} and {{Comparative Analysis}} of {{Data Valuation}} with {{Scalable Algorithms}}},
  author = {Jia, Ruoxi and Sun, Xuehui and Xu, Jiacen and Zhang, Ce and Li, Bo and Song, Dawn},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.07128 [cs, stat]},
  eprint = {1911.07128},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {This paper focuses on valuating training data for supervised learning tasks and studies the Shapley value, a data value notion originated in cooperative game theory. The Shapley value defines a unique value distribution scheme that satisfies a set of appealing properties desired by a data value notion. However, the Shapley value requires exponential complexity to calculate exactly. Existing approximation algorithms, although achieving great improvement over the exact algorithm, relies on retraining models for multiple times, thus remaining limited when applied to larger-scale learning tasks and real-world datasets. In this work, we develop a simple and efficient heuristic for data valuation based on the Shapley value with complexity independent with the model size. The key idea is to approximate the model via a \$K\$-nearest neighbor (\$K\$NN) classifier, which has a locality structure that can lead to efficient Shapley value calculation. We evaluate the utility of the values produced by the \$K\$NN proxies in various settings, including label noise correction, watermark detection, data summarization, active data acquisition, and domain adaption. Extensive experiments demonstrate that our algorithm achieves at least comparable utility to the values produced by existing algorithms while significant efficiency improvement. Moreover, we theoretically analyze the Shapley value and justify its advantage over the leave-one-out error as a data value measure.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/JGW9I2UT/9TTUSC8V.pdf}
}

@inproceedings{jia_scalability_2021,
  title = {Scalability vs. {{Utility}}: {{Do We Have To Sacrifice One}} for the {{Other}} in {{Data Importance Quantification}}?},
  shorttitle = {Scalability vs. {{Utility}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Jia, Ruoxi and Wu, Fan and Sun, Xuehui and Xu, Jiacen and Dao, David and Kailkhura, Bhavya and Zhang, Ce and Li, Bo and Song, Dawn},
  year = {2021},
  eprint = {1911.07128},
  eprinttype = {arxiv},
  pages = {8239--8247},
  abstract = {Quantifying the importance of each training point to a learning task is a fundamental problem in machine learning and the estimated importance scores have been leveraged to guide a range of data workflows such as data summarization and domain adaption. One simple idea is to use the leave-one-out error of each training point to indicate its importance. Recent work has also proposed to use the Shapley value, as it defines a unique value distribution scheme that satisfies a set of appealing properties. However, calculating Shapley values is often expensive, which limits its applicability in real-world applications at scale. Multiple heuristics to improve the scalability of calculating Shapley values have been proposed recently, with the potential risk of compromising their utility in real-world applications. How well do existing data quantification methods perform on existing workflows? How do these methods compare with each other, empirically and theoretically? Must we sacrifice scalability for the utility in these workflows when using these methods? In this paper, we conduct a novel theoretical analysis comparing the utility of different importance quantification methods, and report extensive experimental studies on settings such as noisy label detection, watermark removal, data summarization, data acquisition, and domain adaptation on existing and proposed workflows. We show that Shapley value approximation based on a KNN surrogate over pre-trained feature embeddings obtains comparable utility with existing algorithms while achieving significant scalability improvement, often by orders of magnitude. Our theoretical analysis also justifies its advantage over the leave-one-out error. The code is available at https://github.com/AI-secure/Shapley-Study.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/PIXCVH4I/99XD8F5Y.pdf}
}

@inproceedings{jin_automated_2022,
  title = {Automated {{Self-Supervised Learning}} for {{Graphs}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Jin, Wei and Liu, Xiaorui and Zhao, Xiangyu and Ma, Yao and Shah, Neil and Tang, Jiliang},
  year = {2022},
  abstract = {Graph self-supervised learning has gained increasing attention due to its capacity to learn expressive node representations. Many pretext tasks, or loss functions have been designed from distinct...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/5FHGCMIU/Jin et al. - 2021 - Automated Self-Supervised Learning for Graphs.pdf;/Users/fariedabuzaid/Zotero/storage/6N2YGUMQ/forum.html}
}

@inproceedings{jin_graph_2022,
  title = {Graph {{Condensation}} for {{Graph Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Jin, Wei and Zhao, Lingxiao and Zhang, Shichang and Liu, Yozen and Tang, Jiliang and Shah, Neil},
  year = {2022},
  abstract = {Given the prevalence of large-scale graphs in real-world applications, the storage and time for training neural models have raised increasing concerns. To alleviate the concerns, we propose and...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/YJTYFIVA/Jin et al. - 2021 - Graph Condensation for Graph Neural Networks.pdf;/Users/fariedabuzaid/Zotero/storage/45CS5PFV/forum.html}
}

@inproceedings{jin_iterative_2022,
  title = {Iterative {{Refinement Graph Neural Network}} for {{Antibody Sequence-Structure Co-design}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Jin, Wengong and Wohlwend, Jeremy and Barzilay, Regina and Jaakkola, Tommi S.},
  year = {2022},
  abstract = {Antibodies are versatile proteins that bind to pathogens like viruses and stimulate the adaptive immune system. The specificity of antibody binding is determined by complementarity-determining...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/KNWGYSK4/Jin et al. - 2021 - Iterative Refinement Graph Neural Network for Anti.pdf;/Users/fariedabuzaid/Zotero/storage/6Q8L33YY/forum.html}
}

@inproceedings{jin_junction_2018,
  title = {Junction {{Tree Variational Autoencoder}} for {{Molecular Graph Generation}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  year = {2018},
  month = jul,
  pages = {2323--2332},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/2676ECDF/Jin et al. - 2018 - Junction Tree Variational Autoencoder for Molecula.pdf;/Users/fariedabuzaid/Zotero/storage/FTY5QXDW/Jin et al. - 2018 - Junction Tree Variational Autoencoder for Molecula.pdf}
}

@article{jost_characterizations_2021,
  title = {Characterizations of {{Forman}} Curvature},
  author = {Jost, J{\"u}rgen and M{\"u}nch, Florentin},
  year = {2021},
  month = oct,
  journal = {arXiv:2110.04554 [math]},
  eprint = {2110.04554},
  eprinttype = {arxiv},
  primaryclass = {math},
  abstract = {We characterize Forman curvature lower bounds via contractivity of the Hodge Laplacian semigroup. We prove that Ollivier and Forman curvature coincide on edges when maximizing the Forman curvature over the choice of 2-cells. To this end, we translate between 2-cells and transport plans. Moreover, we give improved diameter bounds. We explicitly warn the reader that our Forman curvature notion does not coincide with Forman's original definition, but can be seen as generalization of the latter one.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/YA69RCXZ/Jost und Münch - 2021 - Characterizations of Forman curvature.pdf;/Users/fariedabuzaid/Zotero/storage/QYK5CINQ/2110.html}
}

@inproceedings{jung_moment_2021,
  title = {Moment {{Multicalibration}} for {{Uncertainty Estimation}}},
  booktitle = {Conference on {{Learning Theory}}},
  author = {Jung, Christopher and Lee, Changhwa and Pai, Mallesh and Roth, Aaron and Vohra, Rakesh},
  year = {2021},
  month = jul,
  pages = {2634--2678},
  publisher = {{PMLR}},
  issn = {2640-3498},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/T5D46IHJ/Jung et al. - 2021 - Moment Multicalibration for Uncertainty Estimation.pdf}
}

@inproceedings{k.gustafssonfredrik_evaluating_2019,
  title = {Evaluating {{Scalable Bayesian Deep Learning Methods}} for {{Robust Computer Vision}}},
  booktitle = {4th Workshop on {{Bayesian Deep Learning}} ({{NeurIPS}} 2019)},
  author = {K. Gustafsson, Fredrik and Danelljan, Martin and B. Sch{\"o}n, Thomas},
  year = {2019},
  publisher = {{arXiv}},
  address = {{Vancouver, Canada}},
  doi = {10.48550/ARXIV.1906.01620},
  abstract = {While Deep Neural Networks (DNNs) have become the go-to approach in computer vision, the vast majority of these models fail to properly capture the uncertainty inherent in their predictions. Estimating this predictive uncertainty can be crucial, for instance in automotive applications. In Bayesian deep learning, predictive uncertainty is often decomposed into the distinct types of aleatoric and epistemic uncertainty. The former can be estimated by letting a DNN output the parameters of a probability distribution. Epistemic uncertainty estimation is a more challenging problem, and while different scalable methods recently have emerged, no com- prehensive comparison has been performed in a real-world setting. We therefore accept this task and propose an evaluation framework for predictive uncertainty estimation that is specifically designed to test the robustness required in real-world computer vision applications. Using the proposed framework, we perform an extensive comparison of the popular ensembling and MC-dropout methods on the tasks of depth completion and street-scene semantic segmentation. Our comparison suggests that ensembling consistently provides more reliable uncertainty estimates.}
}

@article{kac_can_1966,
  title = {Can {{One Hear}} the {{Shape}} of a {{Drum}}?},
  author = {Kac, Mark},
  year = {1966},
  journal = {The American Mathematical Monthly},
  volume = {73},
  number = {4},
  pages = {1--23},
  publisher = {{Mathematical Association of America}},
  issn = {0002-9890},
  doi = {10.2307/2313748}
}

@inproceedings{kakade_approximately_2002,
  title = {Approximately {{Optimal Approximate Reinforcement Learning}}},
  booktitle = {Proceedings of the {{Nineteenth International Conference}} on {{Machine Learning}}},
  author = {Kakade, Sham and Langford, John},
  year = {2002},
  month = jul,
  series = {{{ICML}} '02},
  pages = {267--274},
  publisher = {{Morgan Kaufmann Publishers Inc.}},
  address = {{San Francisco, CA, USA}},
  doi = {10.1.1.7.7601},
  abstract = {In order to solve realistic reinforcement learning problems, it is critical that approximate algorithms be used. In this paper, we present the *conservative policy iteration* algorithm which finds an "approximately" optimal policy, given access to a restart distribution (which draws the next state from a particular distribution) and an approximate greedy policy chooser. Crudely, the greedy policy chooser outputs a policy that usually chooses actions with the largest state-action values of the current policy, ie it outputs an "approximate" greedy policy. This greedy policy chooser can be implemented using standard value function approximation techniques. Under these assumptions, our algorithm: (1) is guaranteed to improve a performane metric (2) is guaranteed to terminate in a "small" number of timesteps and (3) returns an "approximately" optimal policy. The quantified statements of (2) and (3) depend on the quality of the greedy policy chooser, but *not* explicitly on the size of the state space.},
  isbn = {978-1-55860-873-3},
  file = {/Users/fariedabuzaid/Zotero/storage/E29JKXKC/Kakade and Langford - 2002 - Approximately Optimal Approximate Reinforcement Le.pdf}
}

@inproceedings{kakade_natural_2001,
  title = {A {{Natural Policy Gradient}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kakade, Sham M.},
  year = {2001},
  volume = {14},
  abstract = {We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/E3DWM8LP/Kakade - 2001 - A Natural Policy Gradient.pdf}
}

@inproceedings{kanoh_neural_2022,
  title = {A {{Neural Tangent Kernel Perspective}} of {{Infinite Tree Ensembles}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Kanoh, Ryuichi and Sugiyama, Mahito},
  year = {2022},
  abstract = {In practical situations, the tree ensemble is one of the most popular models along with neural networks. A soft tree is a variant of a decision tree. Instead of using a greedy method for searching...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/CDNRHYQH/Kanoh und Sugiyama - 2021 - A Neural Tangent Kernel Perspective of Infinite Tr.pdf;/Users/fariedabuzaid/Zotero/storage/5C34LFHZ/forum.html}
}

@book{kaplan_advanced_2002,
  title = {Advanced {{Calculus}}},
  author = {Kaplan, Wilfred},
  year = {2002},
  month = jul,
  edition = {Fifth},
  publisher = {{Pearson}},
  address = {{Boston}},
  isbn = {978-0-201-79937-8},
  langid = {english}
}

@inproceedings{ke_lightgbm_2017,
  title = {{{LightGBM}}: {{A Highly Efficient Gradient Boosting Decision Tree}}},
  shorttitle = {{{LightGBM}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
  year = {2017},
  month = dec,
  volume = {30},
  pages = {9},
  address = {{Long Beach, CA, USA}},
  abstract = {Gradient Boosting Decision Tree (GBDT) is a popular machine learning algorithm, and has quite a few effective implementations such as XGBoost and pGBRT. Although many engineering optimizations have been adopted in these implementations, the efficiency and scalability are still unsatisfactory when the feature dimension is high and data size is large. A major reason is that for each feature, they need to scan all the data instances to estimate the information gain of all possible split points, which is very time consuming. To tackle this problem, we propose two novel techniques: Gradient-based One-Side Sampling (GOSS) and Exclusive Feature Bundling (EFB). With GOSS, we exclude a significant proportion of data instances with small gradients, and only use the rest to estimate the information gain. We prove that, since the data instances with larger gradients play a more important role in the computation of information gain, GOSS can obtain quite accurate estimation of the information gain with a much smaller data size. With EFB, we bundle mutually exclusive features (i.e., they rarely take nonzero values simultaneously), to reduce the number of features. We prove that finding the optimal bundling of exclusive features is NP-hard, but a greedy algorithm can achieve quite good approximation ratio (and thus can effectively reduce the number of features without hurting the accuracy of split point determination by much). We call our new GBDT implementation with GOSS and EFB LightGBM. Our experiments on multiple public datasets show that, LightGBM speeds up the training process of conventional GBDT by up to over 20 times while achieving almost the same accuracy.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/WHU2K2JL/Ke et al. - LightGBM A Highly Efficient Gradient Boosting Dec.pdf;/Users/fariedabuzaid/Zotero/storage/YQUQVUH7/supplementary-nips.pdf}
}

@inproceedings{kerrigan_combining_2021,
  title = {Combining {{Human Predictions}} with {{Model Probabilities}} via {{Confusion Matrices}} and {{Calibration}}},
  booktitle = {{{arXiv}}:2109.14591 [Cs, Stat]},
  author = {Kerrigan, Gavin and Smyth, Padhraic and Steyvers, Mark},
  year = {2021},
  month = oct,
  eprint = {2109.14591},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {An increasingly common use case for machine learning models is augmenting the abilities of human decision makers. For classification tasks where neither the human or model are perfectly accurate, a key step in obtaining high performance is combining their individual predictions in a manner that leverages their relative strengths. In this work, we develop a set of algorithms that combine the probabilistic output of a model with the class-level output of a human. We show theoretically that the accuracy of our combination model is driven not only by the individual human and model accuracies, but also by the model's confidence. Empirical results on image classification with CIFAR-10 and a subset of ImageNet demonstrate that such human-model combinations consistently have higher accuracies than the model or human alone, and that the parameters of the combination method can be estimated effectively with as few as ten labeled datapoints.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/9GKWBITR/Kerrigan et al. - 2021 - Combining Human Predictions with Model Probabiliti.pdf}
}

@misc{kettunen_elpips_2019,
  title = {E-{{LPIPS}}: {{Robust Perceptual Image Similarity}} via {{Random Transformation Ensembles}}},
  shorttitle = {E-{{LPIPS}}},
  author = {Kettunen, Markus and H{\"a}rk{\"o}nen, Erik and Lehtinen, Jaakko},
  year = {2019},
  month = jun,
  number = {1906.03973},
  eprint = {1906.03973},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {It has been recently shown that the hidden variables of convolutional neural networks make for an efficient perceptual similarity metric that accurately predicts human judgment on relative image similarity assessment. First, we show that such learned perceptual similarity metrics (LPIPS) are susceptible to adversarial attacks that dramatically contradict human visual similarity judgment. While this is not surprising in light of neural networks' well-known weakness to adversarial perturbations, we proceed to show that self-ensembling with an infinite family of random transformations of the input --- a technique known not to render classification networks robust --- is enough to turn the metric robust against attack, while retaining predictive power on human judgments. Finally, we study the geometry imposed by our our novel self-ensembled metric (E-LPIPS) on the space of natural images. We find evidence of "perceptual convexity" by showing that convex combinations of similar-looking images retain appearance, and that discrete geodesics yield meaningful frame interpolation and texture morphing, all without explicit correspondences.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/HG5E5BAR/Kettunen et al. - 2019 - E-LPIPS Robust Perceptual Image Similarity via Ra.pdf}
}

@article{kharazmi_variational_2019,
  ids = {kharazmi_variational_2019a},
  title = {Variational {{Physics-Informed Neural Networks For Solving Partial Differential Equations}}},
  author = {Kharazmi, E. and Zhang, Z. and Karniadakis, G. E.},
  year = {2019},
  month = nov,
  journal = {arXiv:1912.00873 [physics, stat]},
  eprint = {1912.00873},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  abstract = {Physics-informed neural networks (PINNs) use automatic differentiation to solve partial differential equations (PDEs) by penalizing the PDE in the loss function at a random set of points in the domain of interest. Here, we develop a Petrov-Galerkin version of PINNs based on the nonlinear approximation of deep neural networks (DNNs) by selecting the *trial space* to be the space of neural networks and the  *test space* to be the space of Legendre polynomials. We formulate the *variational residual* of the PDE using the DNN approximation by incorporating the variational form of the problem into the loss function of the network and construct a *variational physics-informed neural network* (VPINN). By integrating by parts the integrand in the variational form, we lower the order of the differential operators represented by the neural networks, hence effectively reducing the training cost in VPINNs while increasing their accuracy compared to PINNs that essentially employ delta test functions. For shallow networks with one hidden layer, we analytically obtain explicit forms of the *variational residual*. We demonstrate the performance of the new formulation for several examples that show clear advantages of VPINNs over PINNs in terms of both accuracy and speed.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/WQA2SKCI/Kharazmi et al. - 2019 - Variational Physics-Informed Neural Networks For S.pdf}
}

@article{kim_fermi_2021,
  title = {The {{Fermi}}\textendash{{Dirac}} Distribution Provides a Calibrated Probabilistic Output for Binary Classifiers},
  author = {Kim, Sung-Cheol and Arun, Adith S. and Ahsen, Mehmet Eren and Vogel, Robert and Stolovitzky, Gustavo},
  year = {2021},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {34},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2100761118},
  abstract = {Binary classification is one of the central problems in machine-learning research and, as such, investigations of its general statistical properties are of interest. We studied the ranking statistics of items in binary classification problems and observed that there is a formal and surprising relationship between the probability of a sample belonging to one of the two classes and the Fermi\textendash Dirac distribution determining the probability that a fermion occupies a given single-particle quantum state in a physical system of noninteracting fermions. Using this equivalence, it is possible to compute a calibrated probabilistic output for binary classifiers. We show that the area under the receiver operating characteristics curve (AUC) in a classification problem is related to the temperature of an equivalent physical system. In a similar manner, the optimal decision threshold between the two classes is associated with the chemical potential of an equivalent physical system. Using our framework, we also derive a closed-form expression to calculate the variance for the AUC of a classifier. Finally, we introduce FiDEL (Fermi\textendash Dirac-based ensemble learning), an ensemble learning algorithm that uses the calibrated nature of the classifier's output probability to combine possibly very different classifiers.},
  chapter = {Physical Sciences},
  copyright = {Copyright \textcopyright{} 2021 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).},
  langid = {english},
  pmid = {34413191},
  file = {/Users/fariedabuzaid/Zotero/storage/GWBA5IRD/Kim et al. - 2021 - The Fermi–Dirac distribution provides a calibrated.pdf}
}

@article{kim_interpretability_2018,
  title = {Interpretability {{Beyond Feature Attribution}}: {{Quantitative Testing}} with {{Concept Activation Vectors}} ({{TCAV}})},
  shorttitle = {Interpretability {{Beyond Feature Attribution}}},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  year = {2018},
  month = jun,
  journal = {arXiv:1711.11279 [stat]},
  eprint = {1711.11279},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
  archiveprefix = {arXiv},
  annotation = {notion: https://www.notion.so/appliedaiinitiative/TCAV-Interpreting-Neural-Networks-with-high-level-concepts-62812c4caca848e199150993652a6034 post: https://community.appliedai.de/topics/27304/topic\_feed\_posts/1246102},
  file = {/Users/fariedabuzaid/Zotero/storage/ITJC7WLU/Kim et al. - 2018 - Interpretability Beyond Feature Attribution Quant.pdf;/Users/fariedabuzaid/Zotero/storage/KEAB5HYE/Kim et al. - 2018 - Interpretability Beyond Feature Attribution Quant.pdf}
}

@article{kim_robust_2012,
  title = {Robust Kernel Density Estimation},
  author = {Kim, JooSeuk and Scott, Clayton D.},
  year = {2012},
  month = sep,
  journal = {The Journal of Machine Learning Research},
  volume = {13},
  number = {1},
  pages = {2529--2565},
  issn = {1532-4435},
  abstract = {We propose a method for nonparametric density estimation that exhibits robustness to contamination of the training sample. This method achieves robustness by combining a traditional kernel density estimator (KDE) with ideas from classical M-estimation. We interpret the KDE based on a positive semi-definite kernel as a sample mean in the associated reproducing kernel Hilbert space. Since the sample mean is sensitive to outliers, we estimate it robustly via M-estimation, yielding a robust kernel density estimator (RKDE). An RKDE can be computed efficiently via a kernelized iteratively re-weighted least squares (IRWLS) algorithm. Necessary and sufficient conditions are given for kernelized IRWLS to converge to the global minimizer of the M-estimator objective function. The robustness of the RKDE is demonstrated with a representer theorem, the influence function, and experimental results for density estimation and anomaly detection.},
  file = {/Users/fariedabuzaid/Zotero/storage/ID53J3DP/Kim und Scott - 2012 - Robust kernel density estimation.pdf}
}

@article{kirichenko_why_2020,
  title = {Why Normalizing Flows Fail to Detect Out-of-Distribution Data: 34th {{Conference}} on {{Neural Information Processing Systems}}, {{NeurIPS}} 2020},
  shorttitle = {Why Normalizing Flows Fail to Detect Out-of-Distribution Data},
  author = {Kirichenko, Polina and Izmailov, Pavel and Wilson, Andrew Gordon},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {2020-December},
  issn = {1049-5258},
  abstract = {Detecting out-of-distribution (OOD) data is crucial for robust machine learning systems. Normalizing flows are flexible deep generative models that often surprisingly fail to distinguish between in- and out-of-distribution data: a flow trained on pictures of clothing assigns higher likelihood to handwritten digits. We investigate why normalizing flows perform poorly for OOD detection. We demonstrate that flows learn local pixel correlations and generic image-to-latent-space transformations which are not specific to the target image datasets, focusing on flows based on coupling layers. We show that by modifying the architecture of flow coupling layers we can bias the flow towards learning the semantic structure of the target data, improving OOD detection. Our investigation reveals that properties that enable flows to generate high-fidelity images can have a detrimental effect on OOD detection.}
}

@inproceedings{klein_fast_2017,
  title = {Fast {{Bayesian Optimization}} of {{Machine Learning Hyperparameters}} on {{Large Datasets}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Klein, Aaron and Falkner, Stefan and Bartels, Simon and Hennig, Philipp and Hutter, Frank},
  year = {2017},
  month = apr,
  pages = {528--536},
  abstract = {Bayesian optimization has become a successful tool for hyperparameter optimization of machine learning algorithms, such as support vector machines or deep neural networks. Despite its success, for ...},
  langid = {english},
  annotation = {citecount: 00039},
  file = {/Users/fariedabuzaid/Zotero/storage/S55B524E/Klein et al. - 2017 - Fast Bayesian Optimization of Machine Learning Hyp.pdf}
}

@book{klenke_probability_2014,
  title = {Probability {{Theory}}. {{A}} Comprehensive Course},
  author = {Klenke, Achim},
  year = {2014},
  month = jan,
  series = {Universitext},
  edition = {Second},
  publisher = {{Springer London}},
  doi = {10.1007/978-1-4471-5361-0},
  copyright = {\textcopyright 2014 Springer-Verlag London},
  isbn = {978-1-4471-5360-3 978-1-4471-5361-0},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/7HKN9YGX/Klenke - 2014 - Probability Theory. A comprehensive course.pdf}
}

@article{knothe_contributions_1957,
  title = {Contributions to the Theory of Convex Bodies.},
  author = {Knothe, Herbert},
  year = {1957},
  month = jan,
  journal = {Michigan Mathematical Journal},
  volume = {4},
  number = {1},
  pages = {39--52},
  publisher = {{University of Michigan, Department of Mathematics}},
  issn = {0026-2285, 1945-2365},
  doi = {10.1307/mmj/1028990175},
  abstract = {The Michigan Mathematical Journal},
  file = {/Users/fariedabuzaid/Zotero/storage/C7B8WDYM/Knothe - 1957 - Contributions to the theory of convex bodies..pdf;/Users/fariedabuzaid/Zotero/storage/AXAF46I5/1028990175.html}
}

@article{kobyzev_normalizing_2020,
  title = {Normalizing {{Flows}}: {{An Introduction}} and {{Review}} of {{Current Methods}}},
  shorttitle = {Normalizing {{Flows}}},
  author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
  year = {2020},
  month = apr,
  journal = {arXiv:1908.09257 [cs, stat]},
  eprint = {1908.09257},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00000},
  file = {/Users/fariedabuzaid/Zotero/storage/TDETIDQH/Kobyzev et al. - 2020 - Normalizing Flows An Introduction and Review of C.pdf}
}

@inproceedings{koh_accuracy_2019,
  title = {On the Accuracy of Influence Functions for Measuring Group Effects},
  booktitle = {Proceedings of {{Advances}} in {{Neural Information Processing Systems}}},
  author = {Koh, Pang Wei W and Ang, Kai-Siang and Teo, Hubert and Liang, Percy S},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  month = dec,
  volume = {32},
  eprint = {1905.13289},
  eprinttype = {arxiv},
  publisher = {{Curran Associates, Inc.}},
  address = {{Vancouver, Canada}},
  abstract = {Influence functions estimate the effect of removing a training point on a model without the need to retrain. They are based on a first-order Taylor approximation that is guaranteed to be accurate for sufficiently small changes to the model, and so are commonly used to study the effect of individual points in large datasets. However, we often want to study the effects of large groups of training points, e.g., to diagnose batch effects or apportion credit between different data sources. Removing such large groups can result in significant changes to the model. Are influence functions still accurate in this setting? In this paper, we find that across many different types of groups and for a range of real-world datasets, the predicted effect (using influence functions) of a group correlates surprisingly well with its actual effect, even if the absolute and relative errors are large. Our theoretical analysis shows that such strong correlation arises only under certain settings and need not hold in general, indicating that real-world datasets have particular properties that allow the influence approximation to be accurate.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/72ZABIXV/Koh et al. - 2019 - On the accuracy of influence functions for measuri.pdf}
}

@inproceedings{koh_understanding_2017,
  title = {Understanding {{Black-box Predictions}} via {{Influence Functions}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Koh, Pang Wei and Liang, Percy},
  year = {2017},
  month = jul,
  eprint = {1703.04730},
  eprinttype = {arxiv},
  pages = {1885--1894},
  publisher = {{PMLR}},
  abstract = {How can we explain the predictions of a black-box model? In this paper, we use influence functions \textemdash{} a classic technique from robust statistics \textemdash{} to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/HW6BK957/D9MZ9MUI.pdf;/Users/fariedabuzaid/Zotero/storage/J4YQYLEN/Koh and Liang - 2017 - Understanding Black-box Predictions via Influence .pdf;/Users/fariedabuzaid/Zotero/storage/MHFVLFHJ/Koh and Liang - 2017 - Understanding Black-box Predictions via Influence .pdf}
}

@inproceedings{kong_resolving_2022,
  title = {Resolving {{Training Biases}} via {{Influence-based Data Relabeling}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Kong, Shuming and Shen, Yanyan and Huang, Linpeng},
  year = {2022},
  abstract = {The performance of supervised learning methods easily suffers from the training bias issue caused by train-test distribution mismatch or label noise. Influence function is a  technique that...},
  langid = {english},
  annotation = {video:https://iclr.cc/virtual/2022/oral/6492},
  file = {/Users/fariedabuzaid/Zotero/storage/FYVM434U/Kong et al. - 2021 - Resolving Training Biases via Influence-based Data.pdf}
}

@article{kraska_case_2017,
  title = {The {{Case}} for {{Learned Index Structures}}},
  author = {Kraska, Tim and Beutel, Alex and Chi, Ed H. and Dean, Jeffrey and Polyzotis, Neoklis},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.01208 [cs]},
  eprint = {1712.01208},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Indexes are models: a B-Tree-Index can be seen as a model to map a key to the position of a record within a sorted array, a Hash-Index as a model to map a key to a position of a record within an unsorted array, and a BitMap-Index as a model to indicate if a data record exists or not. In this exploratory research paper, we start from this premise and posit that all existing index structures can be replaced with other types of models, including deep-learning models, which we term learned indexes. The key idea is that a model can learn the sort order or structure of lookup keys and use this signal to effectively predict the position or existence of records. We theoretically analyze under which conditions learned indexes outperform traditional index structures and describe the main challenges in designing learned index structures. Our initial results show, that by using neural nets we are able to outperform cache-optimized B-Trees by up to 70\% in speed while saving an order-of-magnitude in memory over several real-world data sets. More importantly though, we believe that the idea of replacing core components of a data management system through learned models has far reaching implications for future systems designs and that this work just provides a glimpse of what might be possible.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {citecount: 00005},
  file = {/Users/fariedabuzaid/Zotero/storage/NP2DJWB3/Kraska et al. - 2017 - The Case for Learned Index Structures.pdf}
}

@inproceedings{kreuzer_rethinking_2021,
  title = {Rethinking {{Graph Transformers}} with {{Spectral Attention}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kreuzer, Devin and Beaini, Dominique and Hamilton, William L. and L{\'e}tourneau, Vincent and Tossou, Prudencio},
  year = {2021},
  abstract = {The first fully-connected Transformer model to perform well on graph-structured data.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/6QSHTQHR/Kreuzer et al. - 2022 - Rethinking Graph Transformers with Spectral Attent.pdf}
}

@article{krizhevsky_imagenet_2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  journal = {Communications of the ACM},
  volume = {60},
  number = {6},
  pages = {84--90},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/5X6J4HBQ/Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf}
}

@techreport{krizhevsky_learning_2009,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  year = {2009},
  month = apr,
  pages = {60},
  institution = {{Canadian Institute for Advanced Research}},
  abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/4DYA969A/Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf}
}

@inproceedings{kuleshov_accurate_2018,
  title = {Accurate {{Uncertainties}} for {{Deep Learning Using Calibrated Regression}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Kuleshov, Volodymyr and Fenner, Nathan and Ermon, Stefano},
  year = {2018},
  month = jul,
  pages = {2796--2804},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate \{\textemdash\} for example, a 90\% credible interval may not contain the true outcome 90\% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/6WX4NZ7J/Kuleshov et al. - 2018 - Accurate Uncertainties for Deep Learning Using Cal.pdf}
}

@article{kull_sigmoids_2017,
  title = {Beyond Sigmoids: {{How}} to Obtain Well-Calibrated Probabilities from Binary Classifiers with Beta Calibration},
  shorttitle = {Beyond Sigmoids},
  author = {Kull, Meelis and Filho, Telmo M. Silva and Flach, Peter},
  year = {2017},
  journal = {Electronic Journal of Statistics},
  volume = {11},
  number = {2},
  pages = {5052--5080},
  issn = {1935-7524, 1935-7524},
  doi = {10.1214/17-EJS1338SI},
  abstract = {For optimal decision making under variable class distributions and misclassification costs a classifier needs to produce well-calibrated estimates of the posterior probability. Isotonic calibration is a powerful non-parametric method that is however prone to overfitting on smaller datasets; hence a parametric method based on the logistic sigmoidal curve is commonly used. While logistic calibration is designed for normally distributed per-class scores, we demonstrate experimentally that many classifiers including Naive Bayes and Adaboost suffer from a particular distortion where these score distributions are heavily skewed. In such cases logistic calibration can easily yield probability estimates that are worse than the original scores. Moreover, the logistic curve family does not include the identity function, and hence logistic calibration can easily uncalibrate a perfectly calibrated classifier. In this paper we solve all these problems with a richer class of parametric calibration maps based on the beta distribution. We derive the method from first principles and show that fitting it is as easy as fitting a logistic curve. Extensive experiments show that beta calibration is superior to logistic calibration for a wide range of classifiers: Naive Bayes, Adaboost, random forest, logistic regression, support vector machine and multi-layer perceptron. If the original classifier is already calibrated, then beta calibration learns a function close to the identity. On this we build a statistical test to recognise if the model deviates from being well-calibrated.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/X9UD4Q4L/Kull et al. - 2017 - Beyond sigmoids How to obtain well-calibrated pro.pdf}
}

@incollection{kull_temperature_2019,
  title = {Beyond Temperature Scaling: {{Obtaining}} Well-Calibrated Multi-Class Probabilities with {{Dirichlet}} Calibration},
  shorttitle = {Beyond Temperature Scaling},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Kull, Meelis and Perello Nieto, Miquel and K{\"a}ngsepp, Markus and Silva Filho, Telmo and Song, Hao and Flach, Peter},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {12316--12326},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Class probabilities predicted by most multiclass classifiers are uncalibrated, often tending towards over-confidence. With neural networks, calibration can be im proved by temperature scaling, a method to learn a single corrective multiplicative factor for inputs to the last softmax layer. On non-neural models the existing methods apply binary calibration in a pairwise or one-vs-rest fashion. We propose a natively multiclass calibration method applicable to classifiers from any model class, derived from Dirichlet distributions and generalising the beta calibration method from binary classification. It is easily implemented with neural nets since it is equivalent to log-transforming the uncalibrated probabilities, followed by one lin ear layer and softmax. Experiments demonstrate improved probabilistic predictions according to multiple measures (confidence-ECE, classwise-ECE, log-loss, Brier score) across a wide range of datasets and classifiers. Parameters of the learned Dirichlet calibration map provide insights to the biases in the uncalibrated model.},
  annotation = {citecount: 00000},
  file = {/Users/fariedabuzaid/Zotero/storage/LKAJNKIH/Kull et al. - 2019 - Beyond temperature scaling Obtaining well-calibra.pdf}
}

@inproceedings{kumar_problems_2020,
  title = {Problems with {{Shapley-value-based}} Explanations as Feature Importance Measures},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Kumar, I. Elizabeth and Venkatasubramanian, Suresh and Scheidegger, Carlos and Friedler, Sorelle},
  year = {2020},
  month = nov,
  eprint = {2002.11097},
  eprinttype = {arxiv},
  pages = {5491--5500},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Game-theoretic formulations of feature importance have become popular as a way to},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/6BMV9PIA/Kumar et al. - 2020 - Problems with Shapley-value-based explanations as .pdf}
}

@inproceedings{kumar_stabilizing_2019,
  title = {Stabilizing {{Off-Policy Q-Learning}} via {{Bootstrapping Error Reduction}}},
  booktitle = {{{arXiv}}:1906.00949 [Cs, Stat]},
  author = {Kumar, Aviral and Fu, Justin and Tucker, George and Levine, Sergey},
  year = {2019},
  month = nov,
  eprint = {1906.00949},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Off-policy reinforcement learning aims to leverage experience collected from prior policies for sample-efficient learning. However, in practice, commonly used off-policy approximate dynamic programming methods based on Q-learning and actor-critic methods are highly sensitive to the data distribution, and can make only limited progress without collecting additional on-policy data. As a step towards more robust off-policy algorithms, we study the setting where the off-policy experience is fixed and there is no further interaction with the environment. We identify bootstrapping error as a key source of instability in current methods. Bootstrapping error is due to bootstrapping from actions that lie outside of the training data distribution, and it accumulates via the Bellman backup operator. We theoretically analyze bootstrapping error, and demonstrate how carefully constraining action selection in the backup can mitigate it. Based on our analysis, we propose a practical algorithm, bootstrapping error accumulation reduction (BEAR). We demonstrate that BEAR is able to learn robustly from different off-policy distributions, including random and suboptimal demonstrations, on a range of continuous control tasks.},
  archiveprefix = {arXiv},
  annotation = {web: https://sites.google.com/view/bear-off-policyrl},
  file = {/Users/fariedabuzaid/Zotero/storage/5CHFJBGL/Kumar et al. - 2019 - Stabilizing Off-Policy Q-Learning via Bootstrappin.pdf}
}

@inproceedings{kumar_trainable_2018,
  title = {Trainable {{Calibration Measures}} for {{Neural Networks}} from {{Kernel Mean Embeddings}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Kumar, Aviral and Sarawagi, Sunita and Jain, Ujjwal},
  year = {2018},
  month = jul,
  pages = {2805--2814},
  issn = {1938-7228},
  abstract = {Modern neural networks have recently been found to be poorly calibrated, primarily in the direction of over-confidence. Methods like entropy penalty and temperature smoothing improve calibration by...},
  chapter = {Machine Learning},
  langid = {english},
  annotation = {citecount: 00000},
  file = {/Users/fariedabuzaid/Zotero/storage/3CGF42NH/Kumar et al. - 2018 - Trainable Calibration Measures for Neural Networks.pdf;/Users/fariedabuzaid/Zotero/storage/L3CWXQKE/kumar18a-supp.pdf}
}

@inproceedings{kumar_verified_2019,
  title = {Verified {{Uncertainty Calibration}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Kumar, Ananya and Liang, Percy S and Ma, Tengyu},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  eprint = {1909.10155},
  eprinttype = {arxiv},
  pages = {3792--3803},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Applications such as weather forecasting and personalized medicine demand models that output calibrated probability estimates---those representative of the true likelihood of a prediction. Most models are not calibrated out of the box but are recalibrated by post-processing model outputs. We find in this work that popular recalibration methods like Platt scaling and temperature scaling are (i) less calibrated than reported, and (ii) current techniques cannot estimate how miscalibrated they are. An alternative method, histogram binning, has measurable calibration error but is sample inefficient---it requires \$O(B/\textbackslash epsilon\^2)\$ samples, compared to \$O(1/\textbackslash epsilon\^2)\$ for scaling methods, where \$B\$ is the number of distinct probabilities the model can output. To get the best of both worlds, we introduce the scaling-binning calibrator, which first fits a parametric function that acts like a baseline for variance reduction and then bins the function values to actually ensure calibration. This requires only \$O(1/\textbackslash epsilon\^2 + B)\$ samples. We then show that methods used to estimate calibration error are suboptimal---we prove that an alternative estimator introduced in the meteorological community requires fewer samples (\$O(\textbackslash sqrt\{B\})\$ instead of \$O(B)\$). We validate our approach with multiclass calibration experiments on CIFAR-10 and ImageNet, where we obtain a 35\textbackslash\% lower calibration error than histogram binning and, unlike scaling methods, guarantees on true calibration.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {citecount: 00008},
  file = {/Users/fariedabuzaid/Zotero/storage/AURCWUKF/Kumar et al. - 2019 - Verified Uncertainty Calibration.pdf}
}

@inproceedings{kuppers_bayesian_2021,
  title = {Bayesian {{Confidence Calibration}} for {{Epistemic Uncertainty Modelling}}},
  booktitle = {{{arXiv}}:2109.10092 [Cs]},
  author = {K{\"u}ppers, Fabian and Kronenberger, Jan and Schneider, Jonas and Haselhoff, Anselm},
  year = {2021},
  month = sep,
  eprint = {2109.10092},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Modern neural networks have found to be miscalibrated in terms of confidence calibration, i.e., their predicted confidence scores do not reflect the observed accuracy or precision. Recent work has introduced methods for post-hoc confidence calibration for classification as well as for object detection to address this issue. Especially in safety critical applications, it is crucial to obtain a reliable self-assessment of a model. But what if the calibration method itself is uncertain, e.g., due to an insufficient knowledge base? We introduce Bayesian confidence calibration - a framework to obtain calibrated confidence estimates in conjunction with an uncertainty of the calibration method. Commonly, Bayesian neural networks (BNN) are used to indicate a network's uncertainty about a certain prediction. BNNs are interpreted as neural networks that use distributions instead of weights for inference. We transfer this idea of using distributions to confidence calibration. For this purpose, we use stochastic variational inference to build a calibration mapping that outputs a probability distribution rather than a single calibrated estimate. Using this approach, we achieve state-of-the-art calibration performance for object detection calibration. Finally, we show that this additional type of uncertainty can be used as a sufficient criterion for covariate shift detection. All code is open source and available at https://github.com/EFS-OpenSource/calibration-framework.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/BCMUI8EP/Küppers et al. - 2021 - Bayesian Confidence Calibration for Epistemic Unce.pdf}
}

@inproceedings{kuppers_multivariate_2020,
  title = {Multivariate {{Confidence Calibration}} for {{Object Detection}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {K{\"u}ppers, Fabian and Kronenberger, Jan and Shantia, Amirhossein and Haselhoff, Anselm},
  year = {2020},
  month = jun,
  eprint = {2004.13546},
  eprinttype = {arxiv},
  pages = {1322--1330},
  doi = {10.1109/CVPRW50498.2020.00171},
  abstract = {Unbiased confidence estimates of neural networks are crucial especially for safety-critical applications. Many methods have been developed to calibrate biased confidence estimates. Though there is a variety of methods for classification, the field of object detection has not been addressed yet. Therefore, we present a novel framework to measure and calibrate biased (or miscalibrated) confidence estimates of object detection methods. The main difference to related work in the field of classifier calibration is that we also use additional information of the regression output of an object detector for calibration. Our approach allows, for the first time, to obtain calibrated confidence estimates with respect to image location and box scale. In addition, we propose a new measure to evaluate miscalibration of object detectors. Finally, we show that our developed methods outperform state-of-the-art calibration models for the task of object detection and provides reliable confidence estimates across different locations and scales.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/ZZYIT6VP/Küppers et al. - 2020 - Multivariate Confidence Calibration for Object Det.pdf}
}

@misc{kurakin_adversarial_2017,
  title = {Adversarial {{Machine Learning}} at {{Scale}}},
  author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  year = {2017},
  month = feb,
  number = {arXiv:1611.01236},
  eprint = {1611.01236},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1611.01236},
  abstract = {Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a "label leaking" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/L64NFY6Z/Kurakin et al. - 2017 - Adversarial Machine Learning at Scale.pdf;/Users/fariedabuzaid/Zotero/storage/Y7XED3AK/1611.html}
}

@misc{kurakin_adversarial_2017a,
  title = {Adversarial Examples in the Physical World},
  author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
  year = {2017},
  month = feb,
  number = {arXiv:1607.02533},
  eprint = {1607.02533},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1607.02533},
  abstract = {Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/fariedabuzaid/Zotero/storage/6ALFWDK4/Kurakin et al. - 2017 - Adversarial examples in the physical world.pdf;/Users/fariedabuzaid/Zotero/storage/DJDBVVDP/1607.html}
}

@inproceedings{kwon_beta_2022,
  title = {Beta {{Shapley}}: A {{Unified}} and {{Noise-reduced Data Valuation Framework}} for {{Machine Learning}}},
  shorttitle = {Beta {{Shapley}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}}) 2022,},
  author = {Kwon, Yongchan and Zou, James},
  year = {2022},
  month = jan,
  volume = {151},
  eprint = {2110.14049},
  eprinttype = {arxiv},
  publisher = {{PMLR}},
  address = {{Valencia, Spain}},
  abstract = {Data Shapley has recently been proposed as a principled framework to quantify the contribution of individual datum in machine learning. It can effectively identify helpful or harmful data points for a learning algorithm. In this paper, we propose Beta Shapley, which is a substantial generalization of Data Shapley. Beta Shapley arises naturally by relaxing the efficiency axiom of the Shapley value, which is not critical for machine learning settings. Beta Shapley unifies several popular data valuation methods and includes data Shapley as a special case. Moreover, we prove that Beta Shapley has several desirable statistical properties and propose efficient algorithms to estimate it. We demonstrate that Beta Shapley outperforms state-of-the-art data valuation methods on several downstream ML tasks such as: 1) detecting mislabeled training data; 2) learning with subsamples; and 3) identifying points whose addition or removal have the largest positive or negative impact on the model.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/B4AQL6XG/Kwon and Zou - 2022 - Beta Shapley a Unified and Noise-reduced Data Val.pdf}
}

@inproceedings{kwon_efficient_2021,
  title = {Efficient {{Computation}} and {{Analysis}} of {{Distributional Shapley Values}}},
  booktitle = {Proceedings of the 24th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Kwon, Yongchan and Rivas, Manuel A. and Zou, James},
  year = {2021},
  month = mar,
  eprint = {2007.01357},
  eprinttype = {arxiv},
  pages = {793--801},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Distributional data Shapley value (DShapley) has recently been proposed as a principled framework to quantify the contribution of individual datum in machine learning. DShapley develops the founda...},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/WPZLVAG4/X5UC9XCI.pdf}
}

@inproceedings{lachapelle_gradientbased_2020,
  title = {Gradient-{{Based Neural DAG Learning}}},
  booktitle = {Eighth {{International Conference}} on {{Learning Representations}}},
  author = {Lachapelle, S{\'e}bastien and Brouillard, Philippe and Deleu, Tristan and {Lacoste-Julien}, Simon},
  year = {2020},
  month = apr,
  abstract = {We propose a novel score-based approach to learning a directed acyclic graph (DAG) from observational data. We adapt a recently proposed continuous constrained optimization formulation to allow for nonlinear relationships between variables using neural networks. This extension allows to model complex interactions while avoiding the combinatorial nature of the problem. In addition to comparing our method to existing continuous optimization methods, we provide missing empirical comparisons to nonlinear greedy search methods. On both synthetic and real-world data sets, this new method outperforms current continuous methods on most tasks while being competitive with existing greedy search methods on important metrics for causal inference.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/ISGSZUYN/Lachapelle et al. - 2020 - Gradient-Based Neural DAG Learning.pdf}
}

@article{ladicky_datadriven_2015,
  title = {Data-Driven Fluid Simulations Using Regression Forests},
  author = {Ladick{\'y}, L'ubor and Jeong, SoHyeon and Solenthaler, Barbara and Pollefeys, Marc and Gross, Markus},
  year = {2015},
  month = oct,
  journal = {ACM Transactions on Graphics},
  volume = {34},
  number = {6},
  pages = {199:1--199:9},
  issn = {0730-0301},
  doi = {10.1145/2816795.2818129},
  abstract = {Traditional fluid simulations require large computational resources even for an average sized scene with the main bottleneck being a very small time step size, required to guarantee the stability of the solution. Despite a large progress in parallel computing and efficient algorithms for pressure computation in the recent years, realtime fluid simulations have been possible only under very restricted conditions. In this paper we propose a novel machine learning based approach, that formulates physics-based fluid simulation as a regression problem, estimating the acceleration of every particle for each frame. We designed a feature vector, directly modelling individual forces and constraints from the Navier-Stokes equations, giving the method strong generalization properties to reliably predict positions and velocities of particles in a large time step setting on yet unseen test videos. We used a regression forest to approximate the behaviour of particles observed in the large training set of simulations obtained using a traditional solver. Our GPU implementation led to a speed-up of one to three orders of magnitude compared to the state-of-the-art position-based fluid solver and runs in real-time for systems with up to 2 million particles.}
}

@article{lagaris_artificial_1998,
  title = {Artificial Neural Networks for Solving Ordinary and Partial Differential Equations},
  author = {Lagaris, I. E. and Likas, A. and Fotiadis, D. I.},
  year = {1998},
  month = sep,
  journal = {IEEE Transactions on Neural Networks},
  volume = {9},
  number = {5},
  pages = {987--1000},
  issn = {1941-0093},
  doi = {10.1109/72.712178},
  abstract = {We present a method to solve initial and boundary value problems using artificial neural networks. A trial solution of the differential equation is written as a sum of two parts. The first part satisfies the initial/boundary conditions and contains no adjustable parameters. The second part is constructed so as not to affect the initial/boundary conditions. This part involves a feedforward neural network containing adjustable parameters (the weights). Hence by construction the initial/boundary conditions are satisfied and the network is trained to satisfy the differential equation. The applicability of this approach ranges from single ordinary differential equations (ODE), to systems of coupled ODE and also to partial differential equations (PDE). In this article, we illustrate the method by solving a variety of model problems and present comparisons with solutions obtained using the Galerkin finite element method for several cases of partial differential equations. With the advent of neuroprocessors and digital signal processors the method becomes particularly interesting due to the expected essential gains in the execution speed.},
  file = {/Users/fariedabuzaid/Zotero/storage/HP6ED2Q4/Lagaris et al. - 1998 - Artificial neural networks for solving ordinary an.pdf}
}

@article{lagaris_neuralnetwork_2000,
  title = {Neural-Network Methods for Boundary Value Problems with Irregular Boundaries},
  author = {Lagaris, I. E. and Likas, A. C. and Papageorgiou, D. G.},
  year = {2000},
  month = sep,
  journal = {IEEE Transactions on Neural Networks},
  volume = {11},
  number = {5},
  pages = {1041--1049},
  issn = {1941-0093},
  doi = {10.1109/72.870037},
  abstract = {Partial differential equations (PDEs) with boundary conditions (Dirichlet or Neumann) defined on boundaries with simple geometry have been successfully treated using sigmoidal multilayer perceptrons in previous works. The article deals with the case of complex boundary geometry, where the boundary is determined by a number of points that belong to it and are closely located, so as to offer a reasonable representation. Two networks are employed: a multilayer perceptron and a radial basis function network. The later is used to account for the exact satisfaction of the boundary conditions. The method has been successfully tested on two-dimensional and three-dimensional PDEs and has yielded accurate results.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/9R3L6EWE/Lagaris et al. - 2000 - Neural-network methods for boundary value problems.pdf}
}

@inproceedings{laidlaw_perceptual_2021,
  title = {Perceptual {{Adversarial Robustness}}: {{Defense Against Unseen Threat Models}}},
  shorttitle = {Perceptual {{Adversarial Robustness}}},
  booktitle = {{{arXiv}}:2006.12655 [Cs, Stat]},
  author = {Laidlaw, Cassidy and Singla, Sahil and Feizi, Soheil},
  year = {2021},
  month = jul,
  eprint = {2006.12655},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {A key challenge in adversarial robustness is the lack of a precise mathematical characterization of human perception, used in the very definition of adversarial attacks that are imperceptible to human eyes. Most current attacks and defenses try to avoid this issue by considering restrictive adversarial threat models such as those bounded by \$L\_2\$ or \$L\_\textbackslash infty\$ distance, spatial perturbations, etc. However, models that are robust against any of these restrictive threat models are still fragile against other threat models. To resolve this issue, we propose adversarial training against the set of all imperceptible adversarial examples, approximated using deep neural networks. We call this threat model the neural perceptual threat model (NPTM); it includes adversarial examples with a bounded neural perceptual distance (a neural network-based approximation of the true perceptual distance) to natural images. Through an extensive perceptual study, we show that the neural perceptual distance correlates well with human judgements of perceptibility of adversarial examples, validating our threat model. Under the NPTM, we develop novel perceptual adversarial attacks and defenses. Because the NPTM is very broad, we find that Perceptual Adversarial Training (PAT) against a perceptual attack gives robustness against many other types of adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against five diverse adversarial attacks. We find that PAT achieves state-of-the-art robustness against the union of these five attacks, more than doubling the accuracy over the next best model, without training against any of them. That is, PAT generalizes well to unforeseen perturbation types. This is vital in sensitive applications where a particular threat model cannot be assumed, and to the best of our knowledge, PAT is the first adversarial training defense with this property.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/9T2773KJ/Laidlaw et al. - 2021 - Perceptual Adversarial Robustness Defense Against.pdf}
}

@inproceedings{lakshminarayanan_simple_2017,
  title = {Simple and {{Scalable Predictive Uncertainty Estimation}} Using {{Deep Ensembles}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {6402--6413},
  publisher = {{Curran Associates, Inc.}},
  doi = {10.48550/ARXIV.1612.01474},
  abstract = {Deep neural networks (NNs) are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs, which learn a distribution over weights, are currently the state-of-the-art for estimating predictive uncertainty; however these require significant modifications to the training procedure and are computationally expensive compared to standard (non-Bayesian) NNs. We propose an alternative to Bayesian NNs that is simple to implement, readily parallelizable, requires very little hyperparameter tuning, and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks, we demonstrate that our method produces well-calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift, we evaluate the predictive uncertainty on test examples from known and unknown distributions, and show that our method is able to express higher uncertainty on out-of-distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.},
  annotation = {citecount: 00000},
  file = {/Users/fariedabuzaid/Zotero/storage/NHT7VP6U/Lakshminarayanan et al. - 2017 - Simple and Scalable Predictive Uncertainty Estimat.pdf;/Users/fariedabuzaid/Zotero/storage/UGTW8GAG/supplementary.pdf}
}

@article{lan_perfect_2021,
  title = {Perfect Density Models Cannot Guarantee Anomaly Detection},
  author = {Lan, Charline Le and Dinh, Laurent},
  year = {2021},
  month = dec,
  journal = {Entropy},
  volume = {23},
  number = {12},
  eprint = {2012.03808},
  eprinttype = {arxiv},
  pages = {1690},
  issn = {1099-4300},
  doi = {10.3390/e23121690},
  abstract = {Thanks to the tractability of their likelihood, several deep generative models show promise for seemingly straightforward but important applications like anomaly detection, uncertainty estimation, and active learning. However, the likelihood values empirically attributed to anomalies conflict with the expectations these proposed applications suggest. In this paper, we take a closer look at the behavior of distribution densities through the lens of reparametrization and show that these quantities carry less meaningful information than previously thought, beyond estimation issues or the curse of dimensionality. We conclude that the use of these likelihoods for anomaly detection relies on strong and implicit hypotheses, and highlight the necessity of explicitly formulating these assumptions for reliable anomaly detection.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/ZYZ49BU8/Lan und Dinh - 2021 - Perfect density models cannot guarantee anomaly de.pdf;/Users/fariedabuzaid/Zotero/storage/2JRNZ4DU/2012.html}
}

@article{lee_efficient_2020,
  title = {Efficient {{Ensemble Model Generation}} for {{Uncertainty Estimation}} with {{Bayesian Approximation}} in {{Segmentation}}},
  author = {Lee, Hong Joo and Kim, Seong Tae and Lee, Hakmin and Navab, Nassir and Ro, Yong Man},
  year = {2020},
  month = may,
  journal = {arXiv [cs.CV]},
  eprint = {2005.10754},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/ARXIV.2005.10754},
  abstract = {Recent studies have shown that ensemble approaches could not only improve accuracy and but also estimate model uncertainty in deep learning. However, it requires a large number of parameters according to the increase of ensemble models for better prediction and uncertainty estimation. To address this issue, a generic and efficient segmentation framework to construct ensemble segmentation models is devised in this paper. In the proposed method, ensemble models can be efficiently generated by using the stochastic layer selection method. The ensemble models are trained to estimate uncertainty through Bayesian approximation. Moreover, to overcome its limitation from uncertain instances, we devise a new pixel-wise uncertainty loss, which improves the predictive performance. To evaluate our method, comprehensive and comparative experiments have been conducted on two datasets. Experimental results show that the proposed method could provide useful uncertainty information by Bayesian approximation with the efficient ensemble model generation and improve the predictive performance.},
  archiveprefix = {arXiv}
}

@article{leskovec_kronecker_2010,
  title = {Kronecker {{Graphs}}: {{An Approach}} to {{Modeling Networks}}},
  shorttitle = {Kronecker {{Graphs}}},
  author = {Leskovec, Jure and Chakrabarti, Deepayan and Kleinberg, Jon and Faloutsos, Christos and Ghahramani, Zoubin},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {33},
  pages = {985--1042},
  issn = {1533-7928},
  abstract = {How can we generate realistic networks? In addition, how can we do so with a mathematically tractable model that allows for rigorous analysis of network properties? Real networks exhibit a long list of surprising properties: Heavy tails for the in- and out-degree distribution, heavy tails for the eigenvalues and eigenvectors, small diameters, and densification and shrinking diameters over time. Current network models and generators either fail to match several of the above properties, are complicated to analyze mathematically, or both. Here we propose a generative model for networks that is both mathematically tractable and can generate networks that have all the above mentioned structural properties. Our main idea here is to use a non-standard matrix operation, the Kronecker product, to generate graphs which we refer to as "Kronecker graphs". First, we show that Kronecker graphs naturally obey common network properties. In fact, we rigorously prove that they do so. We also provide empirical evidence showing that Kronecker graphs can effectively model the structure of real networks. We then present KRONFIT, a fast and scalable algorithm for fitting the Kronecker graph generation model to large real networks. A naive approach to fitting would take super-exponential time. In contrast, KRONFIT takes linear time, by exploiting the structure of Kronecker matrix multiplication and by using statistical simulation techniques. Experiments on a wide range of large real and synthetic networks show that KRONFIT finds accurate parameters that very well mimic the properties of target networks. In fact, using just four parameters we can accurately model several aspects of global network structure. Once fitted, the model parameters can be used to gain insights about the network structure, and the resulting synthetic graphs can be used for null-models, anonymization, extrapolations, and graph summarization.},
  file = {/Users/fariedabuzaid/Zotero/storage/JIBPU43P/Leskovec et al. - 2010 - Kronecker Graphs An Approach to Modeling Networks.pdf}
}

@inproceedings{lewis_building_2006,
  title = {Building a Test Collection for Complex Document Information Processing},
  booktitle = {Proceedings of the 29th Annual International {{ACM SIGIR}} Conference on {{Research}} and Development in Information Retrieval  - {{SIGIR}} '06},
  author = {Lewis, D. and Agam, G. and Argamon, S. and Frieder, O. and Grossman, D. and Heard, J.},
  year = {2006},
  pages = {665},
  publisher = {{ACM Press}},
  address = {{Seattle, Washington, USA}},
  doi = {10.1145/1148170.1148307},
  abstract = {Research and development of information access technology for scanned paper documents has been hampered by the lack of public test collections of realistic scope and complexity. As part of a project to create a prototype system for search and mining of masses of document images, we are assembling a 1.5 terabyte dataset to support evaluation of both end-to-end complex document information processing (CDIP) tasks (e.g., text retrieval and data mining) as well as component technologies such optical character recognition (OCR), document structure analysis, signature matching, and authorship attribution.},
  isbn = {978-1-59593-369-0},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/KQU8FM58/Lewis et al. - 2006 - Building a test collection for complex document in.pdf}
}

@article{li_fourier_2020,
  title = {Fourier {{Neural Operator}} for {{Parametric Partial Differential Equations}}},
  author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.08895 [cs, math]},
  eprint = {2010.08895},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and the Navier-Stokes equation (including the turbulent regime). Our Fourier neural operator shows state-of-the-art performance compared to existing neural network methodologies and it is up to three orders of magnitude faster compared to traditional PDE solvers.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/WE75LVWG/Li et al. - 2020 - Fourier Neural Operator for Parametric Partial Dif.pdf}
}

@article{li_learning_2016,
  title = {Learning to {{Optimize}}},
  author = {Li, Ke and Malik, Jitendra},
  year = {2016},
  month = jun,
  journal = {arXiv:1606.01885 [cs, math, stat]},
  eprint = {1606.01885},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Algorithm design is a laborious process and often requires many iterations of ideation and validation. In this paper, we explore automating algorithm design and present a method to learn an optimization algorithm, which we believe to be the first method that can automatically discover a better algorithm. We approach this problem from a reinforcement learning perspective and represent any particular optimization algorithm as a policy. We learn an optimization algorithm using guided policy search and demonstrate that the resulting algorithm outperforms existing hand-engineered algorithms in terms of convergence speed and/or the final objective value.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00000},
  file = {/Users/fariedabuzaid/Zotero/storage/IAGM4Q2U/Li and Malik - 2016 - Learning to Optimize.pdf}
}

@article{li_learning_2017,
  title = {Learning to {{Optimize Neural Nets}}},
  author = {Li, Ke and Malik, Jitendra},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.00441 [cs, math, stat]},
  eprint = {1703.00441},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Learning to Optimize is a recently proposed framework for learning optimization algorithms using reinforcement learning. In this paper, we explore learning an optimization algorithm for training shallow neural nets. Such high-dimensional stochastic optimization problems present interesting challenges for existing reinforcement learning algorithms. We develop an extension that is suited to learning optimization algorithms in this setting and demonstrate that the learned optimization algorithm consistently outperforms other known optimization algorithms even on unseen tasks and is robust to changes in stochasticity of gradients and the neural net architecture. More specifically, we show that an optimization algorithm trained with the proposed method on the problem of training a neural net on MNIST generalizes to the problems of training neural nets on the Toronto Faces Dataset, CIFAR-10 and CIFAR-100.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00008},
  file = {/Users/fariedabuzaid/Zotero/storage/QKGAG8CR/Li and Malik - 2017 - Learning to Optimize Neural Nets.pdf}
}

@article{lin_focal_2017,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  year = {2017},
  month = aug,
  journal = {arXiv:1708.02002 [cs]},
  eprint = {1708.02002},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00082},
  file = {/Users/fariedabuzaid/Zotero/storage/FZSC5YQH/Lin et al. - 2017 - Focal Loss for Dense Object Detection.pdf}
}

@inproceedings{lin_uncomputability_2022,
  title = {On the {{Uncomputability}} of {{Partition Functions}} in {{Energy-Based Sequence Models}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Lin, Chu-Cheng and McCarthy, Arya D.},
  year = {2022},
  abstract = {In this paper, we argue that energy-based sequence models backed by expressive parametric families can result in uncomputable and inapproximable partition functions. Among other things, this makes...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/EZ9XQXNB/Lin and McCarthy - 2022 - On the Uncomputability of Partition Functions in E.pdf}
}

@inproceedings{lippe_efficient_2022,
  title = {Efficient {{Neural Causal Discovery}} without {{Acyclicity Constraints}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR2022}})},
  author = {Lippe, Phillip and Cohen, Taco and Gavves, Efstratios},
  year = {2022},
  abstract = {Learning the structure of a causal graphical model using both observational and interventional data is a fundamental problem in many scientific fields. A promising direction is continuous...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/9L2H5K65/Lippe et al. - 2021 - Efficient Neural Causal Discovery without Acyclici.pdf}
}

@inproceedings{liu_exact_2022,
  title = {{{EXACT}}: {{Scalable Graph Neural Networks Training}} via {{Extreme Activation Compression}}},
  shorttitle = {{{EXACT}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Liu, Zirui and Zhou, Kaixiong and Yang, Fan and Li, Li and Chen, Rui and Hu, Xia},
  year = {2022},
  abstract = {Training Graph Neural Networks (GNNs) on large graphs is a fundamental challenge due to the high memory usage, which is mainly occupied by activations (e.g., node embeddings). Previous works...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/FND88SIK/Liu et al. - 2021 - EXACT Scalable Graph Neural Networks Training via.pdf;/Users/fariedabuzaid/Zotero/storage/2PBQVCLB/forum.html}
}

@inproceedings{liu_isolation_2008,
  title = {Isolation {{Forest}}},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  year = {2008},
  month = dec,
  pages = {413--422},
  publisher = {{IEEE}},
  address = {{Pisa, Italy}},
  doi = {10.1109/ICDM.2008.17},
  abstract = {Most existing model-based approaches to anomaly detection construct a profile of normal instances, then identify instances that do not conform to the normal profile as anomalies. This paper proposes a fundamentally different model-based method that explicitly isolates anomalies instead of profiles normal points. To our best knowledge, the concept of isolation has not been explored in current literature. The use of isolation enables the proposed method, iForest, to exploit sub-sampling to an extent that is not feasible in existing methods, creating an algorithm which has a linear time complexity with a low constant and a low memory requirement. Our empirical evaluation shows that iForest performs favourably to ORCA, a near-linear time complexity distance-based method, LOF and Random Forests in terms of AUC and processing time, and especially in large data sets. iForest also works well in high dimensional problems which have a large number of irrelevant attributes, and in situations where training set does not contain any anomalies.},
  isbn = {978-0-7695-3502-9},
  langid = {english},
  annotation = {citecount: 00000},
  file = {/Users/fariedabuzaid/Zotero/storage/YGB2P34L/Liu et al. - 2008 - Isolation Forest.pdf}
}

@inproceedings{liu_pretraining_2022,
  title = {Pre-Training {{Molecular Graph Representation}} with {{3D Geometry}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Liu, Shengchao and Wang, Hanchen and Liu, Weiyang and Lasenby, Joan and Guo, Hongyu and Tang, Jian},
  year = {2022},
  abstract = {Molecular graph representation learning is a fundamental problem in modern drug and material discovery. Molecular graphs are typically modeled by their 2D topological structures, but it has been...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/PQGSGY92/Liu et al. - 2022 - Pre-training Molecular Graph Representation with 3.pdf}
}

@inproceedings{liu_pyraformer_2022,
  title = {Pyraformer: {{Low-Complexity Pyramidal Attention}} for {{Long-Range Time Series Modeling}} and {{Forecasting}}},
  shorttitle = {Pyraformer},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Liu, Shizhan and Yu, Hang and Liao, Cong and Li, Jianguo and Lin, Weiyao and Liu, Alex X. and Dustdar, Schahram},
  year = {2022},
  abstract = {Accurate prediction of the future given the past based on time series data is of paramount importance, since it opens the door for decision making and risk management ahead of time. In practice...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/LIK43KR4/Liu et al. - 2022 - Pyraformer Low-Complexity Pyramidal Attention for.pdf}
}

@inproceedings{liu_spherical_2022,
  title = {Spherical {{Message Passing}} for {{3D Molecular Graphs}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Liu, Yi and Wang, Limei and Liu, Meng and Lin, Yuchao and Zhang, Xuan and Oztekin, Bora and Ji, Shuiwang},
  year = {2022},
  abstract = {We consider representation learning of 3D molecular graphs in which each atom is associated with a spatial position in 3D. This is an under-explored area of research, and a principled message...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/3HUY8SWN/Liu et al. - 2022 - Spherical Message Passing for 3D Molecular Graphs.pdf}
}

@inproceedings{long_pdenet_2018,
  title = {{{PDE-Net}}: {{Learning PDEs}} from {{Data}}},
  shorttitle = {{{PDE-Net}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Long, Zichao and Lu, Yiping and Ma, Xianzhong and Dong, Bin},
  year = {2018},
  month = jul,
  eprint = {1710.09668},
  eprinttype = {arxiv},
  pages = {3208--3216},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Partial differential equations (PDEs) play a prominent role in many disciplines of science and engineering. PDEs are commonly derived based on empirical observations. However, with the rapid develo...},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/SAD6Q2KQ/Long et al. - 2017 - PDE-Net Learning PDEs from Data.pdf}
}

@inproceedings{lu_learning_2022,
  title = {Learning {{Guarantees}} for {{Graph Convolutional Networks}} on the {{Stochastic Block Model}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Lu, Wei},
  year = {2022},
  abstract = {An abundance of neural network models and algorithms for diverse tasks on graphs have been developed in the past five years. However, very few provable guarantees have been available for the...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/LFEGRG7U/Lu - 2022 - Learning Guarantees for Graph Convolutional Networ.pdf}
}

@misc{lu_lululxvi_2021,
  title = {Lululxvi/Deepxde},
  author = {Lu, Lu},
  year = {2021},
  month = jan,
  abstract = {Deep learning library for solving differential equations and more},
  copyright = {Apache-2.0 License         ,                 Apache-2.0 License}
}

@article{luceno_fitting_2006,
  ids = {luceno_fitting_2006a},
  title = {Fitting the Generalized {{Pareto}} Distribution to Data Using Maximum Goodness-of-Fit Estimators},
  author = {Luce{\~n}o, Alberto},
  year = {2006},
  month = nov,
  journal = {Computational Statistics \& Data Analysis},
  volume = {51},
  number = {2},
  pages = {904--917},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2005.09.011},
  abstract = {Some of the most powerful techniques currently available to test the goodness of fit of a hypothesized continuous cumulative distribution function (CDF) use statistics based on the empirical distribution function (EDF), such as those of Kolmogorov, Cramer-von Mises and Anderson-Darling, among others. The use of EDF statistics was analyzed for estimation purposes. In this approach, maximum goodness-of-fit estimators (also called minimum distance estimators) of the parameters of the CDF can be obtained by minimizing any of the EDF statistics with respect to the unknown parameters. The results showed that there is no unique EDF statistic that can be considered most efficient for all situations. Consequently, the possibility of defining new EDF statistics is entertained; in particular, an Anderson-Darling statistic of degree two and one-sided Anderson-Darling statistics of degree one and two appear to be notable in some situations. The procedure is shown to be able to deal successfully with the estimation of the parameters of homogeneous and heterogeneous generalized Pareto distributions, even when maximum likelihood and other estimation methods fail.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/RJFZ49CW/Fitting_the_generalized_Pareto_distribut.pdf}
}

@incollection{lundberg_unified_2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Lundberg, Scott M and Lee, Su-In},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {4765--4774},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as theprediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle tointerpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches},
  file = {/Users/fariedabuzaid/Zotero/storage/MQMECKHP/7IK89B8V.pdf}
}

@inproceedings{ma_homophily_2022,
  title = {Is {{Homophily}} a {{Necessity}} for {{Graph Neural Networks}}?},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Ma, Yao and Liu, Xiaorui and Shah, Neil and Tang, Jiliang},
  year = {2022},
  abstract = {Graph neural networks (GNNs) have shown great prowess in learning representations suitable for numerous graph-based machine learning tasks. When applied to semi-supervised node classification...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/LRL5UGM9/Ma et al. - 2022 - Is Homophily a Necessity for Graph Neural Networks.pdf}
}

@inproceedings{ma_improving_2021,
  title = {Improving {{Uncertainty Calibration}} of {{Deep Neural Networks}} via {{Truth Discovery}} and {{Geometric Optimization}}},
  booktitle = {37th {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence}} ({{UAI}})},
  author = {Ma, Chunwei and Huang, Ziyun and Xian, Jiayi and Gao, Mingchen and Xu, Jinhui},
  year = {2021},
  month = jul,
  pages = {11},
  address = {{Online}},
  abstract = {Deep Neural Networks (DNNs), despite their tremendous success in recent years, could still cast doubts on their predictions due to the intrinsic uncertainty associated with their learning process. Ensemble techniques and post-hoc calibrations are two types of approaches that have individually shown promise in improving the uncertainty calibration of DNNs. However, the synergistic effect of the two types of methods has not been well explored. In this paper, we propose a truth discovery framework to integrate ensemble-based and posthoc calibration methods. Using the geometric variance of the ensemble candidates as a good indicator for sample uncertainty, we design an accuracypreserving truth estimator with provably no accuracy drop. Furthermore, we show that post-hoc calibration can also be enhanced by truth discoveryregularized optimization. On large-scale datasets including CIFAR and ImageNet, our method shows consistent improvement against state-of-the-art calibration approaches on both histogram-based and kernel density-based evaluation metrics. Our code is available at https://github.com/horsepurve/trulyuncertain.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/RFZD93EC/Ma et al. - Improving Uncertainty Calibration of Deep Neural N.pdf}
}

@article{maas_learning_2011,
  title = {Learning {{Word Vectors}} for {{Sentiment Analysis}}},
  author = {Maas, Andrew L and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  year = {2011},
  month = jun,
  journal = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},
  pages = {142--150},
  abstract = {Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term\textendash document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/G9D77QL5/Maas et al. - Learning Word Vectors for Sentiment Analysis.pdf}
}

@inproceedings{madry_deep_2018,
  title = {Towards {{Deep Learning Models Resistant}} to {{Adversarial Attacks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  year = {2018},
  month = feb,
  abstract = {We provide a principled, optimization-based re-look at the notion of adversarial examples, and develop methods that produce models that are adversarially robust against a wide range of adversaries.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/LUUKMYMD/Madry et al. - 2018 - Towards Deep Learning Models Resistant to Adversar.pdf}
}

@article{maleki_bounding_2014,
  title = {Bounding the {{Estimation Error}} of {{Sampling-based Shapley Value Approximation}}},
  author = {Maleki, Sasan and {Tran-Thanh}, Long and Hines, Greg and Rahwan, Talal and Rogers, Alex},
  year = {2014},
  month = feb,
  journal = {arXiv:1306.4265 [cs]},
  eprint = {1306.4265},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The Shapley value is arguably the most central normative solution concept in cooperative game theory. It specifies a unique way in which the reward from cooperation can be "fairly" divided among players. While it has a wide range of real world applications, its use is in many cases hampered by the hardness of its computation. A number of researchers have tackled this problem by (i) focusing on classes of games where the Shapley value can be computed efficiently, or (ii) proposing representation formalisms that facilitate such efficient computation, or (iii) approximating the Shapley value in certain classes of games. For the classical \textbackslash textit\{characteristic function\} representation, the only attempt to approximate the Shapley value for the general class of games is due to Castro \textbackslash textit\{et al.\} \textbackslash cite\{castro\}. While this algorithm provides a bound on the approximation error, this bound is \textbackslash textit\{asymptotic\}, meaning that it only holds when the number of samples increases to infinity. On the other hand, when a finite number of samples is drawn, an unquantifiable error is introduced, meaning that the bound no longer holds. With this in mind, we provide non-asymptotic bounds on the estimation error for two cases: where (i) the \textbackslash textit\{variance\}, and (ii) the \textbackslash textit\{range\}, of the players' marginal contributions is known. Furthermore, for the second case, we show that when the range is significantly large relative to the Shapley value, the bound can be improved (from \$O(\textbackslash frac\{r\}\{m\})\$ to \$O(\textbackslash sqrt\{\textbackslash frac\{r\}\{m\}\})\$). Finally, we propose, and demonstrate the effectiveness of using stratified sampling for improving the bounds further.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/WZ74JCN4/Maleki et al. - 2014 - Bounding the Estimation Error of Sampling-based Sh.pdf}
}

@article{marquardt_algorithm_1963,
  title = {An {{Algorithm}} for {{Least-Squares Estimation}} of {{Nonlinear Parameters}}},
  author = {Marquardt, Donald W.},
  year = {1963},
  month = jun,
  journal = {Journal of the Society for Industrial and Applied Mathematics},
  volume = {11},
  number = {2},
  pages = {431--441},
  issn = {0368-4245, 2168-3484},
  doi = {10.1137/0111030},
  abstract = {Most algorithms for the least-squares estimation of non-linear parameters have centered about either of two approaches. On the one hand, the model may be expanded as a Taylor series and corrections to the several parameters calculated at each iteration on the assumption of local linearity. On the other hand, various modifications of the method of steepest-descent have been used. Both methods not infrequently run aground, the Taylor series method because of divergence of the successive iterates, the steepest-descent (or gradient) methods because of agonizingly slow convergence after the first few iterations. In this paper a maximum neighborhood method is developed which, in effect, performs an optimum interpolation between the Taylor series method and the gradient method, the interpolation being based upon the maximum neighborhood in which the truncated Taylor series gives an adequate representation of the nonlinear model. The results are extended to the problem of solving a set of nonlinear algebraic equations.},
  langid = {english}
}

@article{marwah_parametric_2021,
  title = {Parametric {{Complexity Bounds}} for {{Approximating PDEs}} with {{Neural Networks}}},
  author = {Marwah, Tanya and Lipton, Zachary C. and Risteski, Andrej},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.02138 [cs, math, stat]},
  eprint = {2103.02138},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Recent empirical results show that deep networks can approximate solutions to high dimensional PDEs, seemingly escaping the curse of dimensionality. However many open questions remain regarding the theoretical basis for such approximations, including the number of parameters required. In this paper, we investigate the representational power of neural networks for approximating solutions to linear elliptic PDEs with Dirichlet Boundary conditions. We prove that when a PDE's coefficients are representable by small neural networks, the parameters required to approximate its solution scale polynomially with the input dimension \$d\$ and are proportional to the parameter counts of the coefficient neural networks. Our proof is based on constructing a neural network which simulates gradient descent in an appropriate Hilbert space which converges to the solution of the PDE. Moreover, we bound the size of the neural network needed to represent each iterate in terms of the neural network representing the previous iterate, resulting in a final network whose parameters depend polynomially on \$d\$ and does not depend on the volume of the domain.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/HWV3BJDS/Marwah et al. - 2021 - Parametric Complexity Bounds for Approximating PDE.pdf}
}

@misc{mat_,
  title = {{{MAT Tool}} by {{Intertec}} {$\cdot$} {{Boards}} {$\cdot$} {{appliedAI}} / Products / {{Maturity Assessment Tool}} / {{MAT Website}} {$\cdot$} {{GitLab}}},
  journal = {GitLab},
  abstract = {GitLab.com},
  howpublished = {https://gitlab.com/iaai/products/maturity-assessment-tool/mat-website/-/boards/4068441},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/2TAX3JBM/4068441.html}
}

@inproceedings{mazoure_leveraging_2020,
  title = {Leveraging Exploration in Off-Policy Algorithms via Normalizing Flows},
  booktitle = {Conference on {{Robot Learning}}},
  author = {Mazoure, Bogdan and Doan, Thang and Durand, Audrey and Pineau, Joelle and Hjelm, R. Devon},
  year = {2020},
  month = may,
  pages = {430--444},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {The ability to discover approximately optimal policies in domains with sparse rewards is crucial to applying reinforcement learning (RL) in many real-world scenarios. Approaches such as neural dens...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/3GIW2PHN/Mazoure et al. - 2020 - Leveraging exploration in off-policy algorithms vi.pdf}
}

@article{mcclure_robustly_2018,
  title = {Robustly Representing Uncertainty in Deep Neural Networks through Sampling},
  author = {McClure, Patrick and Kriegeskorte, Nikolaus},
  year = {2018},
  month = jan,
  eprint = {1611.01639},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.01639},
  abstract = {As deep neural networks (DNNs) are applied to increasingly challenging problems, they will need to be able to represent their own uncertainty. Modeling uncertainty is one of the key features of Bayesian methods. Using Bernoulli dropout with sampling at prediction time has recently been proposed as an efficient and well performing variational inference method for DNNs. However, sampling from other multiplicative noise based variational distributions has not been investigated in depth. We evaluated Bayesian DNNs trained with Bernoulli or Gaussian multiplicative masking of either the units (dropout) or the weights (dropconnect). We tested the calibration of the probabilistic predictions of Bayesian convolutional neural networks (CNNs) on MNIST and CIFAR-10. Sampling at prediction time increased the calibration of the DNNs' probabalistic predictions. Sampling weights, whether Gaussian or Bernoulli, led to more robust representation of uncertainty compared to sampling of units. However, using either Gaussian or Bernoulli dropout led to increased test set classification accuracy. Based on these findings we used both Bernoulli dropout and Gaussian dropconnect concurrently, which we show approximates the use of a spike-and-slab variational distribution without increasing the number of learned parameters. We found that spike-and-slab sampling had higher test set performance than Gaussian dropconnect and more robustly represented its uncertainty compared to Bernoulli dropout.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/RJXDL8YZ/McClure and Kriegeskorte - 2018 - Robustly representing uncertainty in deep neural n.pdf;/Users/fariedabuzaid/Zotero/storage/CUZ373S9/1611.html}
}

@inproceedings{mcclurepatrick_representing_2017,
  title = {Representing {{Inferential Uncertainty}} in {{Deep Neural Networks}} through {{Sampling}}},
  booktitle = {{{ICLR}}},
  author = {McClure, Patrick and Kriegeskorte, Nikolaus},
  year = {2017},
  abstract = {As deep neural networks (DNNs) are applied to increasingly challenging prob- lems, they will need to be able to represent their own uncertainty. Modelling uncertainty is one of the key features of Bayesian methods. Bayesian DNNs that use dropout-based variational distributions and scale to complex tasks have re- cently been proposed. We evaluate Bayesian DNNs trained with Bernoulli or Gaussian multiplicative masking of either the units (dropout) or the weights (drop- connect). We compare these Bayesian DNNs ability to represent their uncertainty about their outputs through sampling during inference. We tested the calibra- tion of these Bayesian fully connected and convolutional DNNs on two visual inference tasks (MNIST and CIFAR-10). By adding different levels of Gaussian noise to the test images in z-score space, we assessed how these DNNs repre- sented their uncertainty about regions of input space not covered by the training set. These Bayesian DNNs represented their own uncertainty more accurately than traditional DNNs with a softmax output. We find that sampling of weights, whether Gaussian or Bernoulli, led to more accurate representation of uncertainty compared to sampling of units. However, sampling units using either Gaussian or Bernoulli dropout led to increased convolutional neural network (CNN) clas- sification accuracy. Based on these findings we use both Bernoulli dropout and Gaussian dropconnect concurrently, which approximates the use of a spike-and- slab variational distribution. We find that networks with spike-and-slab sampling combine the advantages of the other methods: they classify with high accuracy and robustly represent the uncertainty of their classifications for all tested archi- tectures.}
}

@inproceedings{mccoy_right_2019,
  title = {Right for the {{Wrong Reasons}}: {{Diagnosing Syntactic Heuristics}} in {{Natural Language Inference}}},
  shorttitle = {Right for the {{Wrong Reasons}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {McCoy, Tom and Pavlick, Ellie and Linzen, Tal},
  year = {2019},
  month = jul,
  pages = {3428--3448},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1334},
  abstract = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.},
  file = {/Users/fariedabuzaid/Zotero/storage/Z85WRKBX/McCoy et al. - 2019 - Right for the Wrong Reasons Diagnosing Syntactic .pdf}
}

@book{mcelreath_statistical_2020,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{Stan}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  series = {Texts in {{Statistical Science}}},
  publisher = {{CRC Press}},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work.  The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding.  The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses.  Features   Integrates working code into the main text   Illustrates concepts through worked data analysis examples   Emphasizes understanding assumptions and how assumptions are reflected in code   Offers more detailed explanations of the mathematics in optional sections   Presents examples of using the dagitty R package to analyze causal graphs   Provides the rethinking R package on the author's website and on GitHub},
  googlebooks = {Ie2vxQEACAAJ},
  isbn = {978-0-367-13991-9},
  langid = {english},
  annotation = {extra: https://xcelab.net/rm/statistical-rethinking/}
}

@book{mcelreath_statistical_2020a,
  title = {Statistical {{Rethinking}}: {{A Bayesian Course}} with {{Examples}} in {{R}} and {{Stan}}},
  shorttitle = {Statistical {{Rethinking}}},
  author = {McElreath, Richard},
  year = {2020},
  month = mar,
  edition = {Second},
  publisher = {{Chapman and Hall/CRC}},
  address = {{New York}},
  doi = {10.1201/9780429029608},
  abstract = {Statistical Rethinking: A Bayesian Course with Examples in R and Stan builds your knowledge of and confidence in making inferences from data. Reflecting the need for scripting in today's model-based statistics, the book pushes you to perform step-by-step calculations that are usually automated. This unique computational approach ensures that you understand enough of the details to make reasonable choices and interpretations in your own modeling work. The text presents causal inference and generalized linear multilevel models from a simple Bayesian perspective that builds on information theory and maximum entropy. The core material ranges from the basics of regression to advanced multilevel models. It also presents measurement error, missing data, and Gaussian process models for spatial and phylogenetic confounding. The second edition emphasizes the directed acyclic graph (DAG) approach to causal inference, integrating DAGs into many examples. The new edition also contains new material on the design of prior distributions, splines, ordered categorical predictors, social relations models, cross-validation, importance sampling, instrumental variables, and Hamiltonian Monte Carlo. It ends with an entirely new chapter that goes beyond generalized linear modeling, showing how domain-specific scientific models can be built into statistical analyses. Features Integrates working code into the main text Illustrates concepts through worked data analysis examples Emphasizes understanding assumptions and how assumptions are reflected in code Offers more detailed explanations of the mathematics in optional sections Presents examples of using the dagitty R package to analyze causal graphs Provides the rethinking R package on the author's website and on GitHub},
  isbn = {978-0-429-02960-8}
}

@inproceedings{metzen_efficient_2022,
  title = {Efficient {{Certified Defenses Against Patch Attacks}} on {{Image Classifiers}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR2022}})},
  author = {Metzen, Jan Hendrik and Yatsura, Maksym},
  year = {2022},
  abstract = {Adversarial patches pose a realistic threat model for physical world attacks on autonomous systems via their perception component. Autonomous systems in safety-critical domains such as automated...},
  langid = {english},
  annotation = {slides: https://iclr.cc/media/iclr-2021/Slides/2629.pdf video: https://iclr.cc/virtual/2021/poster/2629},
  file = {/Users/fariedabuzaid/Zotero/storage/ED4A4DQU/Metzen and Yatsura - 2020 - Efficient Certified Defenses Against Patch Attacks.pdf}
}

@inproceedings{micikevicius_mixed_2018,
  title = {Mixed {{Precision Training}}},
  booktitle = {{{arXiv}}:1710.03740 [Cs, Stat]},
  author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  year = {2018},
  month = feb,
  eprint = {1710.03740},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyperparameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE halfprecision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to halfprecision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/SVHY9JYZ/Micikevicius et al. - 2018 - Mixed Precision Training.pdf}
}

@inproceedings{min_curious_2021,
  title = {The Curious Case of Adversarially Robust Models: {{More}} Data Can Help, Double Descend, or Hurt Generalization},
  shorttitle = {The Curious Case of Adversarially Robust Models},
  booktitle = {Proceedings of the {{Thirty-Seventh Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
  year = {2021},
  month = dec,
  pages = {129--139},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of a decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in classification problems. We first investigate the Gaussian mixture classification with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we analyze a two-dimensional classification problem with a 0-1 loss. We prove that more data always hurts generalization of adversarially trained models with large perturbations. Empirical studies confirm our theoretical results.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/B3RTTTFK/Min et al. - 2021 - The curious case of adversarially robust models M.pdf;/Users/fariedabuzaid/Zotero/storage/EA4FTNBK/Min et al. - 2021 - The curious case of adversarially robust models M.pdf}
}

@article{minderer_revisiting_2021,
  title = {Revisiting the {{Calibration}} of {{Modern Neural Networks}}},
  author = {Minderer, Matthias and Djolonga, Josip and Romijnders, Rob and Hubis, Frances and Zhai, Xiaohua and Houlsby, Neil and Tran, Dustin and Lucic, Mario},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.07998 [cs]},
  eprint = {2106.07998},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/I6KDYMJC/Minderer et al. - 2021 - Revisiting the Calibration of Modern Neural Networ.pdf}
}

@article{mirman_fundamental_2021,
  title = {The {{Fundamental Limits}} of {{Interval Arithmetic}} for {{Neural Networks}}},
  author = {Mirman, Matthew and Baader, Maximilian and Vechev, Martin},
  year = {2021},
  month = dec,
  journal = {arXiv:2112.05235 [cs]},
  eprint = {2112.05235},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Interval analysis (or interval bound propagation, IBP) is a popular technique for verifying and training provably robust deep neural networks, a fundamental challenge in the area of reliable machine learning. However, despite substantial efforts, progress on addressing this key challenge has stagnated, calling into question whether interval arithmetic is a viable path forward. In this paper we present two fundamental results on the limitations of interval arithmetic for analyzing neural networks. Our main impossibility theorem states that for any neural network classifying just three points, there is a valid specification over these points that interval analysis can not prove. Further, in the restricted case of one-hidden-layer neural networks we show a stronger impossibility result: given any radius \$\textbackslash alpha {$<$} 1\$, there is a set of \$O(\textbackslash alpha\^\{-1\})\$ points with robust radius \$\textbackslash alpha\$, separated by distance \$2\$, that no one-hidden-layer network can be proven to classify robustly via interval analysis.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/9B7FGMMG/Mirman et al. - 2021 - The Fundamental Limits of Interval Arithmetic for .pdf}
}

@inproceedings{moosavi-dezfooli_robustness_2019,
  title = {Robustness via {{Curvature Regularization}}, and {{Vice Versa}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Moosavi-Dezfooli}, Seyed-Mohsen and Fawzi, Alhussein and Uesato, Jonathan and Frossard, Pascal},
  year = {2019},
  eprint = {1811.09716},
  eprinttype = {arxiv},
  pages = {9078--9086},
  abstract = {State-of-the-art classifiers have been shown to be largely vulnerable to adversarial perturbations. One of the most effective strategies to improve robustness is adversarial training. In this paper, we investigate the effect of adversarial training on the geometry of the classification landscape and decision boundaries. We show in particular that adversarial training leads to a significant decrease in the curvature of the loss surface with respect to inputs, leading to a drastically more "linear" behaviour of the network. Using a locally quadratic approximation, we provide theoretical evidence on the existence of a strong relation between large robustness and small curvature. To further show the importance of reduced curvature for improving the robustness, we propose a new regularizer that directly minimizes curvature of the loss surface, and leads to adversarial robustness that is on par with adversarial training. Besides being a more efficient and principled alternative to adversarial training, the proposed regularizer confirms our claims on the importance of exhibiting quasi-linear behavior in the vicinity of data points in order to achieve robustness.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/PV4DUERR/Moosavi-Dezfooli et al. - 2019 - Robustness via Curvature Regularization, and Vice .pdf}
}

@inproceedings{moreta_ancestral_2022,
  title = {Ancestral Protein Sequence Reconstruction Using a Tree-Structured {{Ornstein-Uhlenbeck}} Variational Autoencoder},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Moreta, Lys Sanz and R{\o}nning, Ola and {Al-Sibahi}, Ahmad Salim and Hein, Jotun and Theobald, Douglas and Hamelryck, Thomas},
  year = {2022},
  abstract = {We introduce a deep generative model for representation learning of biological sequences that, unlike existing models, explicitly represents the evolutionary process. The model makes use of a...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/DNVW4DRN/Moreta et al. - 2021 - Ancestral protein sequence reconstruction using a .pdf;/Users/fariedabuzaid/Zotero/storage/F7MGUPSS/forum.html}
}

@article{moskovitz_understanding_2022,
  title = {Towards an {{Understanding}} of {{Default Policies}} in {{Multitask Policy Optimization}}},
  author = {Moskovitz, Ted and Arbel, Michael and {Parker-Holder}, Jack and Pacchiano, Aldo},
  year = {2022},
  month = mar,
  journal = {arXiv:2111.02994 [cs]},
  eprint = {2111.02994},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Much of the recent success of deep reinforcement learning has been driven by regularized policy optimization (RPO) algorithms with strong performance across multiple domains. In this family of methods, agents are trained to maximize cumulative reward while penalizing deviation in behavior from some reference, or default policy. In addition to empirical success, there is a strong theoretical foundation for understanding RPO methods applied to single tasks, with connections to natural gradient, trust region, and variational approaches. However, there is limited formal understanding of desirable properties for default policies in the multitask setting, an increasingly important domain as the field shifts towards training more generally capable agents. Here, we take a first step towards filling this gap by formally linking the quality of the default policy to its effect on optimization. Using these results, we then derive a principled RPO algorithm for multitask learning with strong performance guarantees.},
  archiveprefix = {arXiv},
  annotation = {video: https://virtual.aistats.org/virtual/2022/oral/3606},
  file = {/Users/fariedabuzaid/Zotero/storage/8YYPB3B9/Moskovitz et al. - 2022 - Towards an Understanding of Default Policies in Mu.pdf}
}

@inproceedings{mukhoti_calibrating_2020,
  title = {Calibrating Deep Neural Networks Using Focal Loss},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mukhoti, Jishnu and Kulharia, Viveka and Sanyal, Amartya and Golodetz, Stuart and Torr, Philip and Dokania, Puneet},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  eprint = {2002.09437},
  eprinttype = {arxiv},
  pages = {15288--15299},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Miscalibration -- a mismatch between a model's confidence and its correctness -- of Deep Neural Networks (DNNs) makes their predictions hard to rely on. Ideally, we want networks to be accurate, calibrated and confident. We show that, as opposed to the standard cross-entropy loss, focal loss (Lin et al., 2017) allows us to learn models that are already very well calibrated. When combined with temperature scaling, whilst preserving accuracy, it yields state-of-the-art calibrated models. We provide a thorough analysis of the factors causing miscalibration, and use the insights we glean from this to justify the empirically excellent performance of focal loss. To facilitate the use of focal loss in practice, we also provide a principled approach to automatically select the hyperparameter involved in the loss function. We perform extensive experiments on a variety of computer vision and NLP datasets, and with a wide variety of network architectures, and show that our approach achieves state-of-the-art calibration without compromising on accuracy in almost all cases.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/KXZJUMSI/Mukhoti et al. - 2020 - Calibrating Deep Neural Networks using Focal Loss.pdf}
}

@inproceedings{muller_transformers_2022,
  title = {Transformers {{Can Do Bayesian Inference}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  year = {2022},
  abstract = {Currently, it is hard to reap the benefits of deep learning for Bayesian methods, which allow the explicit specification of prior knowledge and accurately capture model uncertainty. We present...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/AEN3IQKV/Müller et al. - 2022 - Transformers Can Do Bayesian Inference.pdf}
}

@inproceedings{muller_when_2020,
  title = {When Does Label Smoothing Help?},
  booktitle = {{{arXiv}}:1906.02629 [Cs, Stat]},
  author = {M{\"u}ller, Rafael and Kornblith, Simon and Hinton, Geoffrey},
  year = {2020},
  month = jun,
  eprint = {1906.02629},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  address = {{Vancouver, Canada}},
  abstract = {The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/8MLCHR63/Müller et al. - 2020 - When Does Label Smoothing Help.pdf}
}

@article{nachum_reinforcement_2020,
  title = {Reinforcement {{Learning}} via {{Fenchel-Rockafellar Duality}}},
  author = {Nachum, Ofir and Dai, Bo},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.01866 [cs, stat]},
  eprint = {2001.01866},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We review basic concepts of convex duality, focusing on the very general and supremely useful Fenchel-Rockafellar duality. We summarize how this duality may be applied to a variety of reinforcement learning (RL) settings, including policy evaluation or optimization, online or offline learning, and discounted or undiscounted rewards. The derivations yield a number of intriguing results, including the ability to perform policy evaluation and on-policy policy gradient with behavior-agnostic offline data and methods to learn a policy via max-likelihood optimization. Although many of these results have appeared previously in various forms, we provide a unified treatment and perspective on these results, which we hope will enable researchers to better use and apply the tools of convex duality to make further progress in RL.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00002},
  file = {/Users/fariedabuzaid/Zotero/storage/V8IGDHGY/Nachum and Dai - 2020 - Reinforcement Learning via Fenchel-Rockafellar Dua.pdf}
}

@article{nagabandi_neural_2017,
  title = {Neural {{Network Dynamics}} for {{Model-Based Deep Reinforcement Learning}} with {{Model-Free Fine-Tuning}}},
  author = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S. and Levine, Sergey},
  year = {2017},
  month = aug,
  journal = {arXiv:1708.02596 [cs]},
  eprint = {1708.02596},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. Videos can be found at https://sites.google.com/view/mbmf},
  archiveprefix = {arXiv},
  annotation = {citecount: 00032  video: https://vimeo.com/252186751 blog: https://bair.berkeley.edu/blog/2017/11/30/model-based-rl/ website: https://sites.google.com/view/mbmf},
  file = {/Users/fariedabuzaid/Zotero/storage/LEVYF7CW/Nagabandi et al. - 2017 - Neural Network Dynamics for Model-Based Deep Reinf.pdf}
}

@inproceedings{nagarajan_understanding_2020,
  title = {Understanding the Failure Modes of Out-of-Distribution Generalization},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Nagarajan, Vaishnavh and Andreassen, Anders and Neyshabur, Behnam},
  year = {2020},
  month = sep,
  abstract = {Empirical studies suggest that machine learning models often rely on features, such as the background, that may be spuriously correlated with the label only during training time, resulting in poor...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/N4DGK3H8/Nagarajan et al. - 2020 - Understanding the failure modes of out-of-distribu.pdf;/Users/fariedabuzaid/Zotero/storage/II9DU2BL/forum.html}
}

@inproceedings{narayanan_iglu_2022,
  title = {{{IGLU}}: {{Efficient GCN Training}} via {{Lazy Updates}}},
  shorttitle = {{{IGLU}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Narayanan, S. Deepak and Sinha, Aditya and Jain, Prateek and Kar, Purushottam and Sellamanickam, Sundararajan},
  year = {2022},
  abstract = {Training multi-layer Graph Convolution Networks (GCN) using standard SGD techniques scales poorly as each descent step ends up updating node embeddings for a large portion of the graph. Recent...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/ICVEEY7V/Narayanan et al. - 2022 - IGLU Efficient GCN Training via Lazy Updates.pdf}
}

@incollection{nesterov_nonlinear_2004,
  title = {Nonlinear {{Optimization}}},
  booktitle = {Introductory {{Lectures}} on {{Convex Optimization}}: {{A Basic Course}}},
  author = {Nesterov, Yurii},
  editor = {Nesterov, Yurii},
  year = {2004},
  series = {Applied {{Optimization}}},
  pages = {1--50},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4419-8853-9_1},
  abstract = {General formulation of the problem; Important examples; Black box and iterative methods; Analytical and arithmetical complexity; Uniform grid method; Lower complexity bounds; Lower bounds for global optimization; Rules of the game.},
  isbn = {978-1-4419-8853-9},
  langid = {english}
}

@incollection{nesterov_nonsmooth_2004,
  title = {Nonsmooth {{Convex Optimization}}},
  booktitle = {Introductory {{Lectures}} on {{Convex Optimization}}: {{A Basic Course}}},
  author = {Nesterov, Yurii},
  editor = {Nesterov, Yurii},
  year = {2004},
  series = {Applied {{Optimization}}},
  pages = {111--170},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4419-8853-9_3},
  abstract = {(Equivalent definitions; Closed functions; Continuity of convex functions; Separation theorems; Subgradients; Computation rules; Optimality conditions.)},
  isbn = {978-1-4419-8853-9},
  langid = {english}
}

@incollection{nesterov_smooth_2004,
  title = {Smooth {{Convex Optimization}}},
  booktitle = {Introductory {{Lectures}} on {{Convex Optimization}}: {{A Basic Course}}},
  author = {Nesterov, Yurii},
  editor = {Nesterov, Yurii},
  year = {2004},
  series = {Applied {{Optimization}}},
  pages = {51--110},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4419-8853-9_2},
  isbn = {978-1-4419-8853-9},
  langid = {english}
}

@incollection{nesterov_structural_2004,
  title = {Structural {{Optimization}}},
  booktitle = {Introductory {{Lectures}} on {{Convex Optimization}}: {{A Basic Course}}},
  author = {Nesterov, Yurii},
  editor = {Nesterov, Yurii},
  year = {2004},
  series = {Applied {{Optimization}}},
  pages = {171--230},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4419-8853-9_4},
  abstract = {(Do we really have a black box? What the Newton method actually does? Definition of self-concordant functions; Main properties; Minimizing the self-concordant function.)},
  isbn = {978-1-4419-8853-9},
  langid = {english}
}

@article{nichol_glide_2022,
  title = {{{GLIDE}}: {{Towards Photorealistic Image Generation}} and {{Editing}} with {{Text-Guided Diffusion Models}}},
  shorttitle = {{{GLIDE}}},
  author = {Nichol, Alex and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and McGrew, Bob and Sutskever, Ilya and Chen, Mark},
  year = {2022},
  month = mar,
  journal = {arXiv:2112.10741 [cs]},
  eprint = {2112.10741},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5 billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/P3WJWIPV/Nichol et al. - 2022 - GLIDE Towards Photorealistic Image Generation and.pdf}
}

@inproceedings{niculescu-mizil_predicting_2005,
  title = {Predicting Good Probabilities with Supervised Learning},
  booktitle = {Proceedings of the 22nd International Conference on {{Machine}} Learning  - {{ICML}} '05},
  author = {{Niculescu-Mizil}, Alexandru and Caruana, Rich},
  year = {2005},
  pages = {625--632},
  publisher = {{ACM Press}},
  address = {{Bonn, Germany}},
  doi = {10.1145/1102351.1102430},
  abstract = {We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.},
  isbn = {978-1-59593-180-1},
  langid = {english},
  annotation = {citecount: 00000},
  file = {/Users/fariedabuzaid/Zotero/storage/DSWJJL7Y/Niculescu-Mizil and Caruana - 2005 - Predicting good probabilities with supervised lear.pdf}
}

@inproceedings{nixon_measuring_2020,
  title = {Measuring {{Calibration}} in {{Deep Learning}}},
  booktitle = {{{arXiv}}:1904.01685 [Cs, Stat]},
  author = {Nixon, Jeremy and Dusenberry, Michael W and Zhang, Linchuan and Jerfel, Ghassen and Tran, Dustin},
  year = {2020},
  month = aug,
  eprint = {1904.01685},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {14},
  abstract = {The reliability of a machine learning model's confidence in its predictions is critical for high-risk applications. Calibration\textemdash the idea that a model's predicted probabilities of outcomes reflect true probabilities of those outcomes\textemdash formalizes this notion. Current calibration metrics fail to consider all of the predictions made by machine learning models, and are inefficient in their estimation of the calibration error. We design the Adaptive Calibration Error (ACE) metric to resolve these pathologies and show that it outperforms other metrics, especially in settings where predictions beyond the maximum prediction that is chosen as the output class matter.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/CEC5EYJG/Nixon et al. - 2020 - Measuring Calibration in Deep Learning.pdf;/Users/fariedabuzaid/Zotero/storage/JKJ2UTC8/Nixon et al. - Measuring Calibration in Deep Learning.pdf}
}

@book{nocedal_numerical_2006,
  title = {Numerical {{Optimization}}},
  author = {Nocedal, Jorge and Wright, S.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  edition = {Second},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-0-387-40065-5},
  abstract = {Numerical Optimization presents a comprehensive and up-to-date description of the most effective methods in continuous optimization. It responds to the growing interest in optimization in engineering, science, and business by focusing on the methods that are best suited to practical problems. For this new edition the book has been thoroughly updated throughout. There are new chapters on nonlinear interior methods and derivative-free methods for optimization, both of which are used widely in practice and the focus of much current research. Because of the emphasis on practical methods, as well as the extensive illustrations and exercises, the book is accessible to a wide audience. It can be used as a graduate text in engineering, operations research, mathematics, computer science, and business. It also serves as a handbook for researchers and practitioners in the field. The authors have strived to produce a text that is pleasant to read, informative, and rigorous - one that reveals both the beautiful nature of the discipline and its practical side.},
  isbn = {978-0-387-30303-1},
  langid = {english}
}

@inproceedings{nurvitadhi_can_2017,
  title = {Can {{FPGAs Beat GPUs}} in {{Accelerating Next-Generation Deep Neural Networks}}?},
  booktitle = {Proceedings of the 2017 {{ACM}}/{{SIGDA International Symposium}} on {{Field-Programmable Gate Arrays}}},
  author = {Nurvitadhi, Eriko and Venkatesh, Ganesh and Sim, Jaewoong and Marr, Debbie and Huang, Randy and Ong Gee Hock, Jason and Liew, Yeong Tat and Srivatsan, Krishnan and Moss, Duncan and Subhaschandra, Suchit and Boudoukh, Guy},
  year = {2017},
  month = feb,
  pages = {5--14},
  publisher = {{ACM}},
  address = {{Monterey California USA}},
  doi = {10.1145/3020078.3021740},
  abstract = {Current-generation Deep Neural Networks (DNNs), such as AlexNet and VGG, rely heavily on dense floating-point matrix multiplication (GEMM), which maps well to GPUs (regular parallelism, high TFLOP/s). Because of this, GPUs are widely used for accelerating DNNs. Current FPGAs offer superior energy efficiency (Ops/Watt), but they do not offer the performance of today's GPUs on DNNs. In this paper, we look at upcoming FPGA technology advances, the rapid pace of innovation in DNN algorithms, and consider whether future high-performance FPGAs will outperform GPUs for next-generation DNNs.},
  isbn = {978-1-4503-4354-1},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/9GE5Z2XK/Nurvitadhi et al. - 2017 - Can FPGAs Beat GPUs in Accelerating Next-Generatio.pdf}
}

@inproceedings{obray_evaluation_2022,
  title = {Evaluation {{Metrics}} for {{Graph Generative Models}}: {{Problems}}, {{Pitfalls}}, and {{Practical Solutions}}},
  shorttitle = {Evaluation {{Metrics}} for {{Graph Generative Models}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {O'Bray, Leslie and Horn, Max and Rieck, Bastian and Borgwardt, Karsten},
  year = {2022},
  abstract = {Graph generative models are a highly active branch of machine learning. Given the steady development of new models of ever-increasing complexity, it is necessary to provide a principled way to...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/DNTAPI4L/O'Bray et al. - 2022 - Evaluation Metrics for Graph Generative Models Pr.pdf}
}

@article{ollivier_ricci_2007,
  title = {Ricci Curvature of Metric Spaces},
  author = {Ollivier, Yann},
  year = {2007},
  month = dec,
  journal = {Comptes Rendus Mathematique},
  volume = {345},
  number = {11},
  pages = {643--646},
  issn = {1631-073X},
  doi = {10.1016/j.crma.2007.10.041},
  abstract = {We define a notion of Ricci curvature in metric spaces equipped with a measure or a random walk. For this we use a local contraction coefficient of the random walk acting on the space of probability measures equipped with a transportation distance. This notions allows to generalize several classical theorems associated with positive Ricci curvature, such as a spectral gap bound (Lichnerowicz theorem), Gaussian concentration of measure (L\'evy\textendash Gromov theorem), logarithmic Sobolev inequalities (a result of Bakry\textendash\'Emery theory) or the Bonnet\textendash Myers theorem. The definition is compatible with Bakry\textendash\'Emery theory, and is robust and very easy to implement in concrete examples such as graphs. To cite this article: Y. Ollivier, C. R. Acad. Sci. Paris, Ser. I 345 (2007). R\'esum\'e Nous d\'efinissons la courbure de Ricci d'un espace m\'etrique muni d'une mesure ou d'une marche al\'eatoire. Notre outil est un coefficient de contraction local de la marche al\'eatoire agissant sur l'espace des mesures de probabilit\'es muni d'une distance de transport. Nous pouvons ainsi g\'en\'eraliser des r\'esultats classiques en courbure de Ricci minor\'ee, comme la borne sur le trou spectral (th\'eor\`eme de Lichnerowicz), la concentration gaussienne de la mesure (th\'eor\`eme de L\'evy\textendash Gromov), l'in\'egalit\'e de Sobolev logarithmique (cons\'equence de la th\'eorie de Bakry\textendash\'Emery) ou le th\'eor\`eme de Bonnet\textendash Myers. Notre d\'efinition est compatible avec la th\'eorie de Bakry\textendash\'Emery, est robuste, et tr\`es simple \`a mettre en \oe uvre concr\`etement, par exemple sur un graphe. Pour citer cet article : Y. Ollivier, C. R. Acad. Sci. Paris, Ser. I 345 (2007).},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/VNTELVED/Ollivier - 2007 - Ricci curvature of metric spaces.pdf}
}

@unpublished{panchenko_classwise_2021,
  type = {Preprint},
  title = {Class-Wise and Reduced Calibration Methods},
  author = {Panchenko, Michael and {de Benito Delgado}, Miguel and Sticher, Lorenzo},
  year = {2021},
  month = may,
  address = {{Munich}},
  abstract = {For many applications of probabilistic classifiers it is important that the predicted confidence vectors reflect true probabilities (one says that the classifier is calibrated). Recently, it has been shown that common models fail to satisfy this property, making reliable methods for measuring and improving calibration important tools. Unfortunately, obtaining these is far from trivial for problems with many classes. We propose two simplifications that can be used in tandem. A reduced calibration method reduces the original problem to a simpler one. We prove that, for confidence calibration, solving the reduced problem minimises expected calibration error (ECE) in the full problem. This reduction allows the use of non-parametric recalibration methods that fail in higher dimensions. The second type of simplification is based on the observation that accurate and overconfident classifiers, as mostly found in practice, can be thought of as a union of K different functions which can be recalibrated separately, one for each class. The resulting class-wise calibration methods, in particular Class-wise Temperature Scaling, can out-perform their non class-wise counterparts. Finally, in order to demonstrate our results we introduce a benchmarking technique based on what we call fake classifiers. We release all methods as open source.},
  langid = {english}
}

@misc{panchenko_kyle_2021,
  title = {Kyle: {{A}} Toolkit for Classifier Calibration},
  author = {Panchenko, Michael and Sticher, Lorenzo},
  year = {2021},
  month = jun,
  address = {{Munich}},
  abstract = {Kyle is a python library that contains utilities for measuring and visualizing calibration of probabilistic classifiers as well as for recalibrating them. Kyle is model agnostic. It also offers support for developing and testing custom calibration metrics and algorithms via custom samplers based on fake classifiers. These samplers can also be fit on an arbitrary data set (the outputs of a classifier together with the labels) in case one wants to mimic a real classifier using a fake one.},
  howpublished = {appliedAI by UnternehmerTUM GmbH}
}

@article{papadimitriou_complexity_1994,
  title = {On the Complexity of the Parity Argument and Other Inefficient Proofs of Existence},
  author = {Papadimitriou, Christos H.},
  year = {1994},
  month = jun,
  journal = {Journal of Computer and System Sciences},
  volume = {48},
  number = {3},
  pages = {498--532},
  issn = {0022-0000},
  doi = {10.1016/S0022-0000(05)80063-7},
  abstract = {We define several new complexity classes of search problems, ``between'' the classes FP and FNP. These new classes are contained, along with factoring, and the class PLS, in the class TFNP of search problems in FNP that always have a witness. A problem in each of these new classes is defined in terms of an implicitly given, exponentially large graph. The existence of the solution sought is established via a simple graph-theoretic argument with an inefficiently constructive proof; for example, PLS can be thought of as corresponding to the lemma ``every dag has a sink.'' The new classes, are based on lemmata such as ``every graph has an even number of odd-degree nodes.'' They contain several important problems for which no polynomial time algorithm is presently known, including the computational versions of Sperner's lemma, Brouwer's fixpoint theorem, Ch\'evalley's theorem, and the Borsuk-Ulam theorem, the linear complementarity problem for P-matrices, finding a mixed equilibrium in a non-zero sum game, finding a second Hamilton circuit in a Hamiltonian cubic graph, a second Hamiltonian decomposition in a quartic graph, and others. Some of these problems are shown to be complete.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/CB3DDE9Q/Papadimitriou - 1994 - On the complexity of the parity argument and other.pdf}
}

@article{papamakarios_fast_2018,
  title = {Fast \$\textbackslash epsilon\$-Free {{Inference}} of {{Simulation Models}} with {{Bayesian Conditional Density Estimation}}},
  author = {Papamakarios, George and Murray, Iain},
  year = {2018},
  month = apr,
  journal = {arXiv:1605.06376 [cs, stat]},
  eprint = {1605.06376},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an \$\textbackslash epsilon\$-ball around the observed data, which is only correct in the limit \$\textbackslash epsilon\textbackslash!\textbackslash rightarrow\textbackslash!0\$. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as \$\textbackslash epsilon\textbackslash!\textbackslash rightarrow\textbackslash!0\$, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/RAQKCDHB/Papamakarios and Murray - 2018 - Fast $epsilon$-free Inference of Simulation Model.pdf}
}

@article{papamakarios_normalizing_2019,
  title = {Normalizing {{Flows}} for {{Probabilistic Modeling}} and {{Inference}}},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.02762 [cs, stat]},
  eprint = {1912.02762},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00021},
  file = {/Users/fariedabuzaid/Zotero/storage/BPGL63E8/Papamakarios et al. - 2019 - Normalizing Flows for Probabilistic Modeling and I.pdf}
}

@article{papamakarios_sequential_2019,
  title = {Sequential {{Neural Likelihood}}: {{Fast Likelihood-free Inference}} with {{Autoregressive Flows}}},
  shorttitle = {Sequential {{Neural Likelihood}}},
  author = {Papamakarios, George and Sterratt, David C. and Murray, Iain},
  year = {2019},
  month = jan,
  journal = {arXiv:1805.07226 [cs, stat]},
  eprint = {1805.07226},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/6VD3Y4AE/Papamakarios et al. - 2019 - Sequential Neural Likelihood Fast Likelihood-free.pdf}
}

@article{papyan_prevalence_2020,
  title = {Prevalence of {{Neural Collapse}} during the Terminal Phase of Deep Learning Training},
  author = {Papyan, Vardan and Han, X. Y. and Donoho, David L.},
  year = {2020},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {40},
  eprint = {2008.08186},
  eprinttype = {arxiv},
  pages = {24652--24663},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2015509117},
  abstract = {Modern practice for training classification deepnets involves a Terminal Phase of Training (TPT), which begins at the epoch where training error first vanishes; During TPT, the training error stays effectively zero while training loss is pushed towards zero. Direct measurements of TPT, for three prototypical deepnet architectures and across seven canonical classification datasets, expose a pervasive inductive bias we call Neural Collapse, involving four deeply interconnected phenomena: (NC1) Cross-example within-class variability of last-layer training activations collapses to zero, as the individual activations themselves collapse to their class-means; (NC2) The class-means collapse to the vertices of a Simplex Equiangular Tight Frame (ETF); (NC3) Up to rescaling, the last-layer classifiers collapse to the class-means, or in other words to the Simplex ETF, i.e. to a self-dual configuration; (NC4) For a given activation, the classifier's decision collapses to simply choosing whichever class has the closest train class-mean, i.e. the Nearest Class Center (NCC) decision rule. The symmetric and very simple geometry induced by the TPT confers important benefits, including better generalization performance, better robustness, and better interpretability.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/BZFK86MB/Papyan et al. - 2020 - Prevalence of Neural Collapse during the terminal .pdf}
}

@article{pardoux_adapted_1990,
  title = {Adapted Solution of a Backward Stochastic Differential Equation},
  author = {Pardoux, E. and Peng, S. G.},
  year = {1990},
  month = jan,
  journal = {Systems \& Control Letters},
  volume = {14},
  number = {1},
  pages = {55--61},
  issn = {0167-6911},
  doi = {10.1016/0167-6911(90)90082-6},
  abstract = {Let Wt; t {$\epsilon$} [0, 1] be a standard k-dimensional Weiner process defined on a probability space ({$\Omega$}, F, P), and let Ft denote its natural filtration. Given a F1 measurable d-dimensional random vector X, we look for an adapted pair of processes \{x(t), y(t); t {$\epsilon$} [0, 1]\} with values in Rd and Rd\texttimes k respectively, which solves an equation of the form: x(t) + {$\int$}t1f(s, x(s), y(s)) ds + {$\int$}t1 [g(s, x(s)) + y(s)] dWs = X. A linearized version of that equation appears in stochastic control theory as the equation satisfied by the adjoint process. We also generalize our results to the following equation: x(t) + {$\int$}t1f(s, x(s), y(s)) ds + {$\int$}t1 g(s, x(s)) + y(s)) dWs = X under rather restrictive assumptions on g.},
  langid = {english}
}

@inproceedings{park_graphens_2022,
  title = {{{GraphENS}}: {{Neighbor-Aware Ego Network Synthesis}} for {{Class-Imbalanced Node Classification}}},
  shorttitle = {{{GraphENS}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Park, Joonhyung and Song, Jaeyun and Yang, Eunho},
  year = {2022},
  abstract = {In many real-world node classification scenarios, nodes are highly class-imbalanced, where graph neural networks (GNNs) can be readily biased to major class instances. Albeit existing class...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/FVJEEAT5/Park et al. - 2021 - GraphENS Neighbor-Aware Ego Network Synthesis for.pdf;/Users/fariedabuzaid/Zotero/storage/IR9YBTZZ/forum.html}
}

@inproceedings{patel_multiclass_2021,
  title = {Multi-Class Uncertainty Calibration via Mutual Information Maximization-Based Binning},
  booktitle = {Ninth {{International Conference On Learning Representations}} ({{ICLR}} 2021)},
  author = {Patel, Kanil and Beluch, William and Yang, Bin and Pfeiffer, Michael and Zhang, Dan},
  year = {2021},
  eprint = {2006.13092},
  eprinttype = {arxiv},
  pages = {32},
  abstract = {Post-hoc multi-class calibration is a common approach for providing high-quality confidence estimates of deep neural network predictions. Recent work has shown that widely used scaling methods underestimate their calibration error, while alternative Histogram Binning (HB) methods often fail to preserve classification accuracy. When classes have small prior probabilities, HB also faces the issue of severe sample-inefficiency after the conversion into K one-vs-rest class-wise calibration problems. The goal of this paper is to resolve the identified issues of HB in order to provide calibrated confidence estimates using only a small holdout calibration dataset for bin optimization while preserving multi-class ranking accuracy. From an information-theoretic perspective, we derive the I-Max concept for binning, which maximizes the mutual information between labels and quantized logits. This concept mitigates potential loss in ranking performance due to lossy quantization, and by disentangling the optimization of bin edges and representatives allows simultaneous improvement of ranking and calibration performance. To improve the sample efficiency and estimates from a small calibration set, we propose a shared class-wise (sCW) calibration strategy, sharing one calibrator among similar classes (e.g., with similar class priors) so that the training sets of their class-wise calibration problems can be merged to train the single calibrator. The combination of sCW and I-Max binning outperforms the state of the art calibration methods on various evaluation metrics across different benchmark datasets and models, using a small calibration set (e.g., 1k samples for ImageNet).},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/KZZ35HIF/Patel et al. - 2021 - Multi-class uncertainty calibration via mutual inf.pdf}
}

@article{pereyra_regularizing_2017,
  title = {Regularizing {{Neural Networks}} by {{Penalizing Confident Output Distributions}}},
  author = {Pereyra, Gabriel and Tucker, George and Chorowski, Jan and Kaiser, {\L}ukasz and Hinton, Geoffrey},
  year = {2017},
  month = jan,
  journal = {arXiv:1701.06548 [cs]},
  eprint = {1701.06548},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00000},
  file = {/Users/fariedabuzaid/Zotero/storage/LBTVG6KT/Pereyra et al. - 2017 - Regularizing Neural Networks by Penalizing Confide.pdf}
}

@article{peters_causal_2016,
  title = {Causal Inference by Using Invariant Prediction: Identification and Confidence Intervals},
  shorttitle = {Causal Inference by Using Invariant Prediction},
  author = {Peters, Jonas and B{\"u}hlmann, Peter and Meinshausen, Nicolai},
  year = {2016},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {78},
  number = {5},
  eprint = {1501.01332},
  eprinttype = {arxiv},
  pages = {947--1012},
  issn = {1369-7412},
  abstract = {What is the difference between a prediction that is made with a causal model and that with a non-causal model? Suppose that we intervene on the predictor variables or change the whole environment. The predictions from a causal model will in general work as well under interventions as for observational data. In contrast, predictions from a non-causal model can potentially be very wrong if we actively intervene on variables. Here, we propose to exploit this invariance of a prediction under a causal model for causal inference: given different experimental settings (e.g. various interventions) we collect all models that do show invariance in their predictive accuracy across settings and interventions. The causal model will be a member of this set of models with high probability. This approach yields valid confidence intervals for the causal relationships in quite general scenarios. We examine the example of structural equation models in more detail and provide sufficient assumptions under which the set of causal predictors becomes identifiable. We further investigate robustness properties of our approach under model misspecification and discuss possible extensions. The empirical properties are studied for various data sets, including large-scale gene perturbation experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/YAEZ3Z39/Peters et al. - 2016 - Causal inference by using invariant prediction id.pdf}
}

@book{peters_elements_2017,
  ids = {peters_elements_2017a},
  title = {Elements of {{Causal Inference}}: {{Foundations}} and {{Learning Algorithms}}},
  shorttitle = {Elements of {{Causal Inference}}},
  author = {Peters, Jonas and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year = {2017},
  month = nov,
  series = {Adaptive {{Computation}} and {{Machine Learning}} Series},
  publisher = {{MIT Press}},
  abstract = {A concise and self-contained introduction to causal inference, increasingly important in data science and machine learning. The mathematization of causality is a relatively recent development, and has become increasingly important in data science and machine learning. This book offers a self-contained and concise introduction to causal models and how to learn them from data.After explaining the need for causal models and discussing some of the principles underlying causal inference, the book teaches readers how to use causal models: how to compute intervention distributions, how to infer causal models from observational and interventional data, and how causal ideas could be exploited for classical machine learning problems. All of these topics are discussed first in terms of two variables and then in the more general multivariate case. The bivariate case turns out to be a particularly hard problem for causal learning because there are no conditional independences as used by classical methods for solving multivariate cases. The authors consider analyzing statistical asymmetries between cause and effect to be highly instructive, and they report on their decade of intensive research into this problem. The book is accessible to readers with a background in machine learning or statistics, and can be used in graduate courses or as a reference for researchers. The text includes code snippets that can be copied and pasted, exercises, and an appendix with a summary of the most important technical concepts.},
  googlebooks = {XPpFDwAAQBAJ},
  isbn = {978-0-262-03731-0},
  langid = {english},
  annotation = {citecount: 00288},
  file = {/Users/fariedabuzaid/Zotero/storage/6S4E52LQ/Peters et al. - 2017 - Elements of Causal Inference Foundations and Lear.pdf}
}

@inproceedings{pethick_escaping_2022,
  title = {Escaping Limit Cycles: {{Global}} Convergence for Constrained Nonconvex-Nonconcave Minimax Problems},
  shorttitle = {Escaping Limit Cycles},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Pethick, Thomas and Latafat, Puya and Patrinos, Panos and Fercoq, Olivier and Cevher, Volkan},
  year = {2022},
  abstract = {This paper introduces a new extragradient-type algorithm for a class of nonconvex-nonconcave minimax problems. It is well-known that finding a local solution for general minimax problems is...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/IC4RYUBB/Pethick et al. - 2022 - Escaping limit cycles Global convergence for cons.pdf}
}

@article{pimentel_review_2014,
  title = {A Review of Novelty Detection},
  author = {Pimentel, Marco A.F. and Clifton, David A. and Clifton, Lei and Tarassenko, Lionel},
  year = {2014},
  month = jun,
  journal = {Signal Processing},
  volume = {99},
  pages = {215--249},
  issn = {01651684},
  doi = {10.1016/j.sigpro.2013.12.026},
  abstract = {Novelty detection is the task of classifying test data that differ in some respect from the data that are available during training. This may be seen as ``one-class classification'', in which a model is constructed to describe ``normal'' training data. The novelty detection approach is typically used when the quantity of available ``abnormal'' data is insufficient to construct explicit models for non-normal classes. Application includes inference in datasets from critical systems, where the quantity of available normal data is very large, such that ``normality'' may be accurately modelled. In this review we aim to provide an updated and structured investigation of novelty detection research papers that have appeared in the machine learning literature during the last decade.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/9DW5M756/Pimentel et al. - 2014 - A review of novelty detection.pdf}
}

@inproceedings{platt_probabilistic_1999,
  title = {Probabilistic {{Outputs}} for {{Support Vector Machines}} and {{Comparisons}} to {{Regularized Likelihood Methods}}},
  booktitle = {Advances in {{Large Margin Classifiers}}},
  author = {Platt, John C.},
  year = {1999},
  pages = {61--74},
  publisher = {{MIT Press}},
  abstract = {The output of a classifier should be a calibrated posterior probability to enable post-processing. Standard SVMs do not provide such probabilities. One method to create probabilities is to directly train a kernel classifier with a logit link function and a regularized maximum likelihood score. However, training with a maximum likelihood score will produce non-sparse kernel machines. Instead, we train an SVM, then train the parameters of an additional sigmoid function to map the SVM outputs into probabilities. This chapter compares classification error rate and likelihood scores for an SVM plus sigmoid versus a kernel method trained with a regularized likelihood error function. These methods are tested on three data-mining-style data sets. The SVM+sigmoid yields probabilities of comparable quality to the regularized maximum likelihood kernel method, while still retaining the sparseness of the SVM.},
  annotation = {citecount: 04558},
  file = {/Users/fariedabuzaid/Zotero/storage/R2XELANQ/Platt - 1999 - Probabilistic Outputs for Support Vector Machines .pdf}
}

@inproceedings{poklukar_delaunay_2022,
  title = {Delaunay {{Component Analysis}} for {{Evaluation}} of {{Data Representations}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Poklukar, Petra and Polianskii, Vladislav and Varava, Anastasiia and Pokorny, Florian T. and Jensfelt, Danica Kragic},
  year = {2022},
  abstract = {Advanced representation learning techniques require reliable and general evaluation methods. Recently, several algorithms based on the common idea of geometric and topological analysis of a...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/Z33G79GY/Poklukar et al. - 2022 - Delaunay Component Analysis for Evaluation of Data.pdf}
}

@inproceedings{posocco_estimating_2021,
  title = {Estimating {{Expected Calibration Errors}}},
  booktitle = {{{arXiv}}:2109.03480 [Cs]},
  author = {Posocco, Nicolas and Bonnefoy, Antoine},
  year = {2021},
  month = sep,
  eprint = {2109.03480},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Uncertainty in probabilistic classifiers predictions is a key concern when models are used to support human decision making, in broader probabilistic pipelines or when sensitive automatic decisions have to be taken. Studies have shown that most models are not intrinsically well calibrated, meaning that their decision scores are not consistent with posterior probabilities. Hence being able to calibrate these models, or enforce calibration while learning them, has regained interest in recent literature. In this context, properly assessing calibration is paramount to quantify new contributions tackling calibration. However, there is room for improvement for commonly used metrics and evaluation of calibration could benefit from deeper analyses. Thus this paper focuses on the empirical evaluation of calibration metrics in the context of classification. More specifically it evaluates different estimators of the Expected Calibration Error (\$ECE\$), amongst which legacy estimators and some novel ones, proposed in this paper. We build an empirical procedure to quantify the quality of these \$ECE\$ estimators, and use it to decide which estimator should be used in practice for different settings.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/6NYXDI3B/Posocco and Bonnefoy - 2021 - Estimating Expected Calibration Errors.pdf}
}

@article{postels_samplingfree_2019,
  title = {Sampling-Free {{Epistemic Uncertainty Estimation Using Approximated Variance Propagation}}},
  author = {Postels, Janis and Ferroni, Francesco and Coskun, Huseyin and Navab, Nassir and Tombari, Federico},
  year = {2019},
  month = dec,
  eprint = {1908.00598},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1908.00598},
  abstract = {We present a sampling-free approach for computing the epistemic uncertainty of a neural network. Epistemic uncertainty is an important quantity for the deployment of deep neural networks in safety-critical applications, since it represents how much one can trust predictions on new data. Recently promising works were proposed using noise injection combined with Monte-Carlo sampling at inference time to estimate this quantity (e.g. Monte-Carlo dropout). Our main contribution is an approximation of the epistemic uncertainty estimated by these methods that does not require sampling, thus notably reducing the computational overhead. We apply our approach to large-scale visual tasks (i.e., semantic segmentation and depth regression) to demonstrate the advantages of our method compared to sampling-based approaches in terms of quality of the uncertainty estimates as well as of computational overhead.},
  archiveprefix = {arXiv},
  langid = {english}
}

@article{power_grokking_2022,
  title = {Grokking: {{Generalization Beyond Overfitting}} on {{Small Algorithmic Datasets}}},
  shorttitle = {Grokking},
  author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.02177 [cs]},
  eprint = {2201.02177},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/SV3JMSB5/Power et al. - 2022 - Grokking Generalization Beyond Overfitting on Sma.pdf}
}

@unpublished{pratt_must_1962,
  type = {Unpublished Seminar Paper},
  title = {Must Subjective Probabilities Be Realized as Relative Frequencies},
  author = {Pratt, J},
  year = {1962},
  address = {{Harvard university graduate school of business administration}},
  langid = {english}
}

@article{qin_binary_2020,
  title = {Binary {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Binary {{Neural Networks}}},
  author = {Qin, Haotong and Gong, Ruihao and Liu, Xianglong and Bai, Xiao and Song, Jingkuan and Sebe, Nicu},
  year = {2020},
  month = sep,
  journal = {Pattern Recognition},
  volume = {105},
  eprint = {2004.03333},
  eprinttype = {arxiv},
  pages = {107281},
  issn = {00313203},
  doi = {10.1016/j.patcog.2020.107281},
  abstract = {The binary neural network, largely saving the storage and computation, serves as a promising technique for deploying deep models on resource-limited devices. However, the binarization inevitably causes severe information loss, and even worse, its discontinuity brings difficulty to the optimization of the deep network. To address these issues, a variety of algorithms have been proposed, and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these algorithms, mainly categorized into the native solutions directly conducting binarization, and the optimized ones using techniques like minimizing the quantization error, improving the network loss function, and reducing the gradient error. We also investigate other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. Then, we give the evaluation and discussions on different tasks, including image classification, object detection and semantic segmentation. Finally, the challenges that may be faced in future research are prospected.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/F849ZSK8/Qin et al. - 2020 - Binary Neural Networks A Survey.pdf}
}

@article{qin_data_2019,
  title = {Data Driven Governing Equations Approximation Using Deep Neural Networks},
  author = {Qin, Tong and Wu, Kailiang and Xiu, Dongbin},
  year = {2019},
  month = oct,
  journal = {Journal of Computational Physics},
  volume = {395},
  pages = {620--635},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2019.06.042},
  abstract = {We present a numerical framework for approximating unknown governing equations using observation data and deep neural networks (DNN). In particular, we propose to use residual network (ResNet) as the basic building block for equation approximation. We demonstrate that the ResNet block can be considered as a one-step method that is exact in temporal integration. We then present two multi-step methods, recurrent ResNet (RT-ResNet) method and recursive ReNet (RS-ResNet) method. The RT-ResNet is a multi-step method on uniform time steps, whereas the RS-ResNet is an adaptive multi-step method using variable time steps. All three methods presented here are based on integral form of the underlying dynamical system. As a result, they do not require time derivative data for equation recovery and can cope with relatively coarsely distributed trajectory data. Several numerical examples are presented to demonstrate the performance of the methods.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/WIU6GQVT/Qin et al. - 2019 - Data driven governing equations approximation usin.pdf}
}

@inproceedings{qu_neural_2022,
  title = {Neural {{Structured Prediction}} for {{Inductive Node Classification}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Qu, Meng and Cai, Huiyu and Tang, Jian},
  year = {2022},
  abstract = {This paper studies node classification in the inductive setting, i.e., aiming to learn a model on labeled training graphs and generalize it to infer node labels on unlabeled test graphs. This...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/A24EJSTS/Qu et al. - 2021 - Neural Structured Prediction for Inductive Node Cl.pdf;/Users/fariedabuzaid/Zotero/storage/CY8MU9HQ/forum.html}
}

@book{quarteroni_numerical_1994,
  title = {Numerical {{Approximation}} of {{Partial Differential Equations}}},
  author = {Quarteroni, Alfio and Valli, Alberto},
  year = {1994},
  series = {Springer {{Series}} in {{Computational Mathematics}}},
  publisher = {{Springer-Verlag}},
  address = {{Berlin Heidelberg}},
  doi = {10.1007/978-3-540-85268-1},
  abstract = {This book deals with the numerical approximation of partial differential equations. Its scope is to provide a thorough illustration of numerical methods, carry out their stability and convergence analysis, derive error bounds, and discuss the algorithmic aspects relative to their implementation. A sound balancing of theoretical analysis, description of algorithms and discussion of applications is one of its main features. Many kinds of problems are addressed. A comprehensive theory of Galerkin method and its variants, as well as that of collocation methods, are developed for the spatial discretization. These theories are then specified to two numerical subspace realizations of remarkable interest: the finite element method and the spectral method. From the reviews:"...The book is excellent and is addressed to post-graduate students, research workers in applied sciences as well as to specialists in numerical mathematics solving PDE. Since it is written very clearly, it would be acceptable for undergraduate students in advanced courses of numerical mathematics. Readers will find this book to be a great pleasure."--MATHEMATICAL REVIEWS},
  isbn = {978-3-540-85267-4},
  langid = {english}
}

@article{radford_improving_2018,
  title = {Improving {{Language Understanding}} by {{Generative Pre-Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  pages = {12},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  langid = {english},
  annotation = {blog: https://openai.com/blog/language-unsupervised/},
  file = {/Users/fariedabuzaid/Zotero/storage/SZBMZJHA/Radford et al. - Improving Language Understanding by Generative Pre.pdf}
}

@article{radosavovic_data_2017,
  title = {Data {{Distillation}}: {{Towards Omni-Supervised Learning}}},
  shorttitle = {Data {{Distillation}}},
  author = {Radosavovic, Ilija and Doll{\'a}r, Piotr and Girshick, Ross and Gkioxari, Georgia and He, Kaiming},
  year = {2017},
  month = dec,
  journal = {arXiv:1712.04440 [cs]},
  eprint = {1712.04440},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We investigate omni-supervised learning, a special regime of semi-supervised learning in which the learner exploits all available labeled data plus internet-scale sources of unlabeled data. Omni-supervised learning is lower-bounded by performance on existing labeled datasets, offering the potential to surpass state-of-the-art fully supervised methods. To exploit the omni-supervised setting, we propose data distillation, a method that ensembles predictions from multiple transformations of unlabeled data, using a single model, to automatically generate new training annotations. We argue that visual recognition models have recently become accurate enough that it is now possible to apply classic ideas about self-training to challenging real-world data. Our experimental results show that in the cases of human keypoint detection and general object detection, state-of-the-art models trained with data distillation surpass the performance of using labeled data from the COCO dataset alone.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00000},
  file = {/Users/fariedabuzaid/Zotero/storage/26CHQ8T9/Radosavovic et al. - 2017 - Data Distillation Towards Omni-Supervised Learnin.pdf}
}

@article{raghu_behaviour_2018,
  title = {Behaviour {{Policy Estimation}} in {{Off-Policy Policy Evaluation}}: {{Calibration Matters}}},
  shorttitle = {Behaviour {{Policy Estimation}} in {{Off-Policy Policy Evaluation}}},
  author = {Raghu, Aniruddh and Gottesman, Omer and Liu, Yao and Komorowski, Matthieu and Faisal, Aldo and {Doshi-Velez}, Finale and Brunskill, Emma},
  year = {2018},
  month = jul,
  journal = {arXiv:1807.01066 [cs, stat]},
  eprint = {1807.01066},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In this work, we consider the problem of estimating a behaviour policy for use in Off-Policy Policy Evaluation (OPE) when the true behaviour policy is unknown. Via a series of empirical studies, we demonstrate how accurate OPE is strongly dependent on the calibration of estimated behaviour policy models: how precisely the behaviour policy is estimated from data. We show how powerful parametric models such as neural networks can result in highly uncalibrated behaviour policy models on a real-world medical dataset, and illustrate how a simple, non-parametric, k-nearest neighbours model produces better calibrated behaviour policy estimates and can be used to obtain superior importance sampling-based OPE estimates.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00000},
  file = {/Users/fariedabuzaid/Zotero/storage/QUY8KRIZ/Raghu et al. - 2018 - Behaviour Policy Estimation in Off-Policy Policy E.pdf}
}

@inproceedings{raghunathan_certified_2018,
  title = {Certified {{Defenses}} against {{Adversarial Examples}}},
  booktitle = {6th {{International Conference}} on {{Learning Representations}} ({{ICLR}} 2018)},
  author = {Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy},
  year = {2018},
  month = feb,
  address = {{Vancouver, Canada}},
  abstract = {We demonstrate a certifiable, trainable, and scalable method for defending against adversarial examples.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/TNMEMU97/Raghunathan et al. - 2018 - Certified Defenses against Adversarial Examples.pdf}
}

@inproceedings{raghunathan_semidefinite_2018,
  title = {Semidefinite Relaxations for Certifying Robustness to Adversarial Examples},
  booktitle = {{{NeurIPS}} 2018},
  author = {Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy S.},
  year = {2018},
  month = jan,
  eprint = {1811.01057},
  eprinttype = {arxiv},
  abstract = {Research on adversarial examples are evolved in arms race between defenders who attempt to train robust networks and attackers who try to prove them wrong. This has spurred interest in methods for...},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/FNRB24BZ/Raghunathan et al. - 2018 - Semidefinite relaxations for certifying robustness.pdf}
}

@inproceedings{raghunathan_understanding_2020,
  title = {Understanding and {{Mitigating}} the {{Tradeoff}} between {{Robustness}} and {{Accuracy}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Raghunathan, Aditi and Xie, Sang Michael and Yang, Fanny and Duchi, John and Liang, Percy},
  year = {2020},
  month = nov,
  eprint = {2002.10716},
  eprinttype = {arxiv},
  pages = {7909--7919},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Adversarial training augments the training set with perturbations to improve the robust error (over worst-case perturbations), but it often leads to an increase in the standard error (on unperturbed test inputs). Previous explanations for this tradeoff rely on the assumption that no predictor in the hypothesis class has low standard and robust error. In this work, we precisely characterize the effect of augmentation on the standard error in linear regression when the optimal linear predictor has zero standard and robust error. In particular, we show that the standard error could increase even when the augmented perturbations have noiseless observations from the optimal linear predictor. We then prove that the recently proposed robust self-training (RST) estimator improves robust error without sacrificing standard error for noiseless linear regression. Empirically, for neural networks, we find that RST with different adversarial training methods improves both standard and robust error for random and adversarial rotations and adversarial l\_infty perturbations in CIFAR-10.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {notion},
  file = {/Users/fariedabuzaid/Zotero/storage/US9BKSHF/Raghunathan et al. - 2020 - Understanding and Mitigating the Tradeoff between .pdf;/Users/fariedabuzaid/Zotero/storage/V4WBN5I5/Raghunathan et al. - 2020 - Understanding and Mitigating the Tradeoff between .pdf}
}

@misc{raghunathan_worstcase_2021,
  type = {Talk},
  title = {Worst-{{Case Robustness}} in {{Machine Learning}}},
  author = {Raghunathan, Aditi},
  year = {2021},
  month = nov,
  address = {{Simons Institute}},
  abstract = {Current machine learning (ML) systems are remarkably brittle, raising serious concerns about their deployment in safety-critical applications like self-driving cars and predictive healthcare. In such applications, models could encounter test distributions that differ wildly from the training distributions. Trustworthy ML thus requires strong robustness guarantees from learning, including robustness to worst-case distribution shifts. Robustness to worst-case distribution shifts raises several computational and statistical challenges over `standard' machine learning. In this talk, I will present two formal settings of worst-case distribution shifts motivated by adversarial attacks on test inputs and presence of spurious correlations like image backgrounds. Empirical observations demonstrate (i) an arms race between attacks and existing heuristic defenses necessitating provable guarantees much like cryptography (ii) increased sample complexity of robust learning (iii) resurgence of the need for regularization in robust learning. We capture each of these observations in simple theoretical models that nevertheless yield principled and scalable approaches to overcome the hurdles in robust learning, particularly via the use of unlabeled data.},
  langid = {english},
  keywords = {notion},
  annotation = {video: https://www.youtube.com/watch?v=ce1qxyP\_U7Y}
}

@article{raissi_deep_2018,
  title = {Deep {{Hidden Physics Models}}: {{Deep Learning}} of {{Nonlinear Partial Differential Equations}}},
  shorttitle = {Deep {{Hidden Physics Models}}},
  author = {Raissi, Maziar},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {19},
  number = {25},
  eprint = {1801.06637},
  eprinttype = {arxiv},
  pages = {1--24},
  issn = {1533-7928},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/UGVXPETQ/Raissi - 2018 - Deep Hidden Physics Models Deep Learning of Nonli.pdf}
}

@article{raissi_forwardbackward_2018,
  title = {Forward-{{Backward Stochastic Neural Networks}}: {{Deep Learning}} of {{High-dimensional Partial Differential Equations}}},
  shorttitle = {Forward-{{Backward Stochastic Neural Networks}}},
  author = {Raissi, Maziar},
  year = {2018},
  month = apr,
  journal = {arXiv:1804.07010 [cs, math, stat]},
  eprint = {1804.07010},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Classical numerical methods for solving partial differential equations suffer from the curse dimensionality mainly due to their reliance on meticulously generated spatio-temporal grids. Inspired by modern deep learning based techniques for solving forward and inverse problems associated with partial differential equations, we circumvent the tyranny of numerical discretization by devising an algorithm that is scalable to high-dimensions. In particular, we approximate the unknown solution by a deep neural network which essentially enables us to benefit from the merits of automatic differentiation. To train the aforementioned neural network we leverage the well-known connection between high-dimensional partial differential equations and forward-backward stochastic differential equations. In fact, independent realizations of a standard Brownian motion will act as training data. We test the effectiveness of our approach for a couple of benchmark problems spanning a number of scientific domains including Black-Scholes-Barenblatt and Hamilton-Jacobi-Bellman equations, both in 100-dimensions.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/ITCGNYMH/Raissi - 2018 - Forward-Backward Stochastic Neural Networks Deep .pdf}
}

@article{raissi_hidden_2018,
  title = {Hidden {{Physics Models}}: {{Machine Learning}} of {{Nonlinear Partial Differential Equations}}},
  shorttitle = {Hidden {{Physics Models}}},
  author = {Raissi, Maziar and Karniadakis, George Em},
  year = {2018},
  month = mar,
  journal = {Journal of Computational Physics},
  volume = {357},
  eprint = {1708.00588},
  eprinttype = {arxiv},
  pages = {125--141},
  issn = {00219991},
  doi = {10.1016/j.jcp.2017.11.039},
  abstract = {While there is currently a lot of enthusiasm about "big data", useful data is usually "small" and expensive to acquire. In this paper, we present a new paradigm of learning partial differential equations from \{\textbackslash em small\} data. In particular, we introduce \textbackslash emph\{hidden physics models\}, which are essentially data-efficient learning machines capable of leveraging the underlying laws of physics, expressed by time dependent and nonlinear partial differential equations, to extract patterns from high-dimensional data generated from experiments. The proposed methodology may be applied to the problem of learning, system identification, or data-driven discovery of partial differential equations. Our framework relies on Gaussian processes, a powerful tool for probabilistic inference over functions, that enables us to strike a balance between model complexity and data fitting. The effectiveness of the proposed approach is demonstrated through a variety of canonical problems, spanning a number of scientific domains, including the Navier-Stokes, Schr\textbackslash "odinger, Kuramoto-Sivashinsky, and time dependent linear fractional equations. The methodology provides a promising new direction for harnessing the long-standing developments of classical methods in applied mathematics and mathematical physics to design learning machines with the ability to operate in complex domains without requiring large quantities of data.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00114},
  file = {/Users/fariedabuzaid/Zotero/storage/EUXYQNZ8/Raissi and Karniadakis - 2018 - Hidden Physics Models Machine Learning of Nonline.pdf}
}

@article{raissi_physicsinformed_2019,
  title = {Physics-Informed Neural Networks: {{A}} Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations},
  shorttitle = {Physics-Informed Neural Networks},
  author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
  year = {2019},
  month = feb,
  journal = {Journal of Computational Physics},
  volume = {378},
  eprint = {1711.10561},
  eprinttype = {arxiv},
  pages = {686--707},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2018.10.045},
  abstract = {We introduce physics-informed neural networks \textendash{} neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge\textendash Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction\textendash diffusion systems, and the propagation of nonlinear shallow-water waves.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/HGVNUXZE/Raissi et al. - 2019 - Physics-informed neural networks A deep learning .pdf}
}

@article{ramesh_hierarchical_2022,
  title = {Hierarchical {{Text-Conditional Image Generation}} with {{CLIP Latents}}},
  author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  year = {2022},
  month = apr,
  journal = {arXiv:2204.06125 [cs]},
  eprint = {2204.06125},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/6AREE8ZG/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf}
}

@inproceedings{ramezani_learn_2022,
  title = {Learn {{Locally}}, {{Correct Globally}}: {{A Distributed Algorithm}} for {{Training Graph Neural Networks}}},
  shorttitle = {Learn {{Locally}}, {{Correct Globally}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Ramezani, Morteza and Cong, Weilin and Mahdavi, Mehrdad and Kandemir, Mahmut and Sivasubramaniam, Anand},
  year = {2022},
  abstract = {Despite the recent success of Graph Neural Networks (GNNs), training GNNs on large graphs remains challenging. The limited resource capacities of the existing servers, the dependency between nodes...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/5JHR48GM/Ramezani et al. - 2021 - Learn Locally, Correct Globally A Distributed Alg.pdf;/Users/fariedabuzaid/Zotero/storage/Z9PNMESM/forum.html}
}

@inproceedings{riad_learning_2022,
  title = {Learning Strides in Convolutional Neural Networks},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Riad, Rachid and Teboul, Olivier and Grangier, David and Zeghidour, Neil},
  year = {2022},
  month = feb,
  eprint = {2202.01653},
  eprinttype = {arxiv},
  abstract = {Convolutional neural networks typically contain several downsampling operators, such as strided convolutions or pooling layers, that progressively reduce the resolution of intermediate representations. This provides some shift-invariance while reducing the computational complexity of the whole architecture. A critical hyperparameter of such layers is their stride: the integer factor of downsampling. As strides are not differentiable, finding the best configuration either requires cross-validation or discrete optimization (e.g. architecture search), which rapidly become prohibitive as the search space grows exponentially with the number of downsampling layers. Hence, exploring this search space by gradient descent would allow finding better configurations at a lower computational cost. This work introduces DiffStride, the first downsampling layer with learnable strides. Our layer learns the size of a cropping mask in the Fourier domain, that effectively performs resizing in a differentiable way. Experiments on audio and image classification show the generality and effectiveness of our solution: we use DiffStride as a drop-in replacement to standard downsampling layers and outperform them. In particular, we show that introducing our layer into a ResNet-18 architecture allows keeping consistent high performance on CIFAR10, CIFAR100 and ImageNet even when training starts from poor random stride configurations. Moreover, formulating strides as learnable variables allows us to introduce a regularization term that controls the computational complexity of the architecture. We show how this regularization allows trading off accuracy for efficiency on ImageNet.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {notion: https://www.notion.so/appliedaiinitiative/Learning-Strides-in-CNNs-937a1d744d4e4f2e9ca60a420b6d26bb post: https://community.appliedai.de/topics/27304/topic\_feed\_posts/1245162 video: https://iclr.cc/virtual/2022/oral/7069},
  file = {/Users/fariedabuzaid/Zotero/storage/XE44LYNC/Riad et al. - 2022 - Learning strides in convolutional neural networks.pdf}
}

@article{ribeiro_why_2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = feb,
  journal = {arXiv:1602.04938 [cs, stat]},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00361},
  file = {/Users/fariedabuzaid/Zotero/storage/485QTXVE/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@inproceedings{rippel_spectral_2015,
  title = {Spectral {{Representations}} for {{Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Rippel, Oren and Snoek, Jasper and Adams, Ryan P},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs).We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality.  This representation also enables a new form of stochastic regularization by randomized modification of resolution.  We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling.  Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.},
  file = {/Users/fariedabuzaid/Zotero/storage/LVRYU6VF/Rippel et al. - 2015 - Spectral Representations for Convolutional Neural .pdf}
}

@mastersthesis{rochman_solving_2020,
  title = {Solving {{Schr\"odinger}}'s Equation with {{Deep Learning}}},
  author = {Rochman, Omer},
  year = {2020},
  month = oct,
  address = {{Munich}},
  abstract = {In this work, we attempt to solve the Time Dependent Schr\"odinger Equation directly by placing a neural network ansatz on the solution and designing a loss metric based on the constraints imposed by the equation. We show that the method works for simple cases, such as those in 1D or with the free Hamiltonian, but fails in higher dimensions (3D+) or when a complex potential is present. We then examine why the neural network fails to converge and identify the main failure mode, in which the model learns the zero function, and its underlying cause. That cause being the pathological training dynamics with different scales, which hinders the optimization process. The different dynamical scales arise from the composite nature of the loss. Optimization is made more difficult because most modern optimizers were designed for a different class of loss functions stemming from more traditional applications of neural networks, which are not equipped to deal with the pathologies of our use case. We attempt to solve this problem by normalizing the gradients and compare our approach to recent developments but conclude that both approaches fail to address the underlying problem.},
  langid = {english},
  school = {Technische Universit\"at M\"unchen}
}

@article{rolf_representation_2021,
  title = {Representation {{Matters}}: {{Assessing}} the {{Importance}} of {{Subgroup Allocations}} in {{Training Data}}},
  shorttitle = {Representation {{Matters}}},
  author = {Rolf, Esther and Worledge, Theodora and Recht, Benjamin and Jordan, Michael I.},
  year = {2021},
  month = mar,
  abstract = {Collecting more diverse and representative training data is often touted as a remedy for the disparate performance of machine learning predictors across subpopulations. However, a precise framework for understanding how dataset properties like diversity affect learning outcomes is largely lacking. By casting data collection as part of the learning process, we demonstrate that diverse representation in training data is key not only to increasing subgroup performances, but also to achieving population level objectives. Our analysis and experiments describe how dataset compositions influence performance and provide constructive results for using trends in existing data, alongside domain knowledge, to help guide intentional, objective-aware dataset design.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/AMYA3G5F/Rolf et al. - 2021 - Representation Matters Assessing the Importance o.pdf}
}

@article{rosenblatt_remarks_1952,
  title = {Remarks on a {{Multivariate Transformation}}},
  author = {Rosenblatt, Murray},
  year = {1952},
  journal = {The Annals of Mathematical Statistics},
  volume = {23},
  number = {3},
  pages = {470--472},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851}
}

@inproceedings{rothenhausler_backshift_2015,
  title = {{{BACKSHIFT}}: {{Learning}} Causal Cyclic Graphs from Unknown Shift Interventions},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Rothenh{\"a}usler, Dominik and Heinze, Christina and Peters, Jonas and Meinshausen, Nicolai},
  editor = {Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  file = {/Users/fariedabuzaid/Zotero/storage/CRLPJJRI/appendix_backshift.pdf;/Users/fariedabuzaid/Zotero/storage/IQS9GL62/Rothenhäusler et al. - 2015 - BACKSHIFT Learning causal cyclic graphs from unkn.pdf}
}

@inproceedings{ruhe_selfsupervised_2022,
  title = {Self-{{Supervised Inference}} in {{State-Space Models}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Ruhe, David and Forr{\'e}, Patrick},
  year = {2022},
  abstract = {We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/GB9VZ6ZA/Ruhe and Forré - 2022 - Self-Supervised Inference in State-Space Models.pdf}
}

@inproceedings{sabanayagam_graphon_2022,
  title = {Graphon Based {{Clustering}} and {{Testing}} of {{Networks}}: {{Algorithms}} and {{Theory}}},
  shorttitle = {Graphon Based {{Clustering}} and {{Testing}} of {{Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Sabanayagam, Mahalakshmi and Vankadara, Leena Chennuru and Ghoshdastidar, Debarghya},
  year = {2022},
  abstract = {Network-valued data are encountered in a wide range of applications, and pose challenges in learning due to their complex structure and absence of vertex correspondence. Typical examples of such...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/3XHTPKHZ/Sabanayagam et al. - 2022 - Graphon based Clustering and Testing of Networks .pdf}
}

@article{sachs_causal_2005,
  title = {Causal {{Protein-Signaling Networks Derived}} from {{Multiparameter Single-Cell Data}}},
  author = {Sachs, Karen and Perez, Omar and Pe'er, Dana and Lauffenburger, Douglas A. and Nolan, Garry P.},
  year = {2005},
  month = apr,
  journal = {Science},
  volume = {308},
  number = {5721},
  pages = {523--529},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1105809},
  abstract = {Machine learning was applied for the automated derivation of causal influences in cellular signaling networks. This derivation relied on the simultaneous measurement of multiple phosphorylated protein and phospholipid components in thousands of individual primary human immune system cells. Perturbing these cells with molecular interventions drove the ordering of connections between pathway components, wherein Bayesian network computational methods automatically elucidated most of the traditionally reported signaling relationships and predicted novel interpathway network causalities, which we verified experimentally. Reconstruction of network models from physiologically relevant primary single cells might be applied to understanding native-state tissue signaling biology, complex drug actions, and dysfunctional signaling in diseased cells. Probabilistic analysis of the biochemical consequences of immune cell stimulation allows construction of causal signaling networks and prediction of new relationships. Probabilistic analysis of the biochemical consequences of immune cell stimulation allows construction of causal signaling networks and prediction of new relationships.},
  copyright = {American Association for the Advancement of Science},
  langid = {english},
  pmid = {15845847},
  annotation = {data: https://science.sciencemag.org/content/suppl/2005/04/21/308.5721.523.DC1},
  file = {/Users/fariedabuzaid/Zotero/storage/FKB3PKQM/Sachs et al. - 2005 - Causal Protein-Signaling Networks Derived from Mul.pdf;/Users/fariedabuzaid/Zotero/storage/HGA2VKF8/Sachs et al. - 2005 - Supplement.pdf}
}

@inproceedings{sahoo_learning_2018,
  title = {Learning {{Equations}} for {{Extrapolation}} and {{Control}}},
  booktitle = {{{arXiv}}:1806.07259 [Cs, Stat]},
  author = {Sahoo, Subham S. and Lampert, Christoph H. and Martius, Georg},
  year = {2018},
  month = jun,
  eprint = {1806.07259},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We present an approach to identify concise equations from data using a shallow neural network approach. In contrast to ordinary black-box regression, this approach allows understanding functional relations and generalizing them from observed data to unseen parts of the parameter space. We show how to extend the class of learnable equations for a recently proposed equation learning network to include divisions, and we improve the learning and model selection strategy to be useful for challenging real-world data. For systems governed by analytical expressions, our method can in many cases identify the true underlying equation and extrapolate to unseen domains. We demonstrate its effectiveness by experiments on a cart-pendulum system, where only 2 random rollouts are required to learn the forward dynamics and successfully achieve the swing-up task.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00004},
  file = {/Users/fariedabuzaid/Zotero/storage/2CXQ4KUH/Sahoo et al. - 2018 - Learning Equations for Extrapolation and Control.pdf}
}

@article{sahoo_reliable_2021,
  title = {Reliable {{Decisions}} with {{Threshold Calibration}}},
  author = {Sahoo, Roshni and Zhao, Shengjia and Chen, Alyssa and Ermon, Stefano},
  year = {2021},
  pages = {24},
  abstract = {Decision makers rely on probabilistic forecasts to predict the loss of different decision rules before deployment. When the forecasted probabilities match the true frequencies, predicted losses will be accurate. Although perfect forecasts are typically impossible, probabilities can be calibrated to match the true frequencies on average. However, we find that this average notion of calibration, which is typically used in practice, does not necessarily guarantee accurate decision loss prediction. Specifically in the regression setting, the loss of threshold decisions, which are decisions based on whether the forecasted outcome falls above or below a cutoff, might not be predicted accurately. We propose a stronger notion of calibration called threshold calibration, which is exactly the condition required to ensure that decision loss is predicted accurately for threshold decisions. We provide an efficient algorithm which takes an uncalibrated forecaster as input and provably outputs a threshold-calibrated forecaster. Our procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, threshold calibration improves decision loss prediction without compromising on the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/J3NBA4S5/Sahoo et al. - Reliable Decisions with Threshold Calibration.pdf}
}

@inproceedings{salimans_progressive_2021,
  title = {Progressive {{Distillation}} for {{Fast Sampling}} of {{Diffusion Models}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Salimans, Tim and Ho, Jonathan},
  year = {2021},
  month = sep,
  abstract = {Diffusion models have recently shown great promise for generative modeling, outperforming GANs on perceptual quality and autoregressive models at density estimation. A remaining downside is their...},
  langid = {english},
  annotation = {video: https://iclr.cc/virtual/2022/spotlight/6538},
  file = {/Users/fariedabuzaid/Zotero/storage/SZ7QTN7W/Salimans and Ho - 2021 - Progressive Distillation for Fast Sampling of Diff.pdf}
}

@inproceedings{salman_convex_2020,
  title = {A {{Convex Relaxation Barrier}} to {{Tight Robustness Verification}} of {{Neural Networks}}},
  booktitle = {Proc. of the Thirty-Third {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Salman, Hadi and Yang, Greg and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Pengchuan},
  year = {2020},
  month = jan,
  eprint = {1902.08722},
  eprinttype = {arxiv},
  address = {{Vancouver, Canada}},
  abstract = {Verification of neural networks enables us to gauge their robustness against adversarial attacks. Verification algorithms fall into two categories: exact verifiers that run in exponential time and relaxed verifiers that are efficient but incomplete. In this paper, we unify all existing LP-relaxed verifiers, to the best of our knowledge, under a general convex relaxation framework. This framework works for neural networks with diverse architectures and nonlinearities and covers both primal and dual views of robustness verification. We further prove strong duality between the primal and dual problems under very mild conditions. Next, we perform large-scale experiments, amounting to more than 22 CPU-years, to obtain exact solution to the convex-relaxed problem that is optimal within our framework for ReLU networks. We find the exact solution does not significantly improve upon the gap between PGD and existing relaxed verifiers for various networks trained normally or robustly on MNIST and CIFAR datasets. Our results suggest there is an inherent barrier to tight verification for the large class of methods captured by our framework. We discuss possible causes of this barrier and potential future directions for bypassing it. Our code and trained models are available at http://github.com/Hadisalman/robust-verify-benchmark .},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/EEBWF3GZ/Salman et al. - 2020 - A Convex Relaxation Barrier to Tight Robustness Ve.pdf}
}

@misc{sanh_distilbert_2020,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  shorttitle = {{{DistilBERT}}, a Distilled Version of {{BERT}}},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  year = {2020},
  month = feb,
  number = {arXiv:1910.01108},
  eprint = {1910.01108},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/WDHWVSEU/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf}
}

@inproceedings{schmidt_adversarially_2018,
  title = {Adversarially {{Robust Generalization Requires More Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Schmidt, Ludwig and Santurkar, Shibani and Tsipras, Dimitris and Talwar, Kunal and Madry, Aleksander},
  year = {2018},
  volume = {31},
  eprint = {1804.11285},
  eprinttype = {arxiv},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Machine learning models are often susceptible to adversarial perturbations of their inputs. Even small perturbations can cause state-of-the-art classifiers with high "standard" accuracy to produce an incorrect prediction with high confidence. To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization. We show that already in a simple natural data model, the sample complexity of robust learning can be significantly larger than that of "standard" learning. This gap is information theoretic and holds irrespective of the training algorithm or the model family. We complement our theoretical results with experiments on popular image classification datasets and show that a similar gap exists here as well. We postulate that the difficulty of training robust classifiers stems, at least partially, from this inherently larger sample complexity.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/YLHWMDU3/Schmidt et al. - 2018 - Adversarially Robust Generalization Requires More .pdf}
}

@article{schulman_proximal_2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  journal = {arXiv:1707.06347 [cs]},
  eprint = {1707.06347},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {blog: https://openai.com/blog/openai-baselines-ppo/},
  file = {/Users/fariedabuzaid/Zotero/storage/77LKGJ5M/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf}
}

@inproceedings{schulman_trust_2015,
  title = {Trust {{Region Policy Optimization}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  year = {2015},
  month = jun,
  eprint = {1502.05477},
  eprinttype = {arxiv},
  pages = {1889--1897},
  publisher = {{PMLR}},
  abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/7HU7YZUR/schulman15-supp.pdf;/Users/fariedabuzaid/Zotero/storage/SM6I52LR/Schulman et al. - 2015 - Trust Region Policy Optimization.pdf}
}

@inproceedings{shah_label_2022,
  title = {Label {{Encoding}} for {{Regression Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR2022}})},
  author = {Shah, Deval and Xue, Zi Yu and Aamodt, Tor},
  year = {2022},
  address = {{Virtual event}},
  abstract = {Deep neural networks are used for a wide range of regression problems. However, there exists a significant gap in accuracy between specialized approaches and generic direct regression in which a network is trained by minimizing the squared or absolute error of output labels. Prior work has shown that solving a regression problem with a set of binary classifiers can improve accuracy by utilizing well-studied binary classification algorithms. We introduce binary-encoded labels (BEL), which generalizes the application of binary classification to regression by providing a framework for considering arbitrary multi-bit values when encoding target values. We identify desirable properties of suitable encoding and decoding functions used for the conversion between real-valued and binary-encoded labels based on theoretical and empirical study. These properties highlight a tradeoff between classification error probability and error-correction capabilities of label encodings. BEL can be combined with off-the-shelf task-specific feature extractors and trained end-to-end. We propose a series of sample encoding, decoding, and training loss functions for BEL and demonstrate they result in lower error than direct regression and specialized approaches while being suitable for a diverse set of regression problems, network architectures, and evaluation metrics. BEL achieves state-of-the-art accuracies for several regression benchmarks. Code is available at https://github.com/ubc-aamodt-group/BEL\_regression.},
  langid = {english},
  annotation = {notion: https://www.notion.so/appliedaiinitiative/Label-Encoding-for-Regression-Networks-b8c658744eca48c5aba4f0370e2b8a0d post: https://community.appliedai.de/topics/27304/topic\_feed\_posts/124573},
  file = {/Users/fariedabuzaid/Zotero/storage/GGZYYLG2/Shah et al. - Label Encoding for Regression Networks.pdf;/Users/fariedabuzaid/Zotero/storage/IRFFEPVY/Shah et al. - 2021 - Label Encoding for Regression Networks.pdf}
}

@article{sharchilev_finding_2018,
  title = {Finding {{Influential Training Samples}} for {{Gradient Boosted Decision Trees}}},
  author = {Sharchilev, Boris and Ustinovsky, Yury and Serdyukov, Pavel and {de Rijke}, Maarten},
  year = {2018},
  month = feb,
  abstract = {We address the problem of finding influential training samples for a particular case of tree ensemble-based models, e.g., Random Forest (RF) or Gradient Boosted Decision Trees (GBDT). A natural way of formalizing this problem is studying how the model's predictions change upon leave-one-out retraining, leaving out each individual training sample. Recent work has shown that, for parametric models, this analysis can be conducted in a computationally efficient way. We propose several ways of extending this framework to non-parametric GBDT ensembles under the assumption that tree structures remain fixed. Furthermore, we introduce a general scheme of obtaining further approximations to our method that balance the trade-off between performance and computational complexity. We evaluate our approaches on various experimental setups and use-case scenarios and demonstrate both the quality of our approach to finding influential training samples in comparison to the baselines and its computational efficiency.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/QNRYBX5P/Sharchilev et al. - 2018 - Finding Influential Training Samples for Gradient .pdf}
}

@inproceedings{sharif_suitability_2018,
  title = {On the {{Suitability}} of {{Lp-Norms}} for {{Creating}} and {{Preventing Adversarial Examples}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Sharif, Mahmood and Bauer, Lujo and Reiter, Michael K.},
  year = {2018},
  month = jun,
  pages = {1686--16868},
  issn = {2160-7516},
  doi = {10.1109/CVPRW.2018.00211},
  abstract = {Much research has been devoted to better understanding adversarial examples, which are specially crafted inputs to machine-learning models that are perceptually similar to benign inputs, but are classified differently (i.e., misclassified). Both algorithms that create adversarial examples and strategies for defending against adversarial examples typically use Lp-norms to measure the perceptual similarity between an adversarial input and its benign original. Prior work has already shown, however, that two images need not be close to each other as measured by an Lp-norm to be perceptually similar. In this work, we show that nearness according to an Lp-norm is not just unnecessary for perceptual similarity, but is also insufficient. Specifically, focusing on datasets (CIFAR10 and MNIST), Lp-norms, and thresholds used in prior work, we show through online user studies that "adversarial examples" that are closer to their benign counterparts than required by commonly used Lp-norm thresholds can nevertheless be perceptually distinct to humans from the corresponding benign examples. Namely, the perceptual distance between two images that are "near" each other according to an Lp-norm can be high enough that participants frequently classify the two images as representing different objects or digits. Combined with prior work, we thus demonstrate that nearness of inputs as measured by Lp-norms is neither necessary nor sufficient for perceptual similarity, which has implications for both creating and defending against adversarial examples. We propose and discuss alternative similarity metrics to stimulate future research in the area.},
  file = {/Users/fariedabuzaid/Zotero/storage/BQEYCG92/Sharif et al. - 2018 - On the Suitability of Lp-Norms for Creating and Pr.pdf}
}

@article{shazeer_outrageously_2017,
  title = {Outrageously {{Large Neural Networks}}: {{The Sparsely-Gated Mixture-of-Experts Layer}}},
  shorttitle = {Outrageously {{Large Neural Networks}}},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  year = {2017},
  month = jan,
  journal = {arXiv:1701.06538 [cs, stat]},
  eprint = {1701.06538},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00073},
  file = {/Users/fariedabuzaid/Zotero/storage/9AN4T3CJ/Shazeer et al. - 2017 - Outrageously Large Neural Networks The Sparsely-G.pdf}
}

@inproceedings{shi_revisiting_2022,
  title = {Revisiting {{Over-smoothing}} in {{BERT}} from the {{Perspective}} of {{Graph}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Shi, Han and Gao, Jiahui and Xu, Hang and Liang, Xiaodan and Li, Zhenguo and Kong, Lingpeng and Lee, Stephen M. S. and Kwok, James},
  year = {2022},
  abstract = {Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/XGL9HEBU/Shi et al. - 2022 - Revisiting Over-smoothing in BERT from the Perspec.pdf}
}

@article{shimizu_directlingam_2011,
  title = {{{DirectLiNGAM}}: {{A Direct Method}} for {{Learning}} a {{Linear Non-Gaussian Structural Equation Model}}},
  shorttitle = {{{DirectLiNGAM}}},
  author = {Shimizu, Shohei and Inazumi, Takanori and Sogawa, Yasuhiro and Hyv{\"a}rinen, Aapo and Kawahara, Yoshinobu and Washio, Takashi and Hoyer, Patrik O. and Bollen, Kenneth},
  year = {2011},
  month = jul,
  journal = {The Journal of Machine Learning Research},
  volume = {12},
  number = {null},
  pages = {1225--1248},
  issn = {1532-4435},
  abstract = {Structural equation models and Bayesian networks have been widely used to analyze causal relations between continuous variables. In such frameworks, linear acyclic models are typically used to model the data-generating process of variables. Recently, it was shown that use of non-Gaussianity identifies the full structure of a linear acyclic model, that is, a causal ordering of variables and their connection strengths, without using any prior knowledge on the network structure, which is not the case with conventional methods. However, existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a finite number of steps. In this paper, we propose a new direct method to estimate a causal ordering and connection strengths based on non-Gaussianity. In contrast to the previous methods, our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small fixed number of steps if the data strictly follows the model, that is, if all the model assumptions are met and the sample size is infinite.},
  file = {/Users/fariedabuzaid/Zotero/storage/X9ZWPUUI/Shimizu et al. - 2011 - DirectLiNGAM A Direct Method for Learning a Linea.pdf}
}

@article{shin_convergence_2020,
  title = {On the Convergence of Physics Informed Neural Networks for Linear Second-Order Elliptic and Parabolic Type {{PDEs}}},
  author = {Shin, Yeonjong and Darbon, Jerome and Karniadakis, George Em},
  year = {2020},
  month = jun,
  journal = {Communications in Computational Physics},
  volume = {28},
  number = {5},
  eprint = {2004.01806},
  eprinttype = {arxiv},
  pages = {2042--2074},
  issn = {1815-2406, 1991-7120},
  doi = {10.4208/cicp.OA-2020-0193},
  abstract = {Physics informed neural networks (PINNs) are deep learning based techniques for solving partial differential equations (PDEs) encounted in computational science and engineering. Guided by data and physical laws, PINNs find a neural network that approximates the solution to a system of PDEs. Such a neural network is obtained by minimizing a loss function in which any prior knowledge of PDEs and data are encoded. Despite its remarkable empirical success in one, two or three dimensional problems, there is little theoretical justification for PINNs. As the number of data grows, PINNs generate a sequence of minimizers which correspond to a sequence of neural networks. We want to answer the question: Does the sequence of minimizers converge to the solution to the PDE? We consider two classes of PDEs: linear second-order elliptic and parabolic. By adapting the Schauder approach and the maximum principle, we show that the sequence of minimizers strongly converges to the PDE solution in \$C\^0\$. Furthermore, we show that if each minimizer satisfies the initial/boundary conditions, the convergence mode becomes \$H\^1\$. Computational examples are provided to illustrate our theoretical findings. To the best of our knowledge, this is the first theoretical work that shows the consistency of PINNs.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/UUCG6YKM/Shin et al. - 2020 - On the convergence of physics informed neural netw.pdf}
}

@inproceedings{siddiqui_finite_2016,
  title = {Finite {{Sample Complexity}} of {{Rare Pattern Anomaly Detection}}},
  booktitle = {31st {{Conference}} on {{Uncertainty}} in {{Artificial Intelligence UAI}}},
  author = {Siddiqui, Amran and Fern, Alan and Dietterich, Thomas G and Das, Shubhomoy},
  year = {2016},
  pages = {10},
  address = {{New York}},
  abstract = {Anomaly detection is a fundamental problem for which a wide variety of algorithms have been developed. However, compared to supervised learning, there has been very little work aimed at understanding the sample complexity of anomaly detection. In this paper, we take a step in this direction by introducing a Probably Approximately Correct (PAC) framework for anomaly detection based on the identification of rare patterns. In analogy with the PAC framework for supervised learning, we develop sample complexity results that relate the complexity of the pattern space to the data requirements needed for PAC guarantees. We instantiate the general result for a number of pattern spaces, some of which are implicit in current state-of-the-art anomaly detectors. Finally, we design a new simple anomaly detection algorithm motivated by our analysis and show experimentally on several benchmark problems that it is competitive with a state-of-the-art detector using the same pattern space.},
  langid = {english},
  annotation = {citecount: 00006},
  file = {/Users/fariedabuzaid/Zotero/storage/4JLBS2YH/Siddiqui et al. - Finite Sample Complexity of Rare Pattern Anomaly D.pdf}
}

@inproceedings{siffer_anomaly_2017,
  title = {Anomaly {{Detection}} in {{Streams}} with {{Extreme Value Theory}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Siffer, Alban and Fouque, Pierre-Alain and Termier, Alexandre and Largouet, Christine},
  year = {2017},
  month = aug,
  pages = {1067--1075},
  publisher = {{ACM}},
  address = {{Halifax NS Canada}},
  doi = {10.1145/3097983.3098144},
  abstract = {Anomaly detection in time series has attracted considerable attention due to its importance in many real-world applications including intrusion detection, energy management and finance. Most approaches for detecting outliers rely on either manually set thresholds or assumptions on the distribution of data according to Chandola, Banerjee and Kumar. Here, we propose a new approach to detect outliers in streaming univariate time series based on Extreme Value Theory that does not require to hand-set thresholds and makes no assumption on the distribution: the main parameter is only the risk, controlling the number of false positives. Our approach can be used for outlier detection, but more generally for automatically setting thresholds, making it useful in wide number of situations. We also experiment our algorithms on various real-world datasets which confirm its soundness and efficiency.},
  isbn = {978-1-4503-4887-4},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/PDRMMBCN/Siffer et al. - 2017 - Anomaly Detection in Streams with Extreme Value Th.pdf}
}

@article{simonyan_very_2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  journal = {arXiv:1409.1556 [cs]},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 \texttimes{} 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16\textendash 19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/EE944GQ2/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf}
}

@inproceedings{singh_deep_2021,
  title = {On {{Deep Neural Network Calibration}} by {{Regularization}} and Its {{Impact}} on {{Refinement}}},
  booktitle = {{{arXiv}}:2106.09385 [Cs]},
  author = {Singh, Aditya and Bay, Alessandro and Sengupta, Biswa and Mirabile, Andrea},
  year = {2021},
  month = oct,
  eprint = {2106.09385},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Deep neural networks have been shown to be highly miscalibrated. often they tend to be overconfident in their predictions. It poses a significant challenge for safety-critical systems to utilise deep neural networks (DNNs), reliably. Many recently proposed approaches to mitigate this have demonstrated substantial progress in improving DNN calibration. However, they hardly touch upon refinement, which historically has been an essential aspect of calibration. Refinement indicates separability of a network's correct and incorrect predictions. This paper presents a theoretically and empirically supported exposition reviewing refinement of a calibrated model. Firstly, we show the breakdown of expected calibration error (ECE), into predicted confidence and refinement under the assumption of over-confident predictions. Secondly, linking with this result, we highlight that regularization based calibration only focuses on naively reducing a model's confidence. This logically has a severe downside to a model's refinement as correct and incorrect predictions become tightly coupled. Lastly, connecting refinement with ECE also provides support to existing refinement based approaches which improve calibration but do not explain the reasoning behind it. We support our claims through rigorous empirical evaluations of many state of the art calibration approaches on widely used datasets and neural networks. We find that many calibration approaches with the likes of label smoothing, mixup etc. lower the usefulness of a DNN by degrading its refinement. Even under natural data shift, this calibration-refinement trade-off holds for the majority of calibration methods.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/RCZNUEMC/Singh et al. - 2021 - On Deep Neural Network Calibration by Regularizati.pdf}
}

@article{sirignano_dgm_2018,
  title = {{{DGM}}: {{A}} Deep Learning Algorithm for Solving Partial Differential Equations},
  shorttitle = {{{DGM}}},
  author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
  year = {2018},
  month = dec,
  journal = {Journal of Computational Physics},
  volume = {375},
  eprint = {1708.07469},
  eprinttype = {arxiv},
  pages = {1339--1364},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2018.08.029},
  abstract = {High-dimensional PDEs have been a longstanding computational challenge. We propose to solve high-dimensional PDEs by approximating the solution with a deep neural network which is trained to satisfy the differential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary PDEs, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton\textendash Jacobi\textendash Bellman PDE and Burgers' equation. The deep learning algorithm approximates the general solution to the Burgers' equation for a continuum of different boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a ``Deep Galerkin Method (DGM)'' since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic PDEs.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {citecount: 00097},
  file = {/Users/fariedabuzaid/Zotero/storage/HDQQ6CJC/Sirignano and Spiliopoulos - 2018 - DGM A deep learning algorithm for solving partial.pdf}
}

@inproceedings{sohl-dickstein_deep_2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  month = jun,
  pages = {2256--2265},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/HFAWL44F/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf}
}

@inproceedings{song_denoising_2022,
  title = {Denoising {{Diffusion Implicit Models}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  year = {2022},
  abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps in order to...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/KB5Z7558/Song et al. - 2020 - Denoising Diffusion Implicit Models.pdf}
}

@article{song_generative_2020,
  title = {Generative {{Modeling}} by {{Estimating Gradients}} of the {{Data Distribution}}},
  author = {Song, Yang and Ermon, Stefano},
  year = {2020},
  month = oct,
  journal = {arXiv:1907.05600 [cs, stat]},
  eprint = {1907.05600},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/BR39I43H/Song and Ermon - 2020 - Generative Modeling by Estimating Gradients of the.pdf}
}

@inproceedings{song_scorebased_2020,
  title = {Score-{{Based Generative Modeling}} through {{Stochastic Differential Equations}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2020},
  month = sep,
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/A4DV5GGP/Song et al. - 2020 - Score-Based Generative Modeling through Stochastic.pdf}
}

@article{soudry_implicit_2018,
  title = {The {{Implicit Bias}} of {{Gradient Descent}} on {{Separable Data}}},
  author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  year = {2018},
  month = dec,
  journal = {Journal of Machine Learning Research},
  volume = {19},
  number = {70},
  eprint = {1710.10345},
  eprinttype = {arxiv},
  pages = {1--57},
  abstract = {We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization n more complex models and with other optimization methods.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/8XPU42G7/Soudry et al. - 2018 - The Implicit Bias of Gradient Descent on Separable.pdf}
}

@book{spirtes_causation_1993,
  title = {Causation, {{Prediction}}, and {{Search}}},
  author = {Spirtes, Peter and Glymour, Clark and Scheines, Richard},
  year = {1993},
  series = {Lecture {{Notes}} in {{Statistics}}},
  publisher = {{Springer-Verlag}},
  address = {{New York}},
  doi = {10.1007/978-1-4612-2748-9},
  abstract = {This book is intended for anyone, regardless of discipline, who is interested in the use of statistical methods to help obtain scientific explanations or to predict the outcomes of actions, experiments or policies. Much of G. Udny Yule's work illustrates a vision of statistics whose goal is to investigate when and how causal influences may be reliably inferred, and their comparative strengths estimated, from statistical samples. Yule's enterprise has been largely replaced by Ronald Fisher's conception, in which there is a fundamental cleavage between experimental and non\- experimental inquiry, and statistics is largely unable to aid in causal inference without randomized experimental trials. Every now and then members of the statistical community express misgivings about this turn of events, and, in our view, rightly so. Our work represents a return to something like Yule's conception of the enterprise of theoretical statistics and its potential practical benefits. If intellectual history in the 20th century had gone otherwise, there might have been a discipline to which our work belongs. As it happens, there is not. We develop material that belongs to statistics, to computer science, and to philosophy; the combination may not be entirely satisfactory for specialists in any of these subjects. We hope it is nonetheless satisfactory for its purpose.},
  isbn = {978-1-4612-7650-0},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/CZLM9JWG/Spirtes et al. - 1993 - Causation, Prediction, and Search.pdf}
}

@inproceedings{sriram_training_2022,
  title = {Towards {{Training Billion Parameter Graph Neural Networks}} for {{Atomic Simulations}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Sriram, Anuroop and Das, Abhishek and Wood, Brandon M. and Goyal, Siddharth and Zitnick, C. Lawrence},
  year = {2022},
  abstract = {Recent progress in Graph Neural Networks (GNNs) for modeling atomic simulations has the potential to revolutionize catalyst discovery, which is a key step in making progress towards the energy...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/HDQ6RK3C/Sriram et al. - 2022 - Towards Training Billion Parameter Graph Neural Ne.pdf}
}

@article{sukhbaatar_adaptive_2019,
  title = {Adaptive {{Attention Span}} in {{Transformers}}},
  author = {Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  year = {2019},
  month = aug,
  journal = {arXiv:1905.07799 [cs, stat]},
  eprint = {1905.07799},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/5MNDL4FP/Sukhbaatar et al. - 2019 - Adaptive Attention Span in Transformers.pdf}
}

@article{sutton_policy_1999,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  author = {Sutton, Richard S. and McAllester, David and Singh, Satinder and Mansour, Yishay},
  year = {1999},
  journal = {Advances in Neural Information Processing Systems},
  volume = {12},
  abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy  is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's  REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration  with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/TVEUZIXR/Sutton et al. - 1999 - Policy Gradient Methods for Reinforcement Learning.pdf}
}

@article{szegedy_intriguing_2013,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  year = {2013},
  month = dec,
  journal = {arXiv:1312.6199 [cs]},
  eprint = {1312.6199},
  eprinttype = {arxiv},
  primaryclass = {cs},
  pages = {10},
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  archiveprefix = {arXiv},
  annotation = {citecount: 00438},
  file = {/Users/fariedabuzaid/Zotero/storage/4CSUVN3L/Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf}
}

@inproceedings{tailor_we_2022,
  title = {Do {{We Need Anisotropic Graph Neural Networks}}?},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Tailor, Shyam A. and Opolka, Felix and Lio, Pietro and Lane, Nicholas Donald},
  year = {2022},
  abstract = {Common wisdom in the graph neural network (GNN) community dictates that anisotropic models---in which messages sent between nodes are a function of both the source and target node---are required to...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/Y84CALEJ/Tailor et al. - 2021 - Do We Need Anisotropic Graph Neural Networks.pdf;/Users/fariedabuzaid/Zotero/storage/Y6CDRPLV/forum.html}
}

@inproceedings{tang_graph_2022,
  title = {Graph {{Auto-Encoder}} via {{Neighborhood Wasserstein Reconstruction}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Tang, Mingyue and Li, Pan and Yang, Carl},
  year = {2022},
  abstract = {Graph neural networks (GNNs) have drawn significant research attention recently, mostly under the setting of semi-supervised learning. When task-agnostic representations are preferred or...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/S8ZYSB5B/Tang et al. - 2021 - Graph Auto-Encoder via Neighborhood Wasserstein Re.pdf}
}

@inproceedings{tang_selfsupervised_2022,
  title = {Self-{{Supervised Graph Neural Networks}} for {{Improved Electroencephalographic Seizure Analysis}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Tang, Siyi and Dunnmon, Jared and Saab, Khaled Kamal and Zhang, Xuan and Huang, Qianying and Dubost, Florian and Rubin, Daniel and {Lee-Messer}, Christopher},
  year = {2022},
  abstract = {Automated seizure detection and classification from electroencephalography (EEG) can greatly improve seizure diagnosis and treatment. However, several modeling challenges remain unaddressed in...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/9I9DB3KL/Tang et al. - 2021 - Self-Supervised Graph Neural Networks for Improved.pdf;/Users/fariedabuzaid/Zotero/storage/GDGPK3S8/forum.html}
}

@inproceedings{taori_measuring_2020,
  title = {Measuring Robustness to Natural Distribution Shifts in Image Classification},
  booktitle = {Advances in {{Neural Information Processing Systems}} 33 ({{NeurIPS}} 2020)},
  author = {Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {18583--18599},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We study how robust current ImageNet models are to distribution shifts arising from natural variations in datasets. Most research on robustness focuses on synthetic image perturbations (noise, simulated weather artifacts, adversarial examples, etc.), which leaves open how robustness on synthetic distribution shift relates to distribution shift arising in real data. Informed by an evaluation of 204 ImageNet models in 213 different test conditions, we find that there is often little to no transfer of robustness from current synthetic to natural distribution shift. Moreover, most current techniques provide no robustness to the natural distribution shifts in our testbed. The main exception is training on larger and more diverse datasets, which in multiple cases increases robustness, but is still far from closing the performance gaps. Our results indicate that distribution shifts arising in real data are currently an open research problem.},
  annotation = {video: https://www.youtube.com/watch?v=YWMlfRuap48\&t=3s\&ab\_channel=SimonsInstitute},
  file = {/Users/fariedabuzaid/Zotero/storage/85AQ2RFH/NeurIPS-2020-measuring-robustness-to-natural-distribution-shifts-in-image-classification-Supplemental.pdf;/Users/fariedabuzaid/Zotero/storage/Z6HHSZMH/Taori et al. - 2020 - Measuring robustness to natural distribution shift.pdf}
}

@inproceedings{tavallaee_detailed_2009,
  title = {A Detailed Analysis of the {{KDD CUP}} 99 Data Set},
  booktitle = {Proceedings of the {{Second IEEE}} International Conference on {{Computational}} Intelligence for Security and Defense Applications},
  author = {Tavallaee, Mahbod and Bagheri, Ebrahim and Lu, Wei and Ghorbani, Ali A.},
  year = {2009},
  month = jul,
  series = {{{CISDA}}'09},
  pages = {53--58},
  publisher = {{IEEE Press}},
  address = {{Ottawa, Ontario, Canada}},
  abstract = {During the last decade, anomaly detection has attracted the attention of many researchers to overcome the weakness of signature-based IDSs in detecting novel attacks, and KDDCUP'99 is the mostly widely used data set for the evaluation of these systems. Having conducted a statistical analysis on this data set, we found two important issues which highly affects the performance of evaluated systems, and results in a very poor evaluation of anomaly detection approaches. To solve these issues, we have proposed a new data set, NSL-KDD, which consists of selected records of the complete KDD data set and does not suffer from any of mentioned shortcomings.},
  isbn = {978-1-4244-3763-4},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/S6DIVTNL/Tavallaee et al. - 2009 - A detailed analysis of the KDD CUP 99 data set.pdf}
}

@inproceedings{thakoor_largescale_2022,
  title = {Large-{{Scale Representation Learning}} on {{Graphs}} via {{Bootstrapping}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Thakoor, Shantanu and Tallec, Corentin and Azar, Mohammad Gheshlaghi and Azabou, Mehdi and Dyer, Eva L. and Munos, Remi and Veli{\v c}kovi{\'c}, Petar and Valko, Michal},
  year = {2022},
  abstract = {Self-supervised learning provides a promising path towards eliminating the need for costly label information in representation learning on graphs.  However, to achieve state-of-the-art performance,...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/C6VFUAD3/Thakoor et al. - 2021 - Large-Scale Representation Learning on Graphs via .pdf;/Users/fariedabuzaid/Zotero/storage/FVR37JLC/forum.html}
}

@inproceedings{tholke_equivariant_2022,
  title = {Equivariant {{Transformers}} for {{Neural Network}} Based {{Molecular Potentials}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Th{\"o}lke, Philipp and Fabritiis, Gianni De},
  year = {2022},
  abstract = {The prediction of quantum mechanical properties is historically plagued by a trade-off between accuracy and speed. Machine learning potentials have previously shown great success in this domain...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/7NYA9LFN/Thölke and Fabritiis - 2022 - Equivariant Transformers for Neural Network based .pdf}
}

@article{thomas_likelihoodfree_2020,
  title = {Likelihood-Free Inference by Ratio Estimation},
  author = {Thomas, Owen and Dutta, Ritabrata and Corander, Jukka and Kaski, Samuel and Gutmann, Michael U.},
  year = {2020},
  month = sep,
  journal = {arXiv:1611.10242 [stat]},
  eprint = {1611.10242},
  eprinttype = {arxiv},
  primaryclass = {stat},
  abstract = {We consider the problem of parametric statistical inference when likelihood computations are prohibitively expensive but sampling from the model is possible. Several so-called likelihood-free methods have been developed to perform inference in the absence of a likelihood function. The popular synthetic likelihood approach infers the parameters by modelling summary statistics of the data by a Gaussian probability distribution. In another popular approach called approximate Bayesian computation, the inference is performed by identifying parameter values for which the summary statistics of the simulated data are close to those of the observed data. Synthetic likelihood is easier to use as no measure of `closeness' is required but the Gaussianity assumption is often limiting. Moreover, both approaches require judiciously chosen summary statistics. We here present an alternative inference approach that is as easy to use as synthetic likelihood but not as restricted in its assumptions, and that, in a natural way, enables automatic selection of relevant summary statistic from a large set of candidates. The basic idea is to frame the problem of estimating the posterior as a problem of estimating the ratio between the data generating distribution and the marginal distribution. This problem can be solved by logistic regression, and including regularising penalty terms enables automatic selection of the summary statistics relevant to the inference task. We illustrate the general theory on canonical examples and employ it to perform inference for challenging stochastic nonlinear dynamical systems and high-dimensional summary statistics.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/MIFQH9ZW/Thomas et al. - 2020 - Likelihood-free inference by ratio estimation.pdf}
}

@inproceedings{thompson_evaluation_2022,
  title = {On {{Evaluation Metrics}} for {{Graph Generative Models}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Thompson, Rylee and Knyazev, Boris and Ghalebi, Elahe and Kim, Jungtaek and Taylor, Graham W.},
  year = {2022},
  abstract = {In image generation, generative models can be evaluated naturally by visually inspecting model outputs. However, this is not always the case for graph generative models (GGMs), making their...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/N3LEIYIP/Thompson et al. - 2021 - On Evaluation Metrics for Graph Generative Models.pdf;/Users/fariedabuzaid/Zotero/storage/YBB35EUF/forum.html}
}

@article{tibshirani_bias_2009,
  title = {A Bias Correction for the Minimum Error Rate in Cross-Validation},
  author = {Tibshirani, Ryan J. and Tibshirani, Robert},
  year = {2009},
  month = jun,
  journal = {The Annals of Applied Statistics},
  volume = {3},
  number = {2},
  pages = {822--829},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/08-AOAS224},
  abstract = {Tuning parameters in supervised learning problems are often estimated by cross-validation. The minimum value of the cross-validation error can be biased downward as an estimate of the test error at that same value of the tuning parameter. We propose a simple method for the estimation of this bias that uses information from the cross-validation process. As a result, it requires essentially no additional computation. We apply our bias estimate to a number of popular classifiers in various settings, and examine its performance.},
  file = {/Users/fariedabuzaid/Zotero/storage/PNW2RRMN/NVN5G8P4.pdf}
}

@inproceedings{topping_understanding_2022,
  title = {Understanding Over-Squashing and Bottlenecks on Graphs via Curvature},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Topping, Jake and Giovanni, Francesco Di and Chamberlain, Benjamin Paul and Dong, Xiaowen and Bronstein, Michael M.},
  year = {2022},
  abstract = {Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from...},
  langid = {english},
  annotation = {video: https://iclr.cc/virtual/2022/oral/6850},
  file = {/Users/fariedabuzaid/Zotero/storage/KX5HT5IB/Topping et al. - 2021 - Understanding over-squashing and bottlenecks on gr.pdf}
}

@inproceedings{touvron_training_2021,
  title = {Training Data-Efficient Image Transformers \& Distillation through Attention},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  year = {2021},
  month = jan,
  volume = {139},
  eprint = {2012.12877},
  eprinttype = {arxiv},
  pages = {10347--10357},
  publisher = {{PMLR}},
  abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/V5TNMAUD/Touvron et al. - 2021 - Training data-efficient image transformers & disti.pdf}
}

@article{tsamardinos_maxmin_2006,
  title = {The Max-Min Hill-Climbing {{Bayesian}} Network Structure Learning Algorithm},
  author = {Tsamardinos, Ioannis and Brown, Laura E. and Aliferis, Constantin F.},
  year = {2006},
  month = oct,
  journal = {Machine Learning},
  volume = {65},
  number = {1},
  pages = {31--78},
  issn = {1573-0565},
  doi = {10.1007/s10994-006-6889-7},
  abstract = {We present a new algorithm for Bayesian network structure learning, called Max-Min Hill-Climbing (MMHC). The algorithm combines ideas from local learning, constraint-based, and search-and-score techniques in a principled and effective way. It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. In our extensive empirical evaluation MMHC outperforms on average and in terms of various metrics several prototypical and state-of-the-art algorithms, namely the PC, Sparse Candidate, Three Phase Dependency Analysis, Optimal Reinsertion, Greedy Equivalence Search, and Greedy Search. These are the first empirical results simultaneously comparing most of the major Bayesian network algorithms against each other. MMHC offers certain theoretical advantages, specifically over the Sparse Candidate algorithm, corroborated by our experiments. MMHC and detailed results of our study are publicly available at http://www.dsl-lab.org/supplements/mmhc\_paper/mmhc\_index.html.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/MSPFD6W5/Tsamardinos et al. - 2006 - The max-min hill-climbing Bayesian network structu.pdf}
}

@inproceedings{tsipras_robustness_2019,
  title = {Robustness {{May Be}} at {{Odds}} with {{Accuracy}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2019)},
  author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
  year = {2019},
  abstract = {We show that adversarial robustness might come at the cost of standard classification performance, but also yields unexpected benefits.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/XQZ3JLSU/Tsipras et al. - 2018 - Robustness May Be at Odds with Accuracy.pdf}
}

@article{tsybakov_nonparametric_1997,
  title = {On {{Nonparametric Estimation}} of {{Density Level Sets}}},
  author = {Tsybakov, A. B.},
  year = {1997},
  journal = {The Annals of Statistics},
  volume = {25},
  number = {3},
  pages = {948--969},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  abstract = {Let X1, ..., Xn be independent identically distributed observations from an unknown probability density f({$\cdot$}). Consider the problem of estimating the level set G = Gf({$\lambda$}) = \{x {$\in$} R2: f(x) {$\geq$} {$\lambda\rbrace$} from the sample X1, ..., Xn, under the assumption that the boundary of G has a certain smoothness. We propose piecewise-polynomial estimators of G based on the maximization of local empirical excess masses. We show that the estimators have optimal rates of convergence in the asymptotically minimax sense within the studied classes of densities. We find also the optimal convergence rates for estimation of convex level sets. A generalization to the N-dimensional case, where \$N {$>$} 2\$, is given.\vphantom\}}
}

@techreport{tsyplakov_evaluation_2013,
  type = {{{SSRN Scholarly Paper}}},
  title = {Evaluation of {{Probabilistic Forecasts}}: {{Proper Scoring Rules}} and {{Moments}}},
  shorttitle = {Evaluation of {{Probabilistic Forecasts}}},
  author = {Tsyplakov, Alexander},
  year = {2013},
  month = mar,
  number = {ID 2236605},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.2236605},
  abstract = {The paper provides an overview of probabilistic forecasting and discusses a theoretical framework for evaluation of probabilistic forecasts which is based on proper scoring rules and moments. An artificial example of predicting second-order autoregression and an example of predicting the RTSI stock index are used as illustrations.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/3S3J7UMZ/Tsyplakov - 2013 - Evaluation of Probabilistic Forecasts Proper Scor.pdf}
}

@inproceedings{um_solverintheloop_2021,
  title = {Solver-in-the-{{Loop}}: {{Learning}} from {{Differentiable Physics}} to {{Interact}} with {{Iterative PDE-Solvers}}},
  shorttitle = {Solver-in-the-{{Loop}}},
  booktitle = {{{arXiv}}:2007.00016 [Physics]},
  author = {Um, Kiwon and Brand, Robert and Yun and Fei and Holl, Philipp and Thuerey, Nils},
  year = {2021},
  month = jan,
  eprint = {2007.00016},
  eprinttype = {arxiv},
  primaryclass = {physics},
  abstract = {Finding accurate solutions to partial differential equations (PDEs) is a crucial task in all scientific and engineering disciplines. It has recently been shown that machine learning methods can improve the solution accuracy by correcting for effects not captured by the discretized PDE. We target the problem of reducing numerical errors of iterative PDE solvers and compare different learning approaches for finding complex correction functions. We find that previously used learning approaches are significantly outperformed by methods that integrate the solver into the training loop and thereby allow the model to interact with the PDE during training. This provides the model with realistic input distributions that take previous corrections into account, yielding improvements in accuracy with stable rollouts of several hundred recurrent evaluation steps and surpassing even tailored supervised variants. We highlight the performance of the differentiable physics networks for a wide variety of PDEs, from non-linear advection-diffusion systems to three-dimensional Navier-Stokes flows.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/3URSXWLU/Um et al. - 2021 - Solver-in-the-Loop Learning from Differentiable P.pdf}
}

@inproceedings{vaicenavicius_evaluating_2019,
  title = {Evaluating Model Calibration in Classification},
  booktitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Vaicenavicius, Juozas and Widmann, David and Andersson, Carl and Lindsten, Fredrik and Roll, Jacob and Sch{\"o}n, Thomas},
  year = {2019},
  month = apr,
  eprint = {1902.06977},
  eprinttype = {arxiv},
  pages = {3459--3467},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Probabilistic classifiers output a probability distribution on target classes rather than just a class prediction. Besides providing a clear separation of prediction and decision making, the main advantage of probabilistic models is their ability to represent uncertainty about predictions. In safety-critical applications, it is pivotal for a model to possess an adequate sense of uncertainty, which for probabilistic classifiers translates into outputting probability distributions that are consistent with the empirical frequencies observed from realized outcomes. A classifier with such a property is called calibrated. In this work, we develop a general theoretical calibration evaluation framework grounded in probability theory, and point out subtleties present in model calibration evaluation that lead to refined interpretations of existing evaluation techniques. Lastly, we propose new ways to quantify and visualize miscalibration in probabilistic classification, including novel multidimensional reliability diagrams.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/PXUAQFHB/Vaicenavicius et al. - 2019 - Evaluating model calibration in classification.pdf}
}

@article{varma_bias_2006,
  title = {Bias in Error Estimation When Using Cross-Validation for Model Selection},
  author = {Varma, Sudhir and Simon, Richard},
  year = {2006},
  month = feb,
  journal = {BMC Bioinformatics},
  volume = {7},
  number = {1},
  pages = {91},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-7-91},
  abstract = {Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data.},
  file = {/Users/fariedabuzaid/Zotero/storage/SQYQ2VXZ/Varma and Simon - 2006 - Bias in error estimation when using cross-validati.pdf}
}

@article{vaswani_attention_2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = dec,
  journal = {arXiv:1706.03762 [cs]},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/4NECBBBK/Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}

@inproceedings{vaswani_general_2022,
  title = {A General Class of Surrogate Functions for Stable and Efficient Reinforcement Learning},
  booktitle = {25th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}} ({{AISTATS}} 2022)},
  author = {Vaswani, Sharan and Bachem, Olivier and Totaro, Simone and Mueller, Robert and Garg, Shivam and Geist, Matthieu and Machado, Marlos C. and Castro, Pablo Samuel and Roux, Nicolas Le},
  year = {2022},
  month = mar,
  eprint = {2108.05828},
  eprinttype = {arxiv},
  address = {{Virtual event}},
  abstract = {Common policy gradient methods rely on the maximization of a sequence of surrogate functions. In recent years, many such surrogate functions have been proposed, most without strong theoretical guarantees, leading to algorithms such as TRPO, PPO or MPO. Rather than design yet another surrogate function, we instead propose a general framework (FMA-PG) based on functional mirror ascent that gives rise to an entire family of surrogate functions. We construct surrogate functions that enable policy improvement guarantees, a property not shared by most existing surrogate functions. Crucially, these guarantees hold regardless of the choice of policy parameterization. Moreover, a particular instantiation of FMA-PG recovers important implementation heuristics (e.g., using forward vs reverse KL divergence) resulting in a variant of TRPO with additional desirable properties. Via experiments on simple bandit problems, we evaluate the algorithms instantiated by FMA-PG. The proposed framework also suggests an improved variant of PPO, whose robustness and efficiency we empirically demonstrate on the MuJoCo suite.},
  archiveprefix = {arXiv},
  langid = {english},
  annotation = {video: https://virtual.aistats.org/virtual/2022/oral/3518},
  file = {/Users/fariedabuzaid/Zotero/storage/G86GHWIF/Vaswani et al. - 2022 - A general class of surrogate functions for stable .pdf;/Users/fariedabuzaid/Zotero/storage/QUKGNSCE/2108.html}
}

@inproceedings{velickovic_graph_2020,
  title = {Graph {{Attention Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2020},
  abstract = {A novel approach to processing graph-structured data by neural networks, leveraging attention over a node's neighborhood. Achieves state-of-the-art results on transductive citation network tasks...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/VDUFXY6Y/Veličković et al. - 2022 - Graph Attention Networks.pdf}
}

@inproceedings{vignac_topn_2022,
  title = {Top-{{N}}: {{Equivariant Set}} and {{Graph Generation}} without {{Exchangeability}}},
  shorttitle = {Top-{{N}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Vignac, Clement and Frossard, Pascal},
  year = {2022},
  abstract = {This work addresses one-shot set and graph generation, and, more specifically, the parametrization of probabilistic decoders that map a vector-shaped prior to a distribution over sets or graphs....},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/PHDS7YDC/Vignac und Frossard - 2021 - Top-N Equivariant Set and Graph Generation withou.pdf;/Users/fariedabuzaid/Zotero/storage/KXTJ6B2A/forum.html}
}

@inproceedings{vincent-cuaz_semirelaxed_2022,
  title = {Semi-Relaxed {{Gromov-Wasserstein}} Divergence and Applications on Graphs},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {{Vincent-Cuaz}, C{\'e}dric and Flamary, R{\'e}mi and Corneli, Marco and Vayer, Titouan and Courty, Nicolas},
  year = {2022},
  abstract = {Comparing structured objects such as graphs is a fundamental operation involved in many learning tasks. To this end, the Gromov-Wasserstein (GW) distance, based on Optimal Transport (OT), has...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/Z8FLWBCR/Vincent-Cuaz et al. - 2021 - Semi-relaxed Gromov-Wasserstein divergence and app.pdf;/Users/fariedabuzaid/Zotero/storage/ZVDDX5WR/forum.html}
}

@inproceedings{wang_chemicalreactionaware_2022,
  title = {Chemical-{{Reaction-Aware Molecule Representation Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Wang, Hongwei and Li, Weijiang and Jin, Xiaomeng and Cho, Kyunghyun and Ji, Heng and Han, Jiawei and Burke, Martin},
  year = {2022},
  abstract = {Molecule representation learning (MRL) methods aim to embed molecules into a real vector space. However, existing SMILES-based (Simplified Molecular-Input Line-Entry System) or GNN-based (Graph...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/748SVUIM/Wang et al. - 2021 - Chemical-Reaction-Aware Molecule Representation Le.pdf;/Users/fariedabuzaid/Zotero/storage/CMWSM6RT/forum.html}
}

@article{wang_deep_2019,
  title = {Deep {{Neural Network Approximation}} for {{Custom Hardware}}: {{Where We}}'ve {{Been}}, {{Where We}}'re {{Going}}},
  shorttitle = {Deep {{Neural Network Approximation}} for {{Custom Hardware}}},
  author = {Wang, Erwei and Davis, James J. and Zhao, Ruizhe and Ng, Ho-Cheung and Niu, Xinyu and Luk, Wayne and Cheung, Peter Y. K. and Constantinides, George A.},
  year = {2019},
  month = may,
  journal = {ACM Computing Surveys},
  volume = {52},
  number = {2},
  eprint = {1901.06955},
  eprinttype = {arxiv},
  pages = {1--39},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3309551},
  abstract = {Deep neural networks have proven to be particularly effective in visual and audio recognition tasks. Existing models tend to be computationally expensive and memory intensive, however, and so methods for hardware-oriented approximation have become a hot topic. Research has shown that custom hardware-based neural network accelerators can surpass their general-purpose processor equivalents in terms of both throughput and energy efficiency. Application-tailored accelerators, when co-designed with approximation-based network training methods, transform large, dense and computationally expensive networks into small, sparse and hardware-efficient alternatives, increasing the feasibility of network deployment. In this article, we provide a comprehensive evaluation of approximation methods for high-performance network inference along with in-depth discussion of their effectiveness for custom hardware implementation. We also include proposals for future research based on a thorough analysis of current trends. This article represents the first survey providing detailed comparisons of custom hardware accelerators featuring approximation for both convolutional and recurrent neural networks, through which we hope to inspire exciting new developments in the field.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/LJ6I9VTR/Wang et al. - 2019 - Deep Neural Network Approximation for Custom Hardw.pdf}
}

@article{wang_enabling_2021,
  title = {Enabling {{Binary Neural Network Training}} on the {{Edge}}},
  author = {Wang, Erwei and Davis, James J. and Moro, Daniele and Zielinski, Piotr and Coelho, Claudionor and Chatterjee, Satrajit and Cheung, Peter Y. K. and Constantinides, George A.},
  year = {2021},
  month = jun,
  journal = {arXiv:2102.04270 [cs]},
  eprint = {2102.04270},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {The ever-growing computational demands of increasingly complex machine learning models frequently necessitate the use of powerful cloudbased infrastructure for their training. Binary neural networks are known to be promising candidates for on-device inference due to their extreme compute and memory savings over higherprecision alternatives. In this paper, we demonstrate that they are also strongly robust to gradient quantization, thereby making the training of modern models on the edge a practical reality. We introduce a low-cost binary neural network training strategy exhibiting sizable memory footprint reductions and energy savings vs Courbariaux \& Bengio's standard approach. Against the latter, we see coincident memory requirement and energy consumption drops of 2\textendash 6\texttimes, while reaching similar test accuracy in comparable time, across a range of small-scale models trained to classify popular datasets. We also showcase ImageNet training of ResNetE-18, achieving a 3.12\texttimes{} memory reduction over the aforementioned standard. Such savings will allow for unnecessary cloud offloading to be avoided, reducing latency, increasing energy efficiency and safeguarding privacy.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/LTTPR6GA/Wang et al. - 2021 - Enabling Binary Neural Network Training on the Edg.pdf}
}

@inproceedings{wang_energybased_2021,
  title = {Energy-{{Based Open-World Uncertainty Modeling}} for {{Confidence Calibration}}},
  booktitle = {{{arXiv}}:2107.12628 [Cs]},
  author = {Wang, Yezhen and Li, Bo and Che, Tong and Zhou, Kaiyang and Liu, Ziwei and Li, Dongsheng},
  year = {2021},
  month = aug,
  eprint = {2107.12628},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Confidence calibration is of great importance to the reliability of decisions made by machine learning systems. However, discriminative classifiers based on deep neural networks are often criticized for producing overconfident predictions that fail to reflect the true correctness likelihood of classification accuracy. We argue that such an inability to model uncertainty is mainly caused by the closed-world nature in softmax: a model trained by the cross-entropy loss will be forced to classify input into one of \$K\$ pre-defined categories with high probability. To address this problem, we for the first time propose a novel \$K\$+1-way softmax formulation, which incorporates the modeling of open-world uncertainty as the extra dimension. To unify the learning of the original \$K\$-way classification task and the extra dimension that models uncertainty, we propose a novel energy-based objective function, and moreover, theoretically prove that optimizing such an objective essentially forces the extra dimension to capture the marginal data distribution. Extensive experiments show that our approach, Energy-based Open-World Softmax (EOW-Softmax), is superior to existing state-of-the-art methods in improving confidence calibration.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/VM64UXLI/Wang et al. - 2021 - Energy-Based Open-World Uncertainty Modeling for C.pdf}
}

@inproceedings{wang_glass_2022,
  title = {{{GLASS}}: {{GNN}} with {{Labeling Tricks}} for {{Subgraph Representation Learning}}},
  shorttitle = {{{GLASS}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Wang, Xiyuan and Zhang, Muhan},
  year = {2022},
  abstract = {Despite the remarkable achievements of Graph Neural Networks (GNNs) on graph representation learning, few works have tried to use them to predict properties of subgraphs in the whole graph. The...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/Y2LI8CUC/Wang and Zhang - 2022 - GLASS GNN with Labeling Tricks for Subgraph Repre.pdf}
}

@inproceedings{wang_gnn_2022,
  title = {{{GNN}} Is a {{Counter}}? {{Revisiting GNN}} for {{Question Answering}}},
  shorttitle = {{{GNN}} Is a {{Counter}}?},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Wang, Kuan and Zhang, Yuyu and Yang, Diyi and Song, Le and Qin, Tao},
  year = {2022},
  abstract = {Question Answering (QA) has been a long-standing research topic in AI and NLP fields, and a wealth of studies has been conducted to attempt to equip QA systems with human-level reasoning...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/SEYMFNBR/Wang et al. - 2021 - GNN is a Counter Revisiting GNN for Question Answ.pdf;/Users/fariedabuzaid/Zotero/storage/YZV6BS6C/forum.html}
}

@article{wang_image_2004,
  title = {Image Quality Assessment: From Error Visibility to Structural Similarity},
  shorttitle = {Image Quality Assessment},
  author = {Wang, Zhou and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  year = {2004},
  month = apr,
  journal = {IEEE Transactions on Image Processing},
  volume = {13},
  number = {4},
  pages = {600--612},
  issn = {1941-0042},
  doi = {10.1109/TIP.2003.819861},
  abstract = {Objective methods for assessing perceptual image quality traditionally attempted to quantify the visibility of errors (differences) between a distorted image and a reference image using a variety of known properties of the human visual system. Under the assumption that human visual perception is highly adapted for extracting structural information from a scene, we introduce an alternative complementary framework for quality assessment based on the degradation of structural information. As a specific example of this concept, we develop a structural similarity index and demonstrate its promise through a set of intuitive examples, as well as comparison to both subjective ratings and state-of-the-art objective methods on a database of images compressed with JPEG and JPEG2000. A MATLAB implementation of the proposed algorithm is available online at http://www.cns.nyu.edu//spl sim/lcv/ssim/.},
  file = {/Users/fariedabuzaid/Zotero/storage/JYU9TUWC/1284395.html}
}

@inproceedings{wang_improving_2022,
  title = {Improving {{Cooperative Game Theory-based Data Valuation}} via {{Data Utility Learning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022). {{Workshop}} on {{Socially Responsible Machine Learning}}},
  author = {Wang, Tianhao and Yang, Yu and Jia, Ruoxi},
  year = {2022},
  month = apr,
  eprint = {2107.06336v2},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.06336},
  abstract = {The Shapley value (SV) and Least core (LC) are classic methods in cooperative game theory for cost/profit sharing problems. Both methods have recently been proposed as a principled solution for data valuation tasks, i.e., quantifying the contribution of individual datum in machine learning. However, both SV and LC suffer computational challenges due to the need for retraining models on combinatorially many data subsets. In this work, we propose to boost the efficiency in computing Shapley value or Least core by learning to estimate the performance of a learning algorithm on unseen data combinations. Theoretically, we derive bounds relating the error in the predicted learning performance to the approximation error in SV and LC. Empirically, we show that the proposed method can significantly improve the accuracy of SV and LC estimation.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/AYBYAA2X/Wang et al. - 2022 - Improving Cooperative Game Theory-based Data Valua.pdf}
}

@inproceedings{wang_language_2022,
  title = {Language Modeling via Stochastic Processes},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Wang, Rose E. and Durmus, Esin and Goodman, Noah and Hashimoto, Tatsunori},
  year = {2022},
  abstract = {Modern language models can generate high-quality short texts. However, they often meander or are incoherent when generating longer texts. These issues arise from the next-token-only language...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/NLGJ5UN9/Wang et al. - 2022 - Language modeling via stochastic processes.pdf}
}

@article{wang_learnability_2021,
  title = {Learnability of {{Learning Performance}} and {{Its Application}} to {{Data Valuation}}},
  author = {Wang, Tianhao and Yang, Yu and Jia, Ruoxi},
  year = {2021},
  month = jul,
  abstract = {For most machine learning (ML) tasks, evaluating learning performance on a given dataset requires intensive computation. On the other hand, the ability to efficiently estimate learning performance may benefit a wide spectrum of applications, such as active learning, data quality management, and data valuation. Recent empirical studies show that for many common ML models, one can accurately learn a parametric model that predicts learning performance for any given input datasets using a small amount of samples. However, the theoretical underpinning of the learnability of such performance prediction models is still missing. In this work, we develop the first theoretical analysis of the ML performance learning problem. We propose a relaxed notion for submodularity that can well describe the behavior of learning performance as a function of input datasets. We give a learning algorithm that achieves a constant-factor approximation under certain assumptions. Further, we give a learning algorithm that achieves arbitrarily small error based on a newly derived structural result. We then discuss a natural, important use case of learning performance learning -- data valuation, which is known to suffer computational challenges due to the requirement of estimating learning performance for many data combinations. We show that performance learning can significantly improve the accuracy of data valuation.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/N6SCUK9Z/T27AJ6VU.pdf}
}

@article{wang_less_2020,
  title = {Less {{Is Better}}: {{Unweighted Data Subsampling}} via {{Influence Function}}},
  shorttitle = {Less {{Is Better}}},
  author = {Wang, Zifeng and Zhu, Hong and Dong, Zhenhua and He, Xiuqiang and Huang, Shao-Lun},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {04},
  eprint = {1912.01321},
  eprinttype = {arxiv},
  pages = {6340--6347},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i04.6103},
  abstract = {In the time of Big Data, training complex models on large-scale data sets is challenging, making it appealing to reduce data volume for saving computation resources by subsampling. Most previous works in subsampling are weighted methods designed to help the performance of subset-model approach the full-set-model, hence the weighted methods have no chance to acquire a subset-model that is better than the full-set-model. However, we question that how can we achieve better model with less data? In this work, we propose a novel Unweighted Influence Data Subsampling (UIDS) method, and prove that the subset-model acquired through our method can outperform the full-set-model. Besides, we show that overly confident on a given test set for sampling is common in Influence-based subsampling methods, which can eventually cause our subset-model's failure in out-of-sample test. To mitigate it, we develop a probabilistic sampling scheme to control the worst-case risk over all distributions close to the empirical distribution. The experiment results demonstrate our methods superiority over existed subsampling methods in diverse tasks, such as text classification, image classification, click-through prediction, etc.},
  archiveprefix = {arXiv},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/2EGIPTFV/Wang et al. - 2021 - Less Is Better Unweighted Data Subsampling via In.pdf;/Users/fariedabuzaid/Zotero/storage/HYUHEVFT/Wang et al. - 2020 - Less Is Better Unweighted Data Subsampling via In.pdf}
}

@article{wang_oneround_2021,
  title = {One-{{Round Active Learning}}},
  author = {Wang, Tianhao and Chen, Si and Jia, Ruoxi},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.11843 [cs]},
  eprint = {2104.11843},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Active learning has been a main solution for reducing data labeling costs. However, existing active learning strategies assume that a data owner can interact with annotators in an online, timely manner, which is usually impractical. Even with such interactive annotators, for existing active learning strategies to be effective, they often require many rounds of interactions between the data owner and annotators, which is often time-consuming. In this work, we initiate the study of one-round active learning, which aims to select a subset of unlabeled data points that achieve the highest utility after being labeled with only the information from initially labeled data points. We propose DULO, a general framework for one-round active learning based on the notion of data utility functions, which map a set of data points to some performance measure of the model trained on the set. We formulate the one-round active learning problem as data utility function maximization. We further propose strategies to make the estimation and optimization of data utility functions scalable to large models and large unlabeled data sets. Our results demonstrate that while existing active learning approaches could succeed with multiple rounds, DULO consistently performs better in the one-round setting.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/LD9L2FSE/Wang et al. - 2021 - One-Round Active Learning.pdf}
}

@inproceedings{wang_pacbayes_2022,
  title = {{{PAC-Bayes Information Bottleneck}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Wang, Zifeng and Huang, Shao-Lun and Kuruoglu, Ercan Engin and Sun, Jimeng and Chen, Xi and Zheng, Yefeng},
  year = {2022},
  abstract = {Understanding the source of the superior generalization ability of NNs remains one of the most important problems in ML research. There have been a series of theoretical works trying to derive...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/YT7WXUB6/Wang et al. - 2021 - PAC-Bayes Information Bottleneck.pdf}
}

@incollection{wang_principled_2020,
  title = {A {{Principled Approach}} to {{Data Valuation}} for {{Federated Learning}}},
  booktitle = {Federated {{Learning}}: {{Privacy}} and {{Incentive}}},
  author = {Wang, Tianhao and Rausch, Johannes and Zhang, Ce and Jia, Ruoxi and Song, Dawn},
  editor = {Yang, Qiang and Fan, Lixin and Yu, Han},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {153--167},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-63076-8_11},
  abstract = {Federated learning (FL) is a popular technique to train machine learning (ML) models on decentralized data sources. In order to sustain long-term participation of data owners, it is important to fairly appraise each data source and compensate data owners for their contribution to the training process. The Shapley value (SV) defines a unique payoff scheme that satisfies many desiderata for a data value notion. It has been increasingly used for valuing training data in centralized learning. However, computing the SV requires exhaustively evaluating the model performance on every subset of data sources, which incurs prohibitive communication cost in the federated setting. Besides, the canonical SV ignores the order of data sources during training, which conflicts with the sequential nature of FL. This chapter proposes a variant of the SV amenable to FL, which we call the federated Shapley value. The federated SV preserves the desirable properties of the canonical SV while it can be calculated without incurring extra communication cost and is also able to capture the effect of participation order on data value. We conduct a thorough empirical study of the federated SV on a range of tasks, including noisy label detection, adversarial participant detection, and data summarization on different benchmark datasets, and demonstrate that it can reflect the real utility of data sources for FL and has the potential to enhance system robustness, security, and efficiency. We also report and analyze ``failure cases'' and hope to stimulate future research.},
  isbn = {978-3-030-63076-8},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/RADHZGBY/Wang et al. - 2020 - A Principled Approach to Data Valuation for Federa.pdf}
}

@inproceedings{wang_uncertainty_2021,
  title = {Uncertainty {{Estimation}} and {{Calibration}} with {{Finite-State Probabilistic RNNs}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2021)},
  author = {Wang, Cheng and Lawrence, Carolin and Niepert, Mathias},
  year = {2021},
  eprint = {2011.12010},
  eprinttype = {arxiv},
  address = {{Virtual event}},
  abstract = {Uncertainty quantification is crucial for building reliable and trustable machine learning systems. We propose to estimate uncertainty in recurrent neural networks (RNNs) via stochastic discrete state transitions over recurrent timesteps. The uncertainty of the model can be quantified by running a prediction several times, each time sampling from the recurrent state transition distribution, leading to potentially different results if the model is uncertain. Alongside uncertainty quantification, our proposed method offers several advantages in different settings. The proposed method can (1) learn deterministic and probabilistic automata from data, (2) learn well-calibrated models on real-world classification tasks, (3) improve the performance of out-of-distribution detection, and (4) control the exploration-exploitation trade-off in reinforcement learning. An implementation is available.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/3BYA8JAN/Wang et al. - 2021 - Uncertainty Estimation and Calibration with Finite.pdf}
}

@article{wang_understanding_2020,
  title = {Understanding and Mitigating Gradient Pathologies in Physics-Informed Neural Networks},
  author = {Wang, Sifan and Teng, Yujun and Perdikaris, Paris},
  year = {2020},
  month = jan,
  journal = {arXiv:2001.04536 [cs, math, stat]},
  eprint = {2001.04536},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {The widespread use of neural networks across different scientific domains often involves constraining them to satisfy certain symmetries, conservation laws, or other domain knowledge. Such constraints are often imposed as soft penalties during model training and effectively act as domain-specific regularizers of the empirical risk loss. Physics-informed neural networks is an example of this philosophy in which the outputs of deep neural networks are constrained to approximately satisfy a given set of partial differential equations. In this work we review recent advances in scientific machine learning with a specific focus on the effectiveness of physics-informed neural networks in predicting outcomes of physical systems and discovering hidden physics from noisy data. We will also identify and analyze a fundamental mode of failure of such approaches that is related to numerical stiffness leading to unbalanced back-propagated gradients during model training. To address this limitation we present a learning rate annealing algorithm that utilizes gradient statistics during model training to balance the interplay between different terms in composite loss functions. We also propose a novel neural network architecture that is more resilient to such gradient pathologies. Taken together, our developments provide new insights into the training of constrained neural networks and consistently improve the predictive accuracy of physics-informed neural networks by a factor of 50-100x across a range of problems in computational physics. All code and data accompanying this manuscript are publicly available at \textbackslash url\{https://github.com/PredictiveIntelligenceLab/GradientPathologiesPINNs\}.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/YT7MJ24X/Wang et al. - 2020 - Understanding and mitigating gradient pathologies .pdf}
}

@article{wang_when_2020,
  title = {When and Why {{PINNs}} Fail to Train: {{A}} Neural Tangent Kernel Perspective},
  shorttitle = {When and Why {{PINNs}} Fail to Train},
  author = {Wang, Sifan and Yu, Xinling and Perdikaris, Paris},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.14527 [cs, math, stat]},
  eprint = {2007.14527},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Physics-informed neural networks (PINNs) have lately received great attention thanks to their flexibility in tackling a wide range of forward and inverse problems involving partial differential equations. However, despite their noticeable empirical success, little is known about how such constrained neural networks behave during their training via gradient descent. More importantly, even less is known about why such models sometimes fail to train at all. In this work, we aim to investigate these questions through the lens of the Neural Tangent Kernel (NTK); a kernel that captures the behavior of fully-connected neural networks in the infinite width limit during training via gradient descent. Specifically, we derive the NTK of PINNs and prove that, under appropriate conditions, it converges to a deterministic kernel that stays constant during training in the infinite-width limit. This allows us to analyze the training dynamics of PINNs through the lens of their limiting NTK and find a remarkable discrepancy in the convergence rate of the different loss components contributing to the total training error. To address this fundamental pathology, we propose a novel gradient descent algorithm that utilizes the eigenvalues of the NTK to adaptively calibrate the convergence rate of the total training error. Finally, we perform a series of numerical experiments to verify the correctness of our theory and the practical effectiveness of the proposed algorithms. The data and code accompanying this manuscript are publicly available at \textbackslash url\{https://github.com/PredictiveIntelligenceLab/PINNsNTK\}.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/PNXS7J9N/Wang et al. - 2020 - When and why PINNs fail to train A neural tangent.pdf}
}

@article{ward_improving_2019,
  title = {Improving {{Exploration}} in {{Soft-Actor-Critic}} with {{Normalizing Flows Policies}}},
  author = {Ward, Patrick Nadeem and Smofsky, Ariella and Bose, Avishek Joey},
  year = {2019},
  month = jun,
  journal = {arXiv:1906.02771 [cs, stat]},
  eprint = {1906.02771},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Deep Reinforcement Learning (DRL) algorithms for continuous action spaces are known to be brittle toward hyperparameters as well as \textbackslash cut\{being\}sample inefficient. Soft Actor Critic (SAC) proposes an off-policy deep actor critic algorithm within the maximum entropy RL framework which offers greater stability and empirical gains. The choice of policy distribution, a factored Gaussian, is motivated by \textbackslash cut\{chosen due\}its easy re-parametrization rather than its modeling power. We introduce Normalizing Flow policies within the SAC framework that learn more expressive classes of policies than simple factored Gaussians. \textbackslash cut\{We also present a series of stabilization tricks that enable effective training of these policies in the RL setting.\}We show empirically on continuous grid world tasks that our approach increases stability and is better suited to difficult exploration in sparse reward settings.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/PNMGX2DN/Ward et al. - 2019 - Improving Exploration in Soft-Actor-Critic with No.pdf}
}

@incollection{widmann_calibration_2019,
  title = {Calibration Tests in Multi-Class Classification: {{A}} Unifying Framework},
  shorttitle = {Calibration Tests in Multi-Class Classification},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Widmann, David and Lindsten, Fredrik and Zachariah, Dave},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {12257--12267},
  publisher = {{Curran Associates, Inc.}},
  abstract = {In safety-critical applications a probabilistic model is usually required to be cali brated, i.e., to capture the uncertainty of its predictions accurately. In multi-class classification, calibration of the most confident predictions only is often not suffi cient. We propose and study calibration measures for multi-class classification that generalize existing measures such as the expected calibration error, the maximum calibration error, and the maximum mean calibration error. We propose and evalu ate empirically different consistent and unbiased estimators for a specific class of measures based on matrix-valued kernels. Importantly, these estimators can be in terpreted as test statistics associated with well-defined bounds and approximations of the p-value under the null hypothesis that the model is calibrated, significantly improving the interpretability of calibration measures, which otherwise lack any meaningful unit or scale.},
  annotation = {citecount: 00002},
  file = {/Users/fariedabuzaid/Zotero/storage/9LHMUS7N/Widmann et al. - 2019 - Calibration tests in multi-class classification A.pdf;/Users/fariedabuzaid/Zotero/storage/IZMQZNN3/main_neurips_supplementary.pdf}
}

@inproceedings{widmann_calibration_2020,
  title = {Calibration Tests beyond Classification},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Widmann, David and Lindsten, Fredrik and Zachariah, Dave},
  year = {2020},
  month = sep,
  abstract = {Most supervised machine learning tasks are subject to irreducible prediction errors. Probabilistic predictive models address this limitation by providing probability distributions that represent a...},
  langid = {english},
  annotation = {blog: https://devmotion.github.io/Calibration\_ICLR2021/dev/},
  file = {/Users/fariedabuzaid/Zotero/storage/JZQTFJBZ/Widmann et al. - 2020 - Calibration tests beyond classification.pdf}
}

@inproceedings{wijesinghe_new_2022,
  title = {A {{New Perspective}} on "{{How Graph Neural Networks Go Beyond Weisfeiler-Lehman}}?"},
  shorttitle = {A {{New Perspective}} on "{{How Graph Neural Networks Go Beyond Weisfeiler-Lehman}}?},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Wijesinghe, Asiri and Wang, Qing},
  year = {2022},
  abstract = {We propose a new perspective on designing powerful Graph Neural Networks (GNNs). In a nutshell, this enables a general solution to inject structural properties of graphs into a message-passing...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/SJ9JIFAP/Wijesinghe und Wang - 2021 - A New Perspective on How Graph Neural Networks Go.pdf;/Users/fariedabuzaid/Zotero/storage/9SSSSTD6/forum.html}
}

@incollection{wolpert_supervised_2002,
  title = {The {{Supervised Learning No-Free-Lunch Theorems}}},
  booktitle = {Soft {{Computing}} and {{Industry}}: {{Recent Applications}}},
  author = {Wolpert, David H.},
  editor = {Roy, Rajkumar and K{\"o}ppen, Mario and Ovaska, Seppo and Furuhashi, Takeshi and Hoffmann, Frank},
  year = {2002},
  pages = {25--42},
  publisher = {{Springer}},
  address = {{London}},
  doi = {10.1007/978-1-4471-0123-9_3},
  abstract = {This paper reviews the supervised learning versions of the no-free-lunch theorems in a simplified form. It also discusses the significance of those theorems, and their relation to other aspects of supervised learning.},
  isbn = {978-1-4471-0123-9},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/E23Z82SI/Wolpert - 2002 - The Supervised Learning No-Free-Lunch Theorems.pdf}
}

@misc{wong_wasserstein_2020,
  title = {Wasserstein {{Adversarial Examples}} via {{Projected Sinkhorn Iterations}}},
  author = {Wong, Eric and Schmidt, Frank R. and Kolter, J. Zico},
  year = {2020},
  month = jan,
  number = {arXiv:1902.07906},
  eprint = {1902.07906},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1902.07906},
  abstract = {A rapidly growing area of work has studied the existence of adversarial examples, datapoints which have been perturbed to fool a classifier, but the vast majority of these works have focused primarily on threat models defined by \$\textbackslash ell\_p\$ norm-bounded perturbations. In this paper, we propose a new threat model for adversarial attacks based on the Wasserstein distance. In the image classification setting, such distances measure the cost of moving pixel mass, which naturally cover "standard" image manipulations such as scaling, rotation, translation, and distortion (and can potentially be applied to other settings as well). To generate Wasserstein adversarial examples, we develop a procedure for projecting onto the Wasserstein ball, based upon a modified version of the Sinkhorn iteration. The resulting algorithm can successfully attack image classification models, bringing traditional CIFAR10 models down to 3\% accuracy within a Wasserstein ball with radius 0.1 (i.e., moving 10\% of the image mass 1 pixel), and we demonstrate that PGD-based adversarial training can improve this adversarial accuracy to 76\%. In total, this work opens up a new direction of study in adversarial robustness, more formally considering convex metrics that accurately capture the invariances that we typically believe should exist in classifiers. Code for all experiments in the paper is available at https://github.com/locuslab/projected\_sinkhorn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/fariedabuzaid/Zotero/storage/7RD397S2/Wong et al. - 2020 - Wasserstein Adversarial Examples via Projected Sin.pdf;/Users/fariedabuzaid/Zotero/storage/9XKE3HRF/1902.html}
}

@article{wright_deep_2022,
  title = {Deep Physical Neural Networks Trained with Backpropagation},
  author = {Wright, Logan G. and Onodera, Tatsuhiro and Stein, Martin M. and Wang, Tianyu and Schachter, Darren T. and Hu, Zoey and McMahon, Peter L.},
  year = {2022},
  month = jan,
  journal = {Nature},
  volume = {601},
  number = {7894},
  pages = {549--555},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04223-6},
  abstract = {Deep-learning models have become pervasive tools in science and engineering. However, their energy requirements now increasingly limit their scalability1. Deep-learning accelerators2\textendash 9 aim to perform deep learning energy-efficiently, usually targeting the inference phase and often by exploiting physical substrates beyond conventional electronics. Approaches so far10\textendash 22 have been unable to apply the backpropagation algorithm to train unconventional novel hardware in situ. The advantages of backpropagation have made it the~de facto~training method for large-scale neural networks, so this deficiency constitutes a major impediment. Here we introduce a hybrid in situ\textendash in silico algorithm,~called physics-aware training, that applies backpropagation to train controllable physical systems. Just as deep learning realizes computations with deep neural networks made from layers of mathematical functions, our approach allows us to train deep~physical neural networks~made from layers of controllable physical systems, even when the physical layers lack any mathematical isomorphism to conventional artificial neural network layers. To demonstrate the universality of our approach, we train diverse physical neural networks based on optics, mechanics and electronics to experimentally perform audio and image classification tasks. Physics-aware training combines the scalability of backpropagation with the automatic mitigation of imperfections and noise achievable with~in situ~algorithms.~Physical neural networks have the potential to perform machine learning faster and more energy-efficiently than conventional electronic processors and, more broadly, can endow physical systems with automatically designed physical functionalities, for example, for robotics23\textendash 26, materials27\textendash 29 and smart sensors30\textendash 32.},
  copyright = {2022 The Author(s)},
  langid = {english},
  annotation = {notion: https://www.notion.so/appliedaiinitiative/Deep-physical-neural-networks-trained-with-backpropagation-808c7fe60638474597c4223ae607ae33 post: https://community.appliedai.de/topics/27304/topic\_feed\_posts/1216584},
  file = {/Users/fariedabuzaid/Zotero/storage/6Z6TUCPV/Wright et al. - 2022 - Deep physical neural networks trained with backpro.pdf}
}

@article{wu_behavior_2019,
  title = {Behavior {{Regularized Offline Reinforcement Learning}}},
  author = {Wu, Yifan and Tucker, George and Nachum, Ofir},
  year = {2019},
  month = nov,
  journal = {arXiv:1911.11361 [cs, stat]},
  eprint = {1911.11361},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {In reinforcement learning (RL) research, it is common to assume access to direct online interactions with the environment. However in many real-world applications, access to the environment is limited to a fixed offline dataset of logged experience. In such settings, standard RL algorithms have been shown to diverge or otherwise yield poor performance. Accordingly, recent work has suggested a number of remedies to these issues. In this work, we introduce a general framework, behavior regularized actor critic (BRAC), to empirically evaluate recently proposed methods as well as a number of simple baselines across a variety of offline continuous control tasks. Surprisingly, we find that many of the technical complexities introduced in recent methods are unnecessary to achieve strong performance. Additional ablations provide insights into which design choices matter most in the offline RL setting.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/UMITTWZM/Wu et al. - 2019 - Behavior Regularized Offline Reinforcement Learnin.pdf}
}

@article{wu_current_2021,
  title = {Current {{Time Series Anomaly Detection Benchmarks}} Are {{Flawed}} and Are {{Creating}} the {{Illusion}} of {{Progress}}},
  author = {Wu, Renjie and Keogh, Eamonn J.},
  year = {2021},
  month = aug,
  journal = {arXiv:2009.13807 [cs, stat]},
  eprint = {2009.13807},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Time series anomaly detection has been a perennially important topic in data science, with papers dating back to the 1950s. However, in recent years there has been an explosion of interest in this topic, much of it driven by the success of deep learning in other domains and for other time series tasks. Most of these papers test on one or more of a handful of popular benchmark datasets, created by Yahoo, Numenta, NASA, etc. In this work we make a surprising claim. The majority of the individual exemplars in these datasets suffer from one or more of four flaws. Because of these four flaws, we believe that many published comparisons of anomaly detection algorithms may be unreliable, and more importantly, much of the apparent progress in recent years may be illusionary. In addition to demonstrating these claims, with this paper we introduce the UCR Time Series Anomaly Archive. We believe that this resource will perform a similar role as the UCR Time Series Classification Archive, by providing the community with a benchmark that allows meaningful comparisons between approaches and a meaningful gauge of overall progress.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/S7U3HRDT/Wu and Keogh - 2021 - Current Time Series Anomaly Detection Benchmarks a.pdf}
}

@inproceedings{wu_discovering_2022,
  title = {Discovering {{Invariant Rationales}} for {{Graph Neural Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Wu, Yingxin and Wang, Xiang and Zhang, An and He, Xiangnan and Chua, Tat-Seng},
  year = {2022},
  abstract = {Intrinsic interpretability of graph neural networks (GNNs) is to find a small subset of the input graph's features --- rationale --- which guides the model prediction. Unfortunately, the leading...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/ZFL88T3N/Wu et al. - 2022 - Discovering Invariant Rationales for Graph Neural .pdf}
}

@inproceedings{wu_handling_2022,
  title = {Handling {{Distribution Shifts}} on {{Graphs}}: {{An Invariance Perspective}}},
  shorttitle = {Handling {{Distribution Shifts}} on {{Graphs}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Wu, Qitian and Zhang, Hengrui and Yan, Junchi and Wipf, David},
  year = {2022},
  abstract = {There is increasing evidence suggesting neural networks' sensitivity to distribution shifts, so that research on out-of-distribution (OOD) generalization comes into the spotlight. Nonetheless...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/4NTKR3Z9/Wu et al. - 2021 - Handling Distribution Shifts on Graphs An Invaria.pdf;/Users/fariedabuzaid/Zotero/storage/BM7B5LCL/forum.html}
}

@article{wu_l1norm_2019,
  title = {L1-{{Norm Batch Normalization}} for {{Efficient Training}} of {{Deep Neural Networks}}},
  author = {Wu, Shuang and Li, Guoqi and Deng, Lei and Liu, Liu and Xie, Yuan and Shi, Luping},
  year = {2019},
  month = jul,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {30},
  number = {7},
  eprint = {1802.09769},
  eprinttype = {arxiv},
  pages = {2043--2051},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2018.2876179},
  abstract = {Batch Normalization (BN) has been proven to be quite effective at accelerating and improving the training of deep neural networks (DNNs). However, BN brings additional computation, consumes more memory and generally slows down the training process by a large margin, which aggravates the training effort. Furthermore, the nonlinear square and root operations in BN also impede the low bit-width quantization techniques, which draws much attention in deep learning hardware community. In this work, we propose an L1-norm BN (L1BN) with only linear operations in both the forward and the backward propagations during training. L1BN is shown to be approximately equivalent to the original L2-norm BN (L2BN) by multiplying a scaling factor. Experiments on various convolutional neural networks (CNNs) and generative adversarial networks (GANs) reveal that L1BN maintains almost the same accuracies and convergence rates compared to L2BN but with higher computational efficiency. On FPGA platform, the proposed signum and absolute operations in L1BN can achieve 1.5\$\textbackslash times\$ speedup and save 50\textbackslash\% power consumption, compared with the original costly square and root operations, respectively. This hardware-friendly normalization method not only surpasses L2BN in speed, but also simplify the hardware design of ASIC accelerators with higher energy efficiency. Last but not the least, L1BN promises a fully quantized training of DNNs, which is crucial to future adaptive terminal devices.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/BTQQ5PVJ/Wu et al. - 2019 - L1-Norm Batch Normalization for Efficient Training.pdf}
}

@inproceedings{wu_spatial_2022,
  title = {Spatial {{Graph Attention}} and {{Curiosity-driven Policy}} for {{Antiviral Drug Discovery}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Wu, Yulun and Choma, Nicholas and Chen, Andrew Deru and Cashman, Mikaela and Prates, Erica Teixeira and Vergara, Veronica G. Melesse and Shah, Manesh B. and Clyde, Austin and Brettin, Thomas and de Jong, Wibe Albert and Kumar, Neeraj and Head, Martha S. and Stevens, Rick L. and Nugent, Peter and Jacobson, Daniel A. and Brown, James B.},
  year = {2022},
  abstract = {We developed Distilled Graph Attention Policy Networks (DGAPNs), a reinforcement learning model to generate novel graph-structured chemical representations that optimize user-defined objectives by...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/HMZV7XH9/Wu et al. - 2021 - Spatial Graph Attention and Curiosity-driven Polic.pdf;/Users/fariedabuzaid/Zotero/storage/ICSHZ6I3/forum.html}
}

@misc{wu_stronger_2020,
  title = {Stronger and {{Faster Wasserstein Adversarial Attacks}}},
  author = {Wu, Kaiwen and Wang, Allen Houze and Yu, Yaoliang},
  year = {2020},
  month = aug,
  number = {arXiv:2008.02883},
  eprint = {2008.02883},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {Deep models, while being extremely flexible and accurate, are surprisingly vulnerable to "small, imperceptible" perturbations known as adversarial attacks. While the majority of existing attacks focus on measuring perturbations under the \$\textbackslash ell\_p\$ metric, Wasserstein distance, which takes geometry in pixel space into account, has long been known to be a suitable metric for measuring image quality and has recently risen as a compelling alternative to the \$\textbackslash ell\_p\$ metric in adversarial attacks. However, constructing an effective attack under the Wasserstein metric is computationally much more challenging and calls for better optimization algorithms. We address this gap in two ways: (a) we develop an exact yet efficient projection operator to enable a stronger projected gradient attack; (b) we show that the Frank-Wolfe method equipped with a suitable linear minimization oracle works extremely fast under Wasserstein constraints. Our algorithms not only converge faster but also generate much stronger attacks. For instance, we decrease the accuracy of a residual network on CIFAR-10 to \$3.4\textbackslash\%\$ within a Wasserstein perturbation ball of radius \$0.005\$, in contrast to \$65.6\textbackslash\%\$ using the previous Wasserstein attack based on an \textbackslash emph\{approximate\} projection operator. Furthermore, employing our stronger attacks in adversarial training significantly improves the robustness of adversarially trained models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/fariedabuzaid/Zotero/storage/HRYF2K3G/Wu et al. - 2020 - Stronger and Faster Wasserstein Adversarial Attack.pdf;/Users/fariedabuzaid/Zotero/storage/YLBRQ38T/2008.html}
}

@misc{xiao_barriernet_2021,
  title = {{{BarrierNet}}: {{A Safety-Guaranteed Layer}} for {{Neural Networks}}},
  shorttitle = {{{BarrierNet}}},
  author = {Xiao, Wei and Hasani, Ramin and Li, Xiao and Rus, Daniela},
  year = {2021},
  month = nov,
  number = {2111.11277},
  eprint = {2111.11277},
  eprinttype = {arxiv},
  publisher = {{arXiv}},
  abstract = {This paper introduces differentiable higher-order control barrier functions (CBF) that are end-to-end trainable together with learning systems. CBFs are usually overly conservative, while guaranteeing safety. Here, we address their conservativeness by softening their definitions using environmental dependencies without loosing safety guarantees, and embed them into differentiable quadratic programs. These novel safety layers, termed a BarrierNet, can be used in conjunction with any neural network-based controller, and can be trained by gradient descent. BarrierNet allows the safety constraints of a neural controller be adaptable to changing environments. We evaluate them on a series of control problems such as traffic merging and robot navigations in 2D and 3D space, and demonstrate their effectiveness compared to state-of-the-art approaches.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/II3YNEIL/Xiao et al. - 2021 - BarrierNet A Safety-Guaranteed Layer for Neural N.pdf}
}

@misc{xie_neurosymbolic_2022,
  title = {Neuro-{{Symbolic Verification}} of {{Deep Neural Networks}}},
  author = {Xie, Xuan and Kersting, Kristian and Neider, Daniel},
  year = {2022},
  month = mar,
  number = {arXiv:2203.00938},
  eprint = {2203.00938},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.00938},
  abstract = {Formal verification has emerged as a powerful approach to ensure the safety and reliability of deep neural networks. However, current verification tools are limited to only a handful of properties that can be expressed as first-order constraints over the inputs and output of a network. While adversarial robustness and fairness fall under this category, many real-world properties (e.g., "an autonomous vehicle has to stop in front of a stop sign") remain outside the scope of existing verification technology. To mitigate this severe practical restriction, we introduce a novel framework for verifying neural networks, named neuro-symbolic verification. The key idea is to use neural networks as part of the otherwise logical specification, enabling the verification of a wide variety of complex, real-world properties, including the one above. Moreover, we demonstrate how neuro-symbolic verification can be implemented on top of existing verification infrastructure for neural networks, making our framework easily accessible to researchers and practitioners alike.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/A4BX6VDK/Xie et al. - 2022 - Neuro-Symbolic Verification of Deep Neural Network.pdf}
}

@inproceedings{xie_tale_2022,
  title = {A {{Tale}} of {{Two Flows}}: {{Cooperative Learning}} of {{Langevin Flow}} and {{Normalizing Flow Toward Energy-Based Model}}},
  shorttitle = {A {{Tale}} of {{Two Flows}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Xie, Jianwen and Zhu, Yaxuan and Li, Jun and Li, Ping},
  year = {2022},
  abstract = {This paper studies the cooperative learning of two generative flow models, in which the two models are iteratively updated based on the jointly synthesized examples. The first flow model is a...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/3VMWQQPX/Xie et al. - 2021 - A Tale of Two Flows Cooperative Learning of Lange.pdf;/Users/fariedabuzaid/Zotero/storage/BBZIU97S/forum.html}
}

@inproceedings{xu_anomaly_2022,
  title = {Anomaly {{Transformer}}: {{Time Series Anomaly Detection}} with {{Association Discrepancy}}},
  shorttitle = {Anomaly {{Transformer}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Xu, Jiehui and Wu, Haixu and Wang, Jianmin and Long, Mingsheng},
  year = {2022},
  abstract = {Unsupervised detection of anomaly points in time series is a challenging problem, which requires the model to derive a distinguishable criterion. Previous methods tackle the problem mainly through...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/W3KH9LMB/Xu et al. - 2022 - Anomaly Transformer Time Series Anomaly Detection.pdf}
}

@inproceedings{xu_geodiff_2022,
  title = {{{GeoDiff}}: {{A Geometric Diffusion Model}} for {{Molecular Conformation Generation}}},
  shorttitle = {{{GeoDiff}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Xu, Minkai and Yu, Lantao and Song, Yang and Shi, Chence and Ermon, Stefano and Tang, Jian},
  year = {2022},
  abstract = {Predicting molecular conformations from molecular graphs is a fundamental problem in cheminformatics and drug discovery. Recently, significant progress has been achieved with machine learning...},
  langid = {english},
  annotation = {video:https://iclr.cc/virtual/2022/oral/7029},
  file = {/Users/fariedabuzaid/Zotero/storage/7FUSLHWR/Xu et al. - 2021 - GeoDiff A Geometric Diffusion Model for Molecular.pdf}
}

@inproceedings{yan_evaluating_2020,
  title = {Evaluating and {{Rewarding Teamwork Using Cooperative Game Abstractions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yan, Tom and Kroer, Christian and Peysakhovich, Alexander},
  year = {2020},
  month = jun,
  volume = {33},
  eprint = {2006.09538},
  eprinttype = {arxiv},
  pages = {6925--6935},
  abstract = {Can we predict how well a team of individuals will perform together? How should individuals be rewarded for their contributions to the team performance? Cooperative game theory gives us a powerful set of tools for answering these questions: the Characteristic Function (CF) and solution concepts like the Shapley Value (SV). There are two major difficulties in applying these techniques to real world problems: first, the CF is rarely given to us and needs to be learned from data. Second, the SV is combinatorial in nature. We introduce a parametric model called cooperative game abstractions (CGAs) for estimating CFs from data. CGAs are easy to learn, readily interpretable, and crucially allow linear-time computation of the SV. We provide identification results and sample complexity bounds for CGA models as well as error bounds in the estimation of the SV using CGAs. We apply our methods to study teams of artificial RL agents as well as real world teams from professional sports.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/DDGL7V6R/Yan et al. - 2020 - Evaluating and Rewarding Teamwork Using Cooperativ.pdf}
}

@inproceedings{yan_if_2021,
  title = {If {{You Like Shapley Then You}}'ll {{Love}} the {{Core}}},
  booktitle = {Proceedings of the 35th {{AAAI Conference}} on {{Artificial Intelligence}}, 2021},
  author = {Yan, Tom and Procaccia, Ariel D.},
  year = {2021},
  month = feb,
  publisher = {{Association for the Advancement of Artificial Intelligence}},
  address = {{Virtual conference}},
  abstract = {The prevalent approach to problems of credit assignment in machine learning \textemdash{} such as feature and data valuation\textemdash{} is to model the problem at hand as a cooperative game and apply the Shapley value. But cooperative game theory offers a rich menu of alternative solution concepts, which famously includes the core and its variants. Our goal is to challenge the machine learning community's current consensus around the Shapley value, and make a case for the core as a viable alternative. To that end, we prove that arbitrarily good approximations to the least core \textemdash{} a core relaxation that is always feasible \textemdash{} can be computed efficiently (but prove an impossibility for a more refined solution concept, the nucleolus). We also perform experiments that corroborate these theoretical results and shed light on settings where the least core may be preferable to the Shapley value.},
  copyright = {Copyright (c) 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/ENJSBQSF/Yan, Tom and Procaccia, Ariel D. - 2021 - If You Like Shapley Then You’ll Love the Core.pdf}
}

@article{yang_physicsinformed_2018,
  title = {Physics-{{Informed Generative Adversarial Networks}} for {{Stochastic Differential Equations}}},
  author = {Yang, Liu and Zhang, Dongkun and Karniadakis, George Em},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.02033 [cs, math, stat]},
  eprint = {1811.02033},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {We developed a new class of physics-informed generative adversarial networks (PI-GANs) to solve in a unified manner forward, inverse and mixed stochastic problems based on a limited number of scattered measurements. Unlike standard GANs relying only on data for training, here we encoded into the architecture of GANs the governing physical laws in the form of stochastic differential equations (SDEs) using automatic differentiation. In particular, we applied Wasserstein GANs with gradient penalty (WGAN-GP) for its enhanced stability compared to vanilla GANs. We first tested WGAN-GP in approximating Gaussian processes of different correlation lengths based on data realizations collected from simultaneous reads at sparsely placed sensors. We obtained good approximation of the generated stochastic processes to the target ones even for a mismatch between the input noise dimensionality and the effective dimensionality of the target stochastic processes. We also studied the overfitting issue for both the discriminator and generator, and we found that overfitting occurs also in the generator in addition to the discriminator as previously reported. Subsequently, we considered the solution of elliptic SDEs requiring approximations of three stochastic processes, namely the solution, the forcing, and the diffusion coefficient. We used three generators for the PI-GANs, two of them were feed forward deep neural networks (DNNs) while the other one was the neural network induced by the SDE. Depending on the data, we employed one or multiple feed forward DNNs as the discriminators in PI-GANs. Here, we have demonstrated the accuracy and effectiveness of PI-GANs in solving SDEs for up to 30 dimensions, but in principle, PI-GANs could tackle very high dimensional problems given more sensor data with low-polynomial growth in computational cost.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/DA6LLUDY/Yang et al. - 2018 - Physics-Informed Generative Adversarial Networks f.pdf}
}

@inproceedings{yin_understanding_2019,
  title = {Understanding {{Straight-Through Estimator}} in {{Training Activation Quantized Neural Nets}}},
  booktitle = {{{arXiv}}:1903.05662 [Cs, Math, Stat]},
  author = {Yin, Penghang and Lyu, Jiancheng and Zhang, Shuai and Osher, Stanley and Qi, Yingyong and Xin, Jack},
  year = {2019},
  month = sep,
  eprint = {1903.05662},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  abstract = {Training activation quantized neural networks involves minimizing a piecewise constant function whose gradient vanishes almost everywhere, which is undesirable for the standard back-propagation or chain rule. An empirical way around this issue is to use a straight-through estimator (STE) (Bengio et al., 2013) in the backward pass only, so that the ``gradient'' through the modified chain rule becomes non-trivial. Since this unusual ``gradient'' is certainly not the gradient of loss function, the following question arises: why searching in its negative direction minimizes the training loss? In this paper, we provide the theoretical justification of the concept of STE by answering this question. We consider the problem of learning a two-linear-layer network with binarized ReLU activation and Gaussian input data. We shall refer to the unusual ``gradient'' given by the STE-modifed chain rule as coarse gradient. The choice of STE is not unique. We prove that if the STE is properly chosen, the expected coarse gradient correlates positively with the population gradient (not available for the training), and its negation is a descent direction for minimizing the population loss. We further show the associated coarse gradient descent algorithm converges to a critical point of the population loss minimization problem. Moreover, we show that a poor choice of STE leads to instability of the training algorithm near certain local minima, which is verified with CIFAR-10 experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/S2K7LGAU/Yin et al. - 2019 - Understanding Straight-Through Estimator in Traini.pdf}
}

@inproceedings{yona_who_2021,
  title = {Who's {{Responsible}}? {{Jointly Quantifying}} the {{Contribution}} of the {{Learning Algorithm}} and {{Data}}},
  shorttitle = {Who's {{Responsible}}?},
  booktitle = {Proceedings of the 2021 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Yona, Gal and Ghorbani, Amirata and Zou, James},
  year = {2021},
  month = jul,
  pages = {1034--1041},
  publisher = {{ACM}},
  address = {{Virtual Event USA}},
  doi = {10.1145/3461702.3462574},
  abstract = {A learning algorithm A trained on a dataset D is revealed to have poor performance on some subpopulation at test time. Where should the responsibility for this lay? It can be argued that the data is responsible, if for example training A on a more representative dataset D' would have improved the performance. But it can similarly be argued that A itself is at fault, if training a different variant A' on the same dataset D would have improved performance. As ML becomes widespread and such failure cases more common, these types of questions are proving to be far from hypothetical. With this motivation in mind, in this work we provide a rigorous formulation of the joint credit assignment problem between a learning algorithm A and a dataset D. We propose Extended Shapley as a principled framework for this problem, and experiment empirically with how it can be used to address questions of ML accountability.},
  isbn = {978-1-4503-8473-5},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/DMIUWZX3/Yona et al. - 2021 - Who's Responsible Jointly Quantifying the Contrib.pdf}
}

@inproceedings{yoon_data_2020,
  title = {Data {{Valuation}} Using {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 37 Th {{International Conference}} on {{Machine Learning}},},
  author = {Yoon, Jinsung and Ar{\i}k, Sercan {\"O} and Pfister, Tomas},
  year = {2020},
  volume = {119},
  pages = {10},
  address = {{Viena}},
  abstract = {Quantifying the value of data is a fundamental problem in machine learning and has multiple important use cases: (1) building insights about the dataset and task, (2) domain adaptation, (3) corrupted sample discovery, and (4) robust learning. We propose Data Valuation using Reinforcement Learning (DVRL), to adaptively learn data values jointly with the predictor model. DVRL uses a data value estimator (DVE) to learn how likely each datum is used in training of the predictor model. DVE is trained using a reinforcement signal that reflects performance on the target task. We demonstrate that DVRL yields superior data value estimates compared to alternative methods across numerous datasets and application scenarios. The corrupted sample discovery performance of DVRL is close to optimal in many regimes (i.e. as if the noisy samples were known apriori), and for domain adaptation and robust learning DVRL significantly outperforms state-of-the-art by 14.6\% and 10.8\%, respectively.},
  langid = {english},
  annotation = {video: https://icml.cc/virtual/2020/poster/6276},
  file = {/Users/fariedabuzaid/Zotero/storage/ZCWV3XAG/24ESAJ3I.pdf}
}

@inproceedings{you_bayesian_2022,
  title = {Bayesian {{Modeling}} and {{Uncertainty Quantification}} for {{Learning}} to {{Optimize}}: {{What}}, {{Why}}, and {{How}}},
  shorttitle = {Bayesian {{Modeling}} and {{Uncertainty Quantification}} for {{Learning}} to {{Optimize}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {You, Yuning and Cao, Yue and Chen, Tianlong and Wang, Zhangyang and Shen, Yang},
  year = {2022},
  abstract = {Optimizing an objective function with uncertainty awareness is well-known to improve the accuracy and confidence of optimization solutions. Meanwhile, another relevant but very different question...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/DPB6HHAR/You et al. - 2021 - Bayesian Modeling and Uncertainty Quantification f.pdf;/Users/fariedabuzaid/Zotero/storage/Q2C7HN75/forum.html}
}

@inproceedings{zarka_separation_2022,
  title = {Separation and {{Concentration}} in {{Deep Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR2022}})},
  author = {Zarka, John and Guth, Florentin and Mallat, St{\'e}phane},
  year = {2022},
  abstract = {Numerical experiments demonstrate that deep neural network classifiers progressively separate class distributions around their mean, achieving linear separability on the training set, and...},
  langid = {english},
  annotation = {video: https://iclr.cc/virtual/2021/poster/2842},
  file = {/Users/fariedabuzaid/Zotero/storage/6DX75JTV/Zarka et al. - 2020 - Separation and Concentration in Deep Networks.pdf}
}

@inproceedings{zhang_boosting_2022,
  title = {Boosting the {{Certified Robustness}} of {{L-infinity Distance Nets}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Zhang, Bohang and Jiang, Du and He, Di and Wang, Liwei},
  year = {2022},
  abstract = {Recently, Zhang et al. (2021) developed a new neural network architecture based on \$\textbackslash ell\_\textbackslash infty\$-distance functions, which naturally possesses certified \$\textbackslash ell\_\textbackslash infty\$ robustness by its...},
  langid = {english},
  annotation = {video: https://iclr.cc/virtual/2022/poster/6543},
  file = {/Users/fariedabuzaid/Zotero/storage/R32FGZZN/Zhang et al. - 2021 - Boosting the Certified Robustness of L-infinity Di.pdf}
}

@article{zhang_certifying_2021,
  title = {Towards {{Certifying L-infinity Robustness}} Using {{Neural Networks}} with {{L-inf-dist Neurons}}},
  author = {Zhang, Bohang and Cai, Tianle and Lu, Zhou and He, Di and Wang, Liwei},
  year = {2021},
  month = jun,
  journal = {arXiv:2102.05363 [cs, stat]},
  eprint = {2102.05363},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {It is well-known that standard neural networks, even with a high classification accuracy, are vulnerable to small \$\textbackslash ell\_\textbackslash infty\$-norm bounded adversarial perturbations. Although many attempts have been made, most previous works either can only provide empirical verification of the defense to a particular attack method, or can only develop a certified guarantee of the model robustness in limited scenarios. In this paper, we seek for a new approach to develop a theoretically principled neural network that inherently resists \$\textbackslash ell\_\textbackslash infty\$ perturbations. In particular, we design a novel neuron that uses \$\textbackslash ell\_\textbackslash infty\$-distance as its basic operation (which we call \$\textbackslash ell\_\textbackslash infty\$-dist neuron), and show that any neural network constructed with \$\textbackslash ell\_\textbackslash infty\$-dist neurons (called \$\textbackslash ell\_\{\textbackslash infty\}\$-dist net) is naturally a 1-Lipschitz function with respect to \$\textbackslash ell\_\textbackslash infty\$-norm. This directly provides a rigorous guarantee of the certified robustness based on the margin of prediction outputs. We then prove that such networks have enough expressive power to approximate any 1-Lipschitz function with robust generalization guarantee. We further provide a holistic training strategy that can greatly alleviate optimization difficulties. Experimental results show that using \$\textbackslash ell\_\{\textbackslash infty\}\$-dist nets as basic building blocks, we consistently achieve state-of-the-art performance on commonly used datasets: 93.09\% certified accuracy on MNIST (\$\textbackslash epsilon=0.3\$), 35.42\% on CIFAR-10 (\$\textbackslash epsilon=8/255\$) and 16.31\% on TinyImageNet (\$\textbackslash epsilon=1/255\$).},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/5LACWE6J/Zhang et al. - 2021 - Towards Certifying L-infinity Robustness using Neu.pdf}
}

@article{zhang_fbsde_2020,
  title = {{{FBSDE}} Based Neural Network Algorithms for High-Dimensional Quasilinear Parabolic {{PDEs}}},
  author = {Zhang, Wenzhong and Cai, Wei},
  year = {2020},
  month = dec,
  journal = {arXiv:2012.07924 [cs, math]},
  eprint = {2012.07924},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  abstract = {In this paper, we propose forward and backward stochastic differential equations (FBSDEs) based deep neural network (DNN) learning algorithms for the solution of high dimensional quasilinear parabolic partial differential equations (PDEs). The algorithms relies on a learning process by minimizing the path-wise difference of two discrete stochastic processes, which are defined by the time discretization of the FBSDEs and the DNN representation of the PDE solutions, respectively. The proposed algorithms demonstrate a convergence for a 100-dimensional Black--Scholes--Barenblatt equation at a rate similar to that of the Euler--Maruyama discretization of the FBSDEs.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/S9RQ2YMB/Zhang and Cai - 2020 - FBSDE based neural network algorithms for high-dim.pdf}
}

@article{zhang_fsim_2011,
  title = {{{FSIM}}: {{A Feature Similarity Index}} for {{Image Quality Assessment}}},
  shorttitle = {{{FSIM}}},
  author = {Zhang, Lin and Zhang, Lei and Mou, Xuanqin and Zhang, David},
  year = {2011},
  month = aug,
  journal = {IEEE Transactions on Image Processing},
  volume = {20},
  number = {8},
  pages = {2378--2386},
  issn = {1941-0042},
  doi = {10.1109/TIP.2011.2109730},
  abstract = {Image quality assessment (IQA) aims to use computational models to measure the image quality consistently with subjective evaluations. The well-known structural similarity index brings IQA from pixel- to structure-based stage. In this paper, a novel feature similarity (FSIM) index for full reference IQA is proposed based on the fact that human visual system (HVS) understands an image mainly according to its low-level features. Specifically, the phase congruency (PC), which is a dimensionless measure of the significance of a local structure, is used as the primary feature in FSIM. Considering that PC is contrast invariant while the contrast information does affect HVS' perception of image quality, the image gradient magnitude (GM) is employed as the secondary feature in FSIM. PC and GM play complementary roles in characterizing the image local quality. After obtaining the local quality map, we use PC again as a weighting function to derive a single quality score. Extensive experiments performed on six benchmark IQA databases demonstrate that FSIM can achieve much higher consistency with the subjective evaluations than state-of-the-art IQA metrics.},
  file = {/Users/fariedabuzaid/Zotero/storage/ELG9ZKNT/Zhang et al. - 2011 - FSIM A Feature Similarity Index for Image Quality.pdf}
}

@inproceedings{zhang_greaselm_2022,
  title = {{{GreaseLM}}: {{Graph REASoning Enhanced Language Models}}},
  shorttitle = {{{GreaseLM}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Zhang, Xikun and Bosselut, Antoine and Yasunaga, Michihiro and Ren, Hongyu and Liang, Percy and Manning, Christopher D. and Leskovec, Jure},
  year = {2022},
  abstract = {Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/PGDMMHSU/Zhang et al. - 2022 - GreaseLM Graph REASoning Enhanced Language Models.pdf}
}

@inproceedings{zhang_information_2022,
  title = {Information {{Gain Propagation}}: A {{New Way}} to {{Graph Active Learning}} with {{Soft Labels}}},
  shorttitle = {Information {{Gain Propagation}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Zhang, Wentao and Wang, Yexin and You, Zhenbang and Cao, Meng and Huang, Ping and Shan, Jiulong and Yang, Zhi and Cui, Bin},
  year = {2022},
  abstract = {Graph Neural Networks (GNNs) have achieved great success in various tasks, but their performance highly relies on a large number of labeled nodes, which typically requires considerable human...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/22W84RV8/Zhang et al. - 2022 - Information Gain Propagation a New Way to Graph A.pdf}
}

@inproceedings{zhang_theoretically_2019,
  title = {Theoretically {{Principled Trade-off}} between {{Robustness}} and {{Accuracy}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and Ghaoui, Laurent El and Jordan, Michael},
  year = {2019},
  month = may,
  eprint = {1901.08573},
  eprinttype = {arxiv},
  pages = {7472--7482},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of ~2,000 submissions, surpassing the runner-up approach by 11.41\% in terms of mean L\_2 perturbation distance.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/A6GYKD2E/Zhang et al. - 2019 - Theoretically Principled Trade-off between Robustn.pdf}
}

@article{zhang_understanding_2016,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2016},
  month = nov,
  abstract = {Through extensive systematic experiments, we show how the traditional approaches fail to explain why large neural networks generalize well in practice, and why understanding deep learning requires...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/JW97WAJA/Zhang et al. - 2016 - Understanding deep learning requires rethinking ge.pdf}
}

@inproceedings{zhang_understanding_2021,
  title = {Understanding {{Failures}} in {{Out-of-Distribution Detection}} with {{Deep Generative Models}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Zhang, Lily and Goldstein, Mark and Ranganath, Rajesh},
  year = {2021},
  month = jul,
  pages = {12427--12436},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Deep generative models (DGMs) seem a natural fit for detecting out-of-distribution (OOD) inputs, but such models have been shown to assign higher probabilities or densities to OOD images than images from the training distribution. In this work, we explain why this behavior should be attributed to model misestimation. We first prove that no method can guarantee performance beyond random chance without assumptions on which out-distributions are relevant. We then interrogate the typical set hypothesis, the claim that relevant out-distributions can lie in high likelihood regions of the data distribution, and that OOD detection should be defined based on the data distribution's typical set. We highlight the consequences implied by assuming support overlap between in- and out-distributions, as well as the arbitrariness of the typical set for OOD detection. Our results suggest that estimation error is a more plausible explanation than the misalignment between likelihood-based OOD detection and out-distributions of interest, and we illustrate how even minimal estimation error can lead to OOD detection failures, yielding implications for future work in deep generative modeling and OOD detection.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/6GISSRSI/Zhang et al. - 2021 - Understanding Failures in Out-of-Distribution Dete.pdf;/Users/fariedabuzaid/Zotero/storage/MRAYWX72/Zhang et al. - 2021 - Understanding Failures in Out-of-Distribution Dete.pdf}
}

@inproceedings{zhang_understanding_2022,
  title = {Understanding {{Intrinsic Robustness Using Label Uncertainty}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR2022}})},
  author = {Zhang, Xiao and Evans, David},
  year = {2022},
  abstract = {A fundamental question in adversarial machine learning is whether a robust classifier exists for a given task. A line of research has made some progress towards this goal by studying the...},
  langid = {english},
  annotation = {slides: https://iclr.cc/media/iclr-2022/Slides/6167.pdf video: https://iclr.cc/virtual/2022/poster/6167},
  file = {/Users/fariedabuzaid/Zotero/storage/QNZPZ3V6/Zhang and Evans - 2021 - Understanding Intrinsic Robustness Using Label Unc.pdf}
}

@inproceedings{zhang_unifying_2022,
  title = {Unifying {{Likelihood-free Inference}} with {{Black-box Optimization}} and {{Beyond}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Zhang, Dinghuai and Fu, Jie and Bengio, Yoshua and Courville, Aaron},
  year = {2022},
  address = {{Virtual event}},
  abstract = {Black-box optimization formulations for biological sequence design have drawn recent attention due to their promising potential impact on the pharmaceutical industry. In this work, we propose to...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/EG29IMI8/Zhang et al. - 2021 - Unifying Likelihood-free Inference with Black-box .pdf}
}

@inproceedings{zhang_unreasonable_2018,
  title = {The {{Unreasonable Effectiveness}} of {{Deep Features}} as a {{Perceptual Metric}}},
  booktitle = {{{CVPR}} 2018},
  author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A. and Shechtman, Eli and Wang, Oliver},
  year = {2018},
  month = apr,
  eprint = {1801.03924},
  eprinttype = {arxiv},
  abstract = {While it is nearly effortless for humans to quickly assess the perceptual similarity between two images, the underlying processes are thought to be quite complex. Despite this, the most widely used perceptual metrics today, such as PSNR and SSIM, are simple, shallow functions, and fail to account for many nuances of human perception. Recently, the deep learning community has found that features of the VGG network trained on ImageNet classification has been remarkably useful as a training loss for image synthesis. But how perceptual are these so-called "perceptual losses"? What elements are critical for their success? To answer these questions, we introduce a new dataset of human perceptual similarity judgments. We systematically evaluate deep features across different architectures and tasks and compare them with classic metrics. We find that deep features outperform all previous metrics by large margins on our dataset. More surprisingly, this result is not restricted to ImageNet-trained VGG features, but holds across different deep architectures and levels of supervision (supervised, self-supervised, or even unsupervised). Our results suggest that perceptual similarity is an emergent property shared across deep visual representations.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/WZ27BR58/Zhang et al. - 2018 - The Unreasonable Effectiveness of Deep Features as.pdf}
}

@article{zhao_calibrating_2021,
  title = {Calibrating {{Predictions}} to {{Decisions}}: {{A Novel Approach}} to {{Multi-Class Calibration}}},
  shorttitle = {Calibrating {{Predictions}} to {{Decisions}}},
  author = {Zhao, Shengjia and Kim, Michael P. and Sahoo, Roshni and Ma, Tengyu and Ermon, Stefano},
  year = {2021},
  month = jul,
  journal = {arXiv:2107.05719 [cs, stat]},
  eprint = {2107.05719},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {When facing uncertainty, decision-makers want predictions they can trust. A machine learning provider can convey confidence to decision-makers by guaranteeing their predictions are distribution calibrated -- amongst the inputs that receive a predicted class probabilities vector \$q\$, the actual distribution over classes is \$q\$. For multi-class prediction problems, however, achieving distribution calibration tends to be infeasible, requiring sample complexity exponential in the number of classes \$C\$. In this work, we introduce a new notion -- \textbackslash emph\{decision calibration\} -- that requires the predicted distribution and true distribution to be ``indistinguishable'' to a set of downstream decision-makers. When all possible decision makers are under consideration, decision calibration is the same as distribution calibration. However, when we only consider decision makers choosing between a bounded number of actions (e.g. polynomial in \$C\$), our main result shows that decisions calibration becomes feasible -- we design a recalibration algorithm that requires sample complexity polynomial in the number of actions and the number of classes. We validate our recalibration algorithm empirically: compared to existing methods, decision calibration improves decision-making on skin lesion and ImageNet classification with modern neural network predictors.},
  archiveprefix = {arXiv},
  file = {/Users/fariedabuzaid/Zotero/storage/J4R3HNGX/Zhao et al. - 2021 - Calibrating Predictions to Decisions A Novel Appr.pdf}
}

@inproceedings{zhao_comparing_2022,
  title = {Comparing {{Distributions}} by {{Measuring Differences}} That {{Affect Decision Making}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Zhao, Shengjia and Sinha, Abhishek and He, Yutong and Perreault, Aidan and Song, Jiaming and Ermon, Stefano},
  year = {2022},
  abstract = {Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a...},
  langid = {english},
  annotation = {video: https://iclr.cc/virtual/2022/oral/6696},
  file = {/Users/fariedabuzaid/Zotero/storage/NGI7ZNRR/Zhao et al. - 2021 - Comparing Distributions by Measuring Differences t.pdf}
}

@incollection{zheng_dags_2018,
  title = {{{DAGs}} with {{NO TEARS}}: {{Continuous Optimization}} for {{Structure Learning}}},
  shorttitle = {{{DAGs}} with {{NO TEARS}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep K and Xing, Eric P},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {9472--9483},
  publisher = {{Curran Associates, Inc.}},
  annotation = {citecount: 00000},
  file = {/Users/fariedabuzaid/Zotero/storage/68EY9XJQ/supplement.pdf;/Users/fariedabuzaid/Zotero/storage/PZFFVSVY/Zheng et al. - 2018 - DAGs with NO TEARS Continuous Optimization for St.pdf}
}

@inproceedings{zheng_deep_2022,
  title = {Deep {{AutoAugment}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR}} 2022)},
  author = {Zheng, Yu and Zhang, Zhi and Yan, Shen and Zhang, Mi},
  year = {2022},
  abstract = {While recent automated data augmentation methods lead to state-of-the-art results, their design spaces and the derived data augmentation strategies still incorporate strong human priors. In this...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/ZBMXEUZ4/Zheng et al. - 2021 - Deep AutoAugment.pdf}
}

@inproceedings{zheng_improving_2016,
  title = {Improving the {{Robustness}} of {{Deep Neural Networks}} via {{Stability Training}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Zheng, Stephan and Song, Yang and Leung, Thomas and Goodfellow, Ian},
  year = {2016},
  pages = {4480--4488},
  abstract = {In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep networks against small input distortions that result from various types of common image processing, such as compression, rescaling, and cropping. We validate our method by stabilizing the state-of-the-art Inception architecture against these types of distortions. In addition, we demonstrate that our stabilized model gives robust state-of-the-art performance on large-scale near-duplicate detection, similar-image ranking, and classification on noisy datasets.},
  file = {/Users/fariedabuzaid/Zotero/storage/QZGBN4DW/Zheng et al. - 2016 - Improving the Robustness of Deep Neural Networks v.pdf}
}

@inproceedings{zhu_causal_2020,
  title = {Causal {{Discovery}} with {{Reinforcement Learning}}},
  booktitle = {{{arXiv}}:1906.04477 [Cs, Stat]},
  author = {Zhu, Shengyu and Ng, Ignavier and Chen, Zhitang},
  year = {2020},
  month = jun,
  eprint = {1906.04477},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  address = {{Virtual conference}},
  abstract = {Discovering causal structure among a set of variables is a fundamental problem in many empirical sciences. Traditional score-based casual discovery methods rely on various local heuristics to search for a Directed Acyclic Graph (DAG) according to a predefined score function. While these methods, e.g., greedy equivalence search, may have attractive results with infinite samples and certain model assumptions, they are usually less satisfactory in practice due to finite data and possible violation of assumptions. Motivated by recent advances in neural combinatorial optimization, we propose to use Reinforcement Learning (RL) to search for the DAG with the best scoring. Our encoder-decoder model takes observable data as input and generates graph adjacency matrices that are used to compute rewards. The reward incorporates both the predefined score function and two penalty terms for enforcing acyclicity. In contrast with typical RL applications where the goal is to learn a policy, we use RL as a search strategy and our final output would be the graph, among all graphs generated during training, that achieves the best reward. We conduct experiments on both synthetic and real datasets, and show that the proposed approach not only has an improved search ability but also allows a flexible score function under the acyclicity constraint.},
  archiveprefix = {arXiv},
  annotation = {video: https://iclr.cc/virtual\_2020/poster\_S1g2skStPB.html},
  file = {/Users/fariedabuzaid/Zotero/storage/YANUMHBJ/Zhu et al. - 2020 - Causal Discovery with Reinforcement Learning.pdf}
}

@article{zhu_physicsconstrained_2019,
  title = {Physics-Constrained Deep Learning for High-Dimensional Surrogate Modeling and Uncertainty Quantification without Labeled Data},
  author = {Zhu, Yinhao and Zabaras, Nicholas and Koutsourelakis, Phaedon-Stelios and Perdikaris, Paris},
  year = {2019},
  month = oct,
  journal = {Journal of Computational Physics},
  volume = {394},
  pages = {56--81},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2019.05.024},
  abstract = {Surrogate modeling and uncertainty quantification tasks for PDE systems are most often considered as supervised learning problems where input and output data pairs are used for training. The construction of such emulators is by definition a small data problem which poses challenges to deep learning approaches that have been developed to operate in the big data regime. Even in cases where such models have been shown to have good predictive capability in high dimensions, they fail to address constraints in the data implied by the PDE model. This paper provides a methodology that incorporates the governing equations of the physical model in the loss/likelihood functions. The resulting physics-constrained, deep learning models are trained without any labeled data (e.g. employing only input data) and provide comparable predictive responses with data-driven models while obeying the constraints of the problem at hand. This work employs a convolutional encoder-decoder neural network approach as well as a conditional flow-based generative model for the solution of PDEs, surrogate model construction, and uncertainty quantification tasks. The methodology is posed as a minimization problem of the reverse Kullback-Leibler (KL) divergence between the model predictive density and the reference conditional density, where the later is defined as the Boltzmann-Gibbs distribution at a given inverse temperature with the underlying potential relating to the PDE system of interest. The generalization capability of these models to out-of-distribution input is considered. Quantification and interpretation of the predictive uncertainty is provided for a number of problems.},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/B5LDS8PU/Zhu et al. - 2019 - Physics-constrained deep learning for high-dimensi.pdf}
}

@techreport{ziegler_unsupervised_2020,
  title = {Unsupervised {{Recalibration}}},
  author = {Ziegler, Albert and Czy{\.z}, Pawe{\l}},
  year = {2020},
  month = oct,
  eprint = {1908.09157},
  eprinttype = {arxiv},
  institution = {{GitHub}},
  abstract = {Unsupervised recalibration (URC) is a general way to improve the accuracy of an already trained probabilistic classification or regression model upon encountering new data while deployed in the field. URC does not require any ground truth associated with the new field data. URC merely observes the model's predictions and recognizes when the training set is not representative of field data, and then corrects to remove any introduced bias. URC can be particularly useful when applied separately to different subpopulations observed in the field that were not considered as features when training the machine learning model. This makes it possible to exploit subpopulation information without retraining the model or even having ground truth for some or all subpopulations available. Additionally, if these subpopulations are the object of study, URC serves to determine the correct ground truth distributions for them, where naive aggregation methods, like averaging the model's predictions, systematically underestimate their differences.},
  archiveprefix = {arXiv},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/89SXMWD6/Ziegler and Czyż - 2020 - Unsupervised Recalibration.pdf}
}

@inproceedings{zugner_endtoend_2022,
  title = {End-to-{{End Learning}} of {{Probabilistic Hierarchies}} on {{Graphs}}},
  booktitle = {International {{Conference}} on {{Learning Representations}} ({{ICLR2022}})},
  author = {Z{\"u}gner, Daniel and Charpentier, Bertrand and Ayle, Morgane and Geringer, Sascha and G{\"u}nnemann, Stephan},
  year = {2022},
  abstract = {We propose a novel probabilistic model over hierarchies on graphs obtained by continuous relaxation of tree-based hierarchies. We draw connections to Markov chain theory, enabling us to perform...},
  langid = {english},
  file = {/Users/fariedabuzaid/Zotero/storage/KL6RZVYJ/Zügner et al. - 2022 - End-to-End Learning of Probabilistic Hierarchies o.pdf}
}



---
__object__: src.explib.base.ExperimentCollection
name: concrete_diabetes_cancer
experiments:
  - &concrete
    __object__: src.explib.hyperopt.HyperoptExperiment
    name: concrete_flow
    device: "cpu"
    scheduler:  
      __object__: ray.tune.schedulers.ASHAScheduler
      max_t: 1000000
      grace_period: 1000000
      reduction_factor: 2
    num_hyperopt_samples: &num_hyperopt 1
    gpus_per_trial: 0
    cpus_per_trial: &cpus_per_trial 2
    tuner_params:
      metric: val_loss
      mode: min
    trial_config:
      logging:
        images: false
        "image_shape": [9, 1]
      dataset:
        __object__: src.explib.datasets.TabularData
        dataloc: "/home/mustafa/Downloads/datasets/tabular_data/trashcan/this_should_be_empty/"
        save_split_dir: "/home/mustafa/Downloads/datasets/tabular_data/concrete"
        drop_columns: []
        first_run: false
        device: "cpu"
      epochs:  200000  # 200000
      patience: &patience 100
      batch_size:
        __eval__: tune.choice([16])
      optim_cfg:
        optimizer:
          __class__: torch.optim.Adam
        params:
          lr:
            __eval__: &lr tune.loguniform(1e-5, 1e-3)
          weight_decay: 0.0
      model_cfg:
        type:
          __class__:  src.usflows.flows.USFlow
        params:
          in_dims: [9]
          coupling_blocks: 2
          conditioner_cls:
            __class__: src.usflows.networks.ConvNet2D
          conditioner_args:
            c_in: 1
            num_layers: 3
          affine_conjugation: true
          lu_transform: 1
          householder: 0
          soft_training: false
          masktype: checkerboard
          training_noise_prior:
            __object__: pyro.distributions.Laplace
            loc:
              __eval__: torch.zeros(9).to("cpu")
            scale:
              __eval__: torch.ones(9).to("cpu")
          prior_scale: 1.0
          base_distribution:
            __object__: src.usflows.distributions.RadialDistribution
            device: "cpu"
            p: 1.0
            loc:
              __eval__: torch.zeros(9).to("cpu")
            norm_distribution:
              __object__: pyro.distributions.LogNormal # loc=0, scale=1
              loc:
                __eval__: torch.ones(1).to("cpu")
              scale:
                __eval__:   (0.5 * torch.ones(1)).to("cpu")
  - &cancer
    __object__: src.explib.hyperopt.HyperoptExperiment
    name: cancer_flow
    device: "cpu"
    scheduler:
      __object__: ray.tune.schedulers.ASHAScheduler
      max_t: 1000000
      grace_period: 1000000
      reduction_factor: 2
    num_hyperopt_samples: *num_hyperopt
    gpus_per_trial: 0
    cpus_per_trial: *cpus_per_trial
    tuner_params:
      metric: val_loss
      mode: min
    trial_config:
      logging:
        images: false
        "image_shape": [31, 1]
      dataset:
        __object__: src.explib.datasets.TabularData
        dataloc: "/home/mustafa/Downloads/datasets/tabular_data/trashcan/this_should_be_empty/"
        save_split_dir: "/home/mustafa/Downloads/datasets/tabular_data/cancer"
        drop_columns: []
        first_run: false
        device: "cpu"
      epochs:  200000  # 200000
      patience: *patience
      batch_size:
        __eval__: tune.choice([16])
      optim_cfg:
        optimizer:
          __class__: torch.optim.Adam
        params:
          lr:
            __eval__: tune.loguniform(1e-5, 1e-3)
          weight_decay: 0.0
      model_cfg:
        type:
          __class__:  src.usflows.flows.NiceFlow
        params:
          masktype: alternate
          soft_training: false
          training_noise_prior:
            __object__: pyro.distributions.Laplace
            loc:
              __eval__: torch.zeros(31).to("cpu")
            scale:
              __eval__: torch.ones(31).to("cpu")
          prior_scale: 1.0
          coupling_layers:
            __eval__: tune.choice([l for l in range(1, 3)])
          coupling_nn_layers:
            __eval__: tune.choice([[31] * l for l in range(1, 3)])
          nonlinearity:
            __eval__: tune.choice([torch.nn.ReLU()])
          split_dim: 15
          base_distribution:
            __object__: src.usflows.distributions.RadialDistribution
            device: "cpu"
            p: 1.0
            loc:
              __eval__: torch.zeros(31).to("cpu")
            norm_distribution:
              __object__: pyro.distributions.LogNormal # loc=0, scale=1
              loc:
                __eval__: torch.ones(1).to("cpu")
              scale:
                __eval__:   (0.5 * torch.ones(1)).to("cpu")
          use_lu: true
  - &diabetes
    __object__: src.explib.hyperopt.HyperoptExperiment
    name: diabetes_flow
    device: "cpu"
    scheduler:
      __object__: ray.tune.schedulers.ASHAScheduler
      max_t: 1000000
      grace_period: 1000000
      reduction_factor: 2
    num_hyperopt_samples: *num_hyperopt
    gpus_per_trial: 0
    cpus_per_trial: *cpus_per_trial
    tuner_params:
      metric: val_loss
      mode: min
    trial_config:
      logging:
        images: false
        "image_shape": [9, 1]
      dataset:
        __object__: src.explib.datasets.TabularData
        dataloc: "/home/mustafa/Downloads/datasets/tabular_data/trashcan/this_should_be_empty/"
        save_split_dir: "/home/mustafa/Downloads/datasets/tabular_data/diabetes"
        drop_columns: []
        first_run: false
        device: "cpu"
      epochs:  200000  # 200000
      patience: *patience
      batch_size:
        __eval__: tune.choice([16])
      optim_cfg:
        optimizer:
          __class__: torch.optim.Adam
        params:
          lr:
            __eval__: tune.loguniform(1e-5, 1e-3)
          weight_decay: 0.0
      model_cfg:
        type:
          __class__:  src.usflows.flows.NiceFlow
        params:
          masktype: alternate
          soft_training: false
          training_noise_prior:
            __object__: pyro.distributions.Laplace
            loc:
              __eval__: torch.zeros(9).to("cpu")
            scale:
              __eval__: torch.ones(9).to("cpu")
          prior_scale: 1.0
          coupling_layers:
            __eval__: tune.choice([l for l in range(1, 3)])
          coupling_nn_layers:
            __eval__: tune.choice([[9] * l for l in range(1, 3)])
          nonlinearity:
            __eval__: tune.choice([torch.nn.ReLU()])
          split_dim: 5
          base_distribution:
            __object__: src.usflows.distributions.RadialDistribution
            device: "cpu"
            p: 1.0
            loc:
              __eval__: torch.zeros(9).to("cpu")
            norm_distribution:
              __object__: pyro.distributions.LogNormal # loc=0, scale=1
              loc:
                __eval__: torch.ones(1).to("cpu")
              scale:
                __eval__:   (0.5 * torch.ones(1)).to("cpu")
          use_lu: true














          ---
          __object__: src.explib.base.ExperimentCollection
          name: concrete_diabetes_cancer
          experiments:
            - &concrete
              __object__: src.explib.hyperopt.HyperoptExperiment
              name: concrete_flow
              device: "cpu"
              scheduler:
                __object__: ray.tune.schedulers.ASHAScheduler
                max_t: 1000000
                grace_period: 1000000
                reduction_factor: 2
              num_hyperopt_samples: &num_hyperopt 1
              gpus_per_trial: 0
              cpus_per_trial: &cpus_per_trial 2
              tuner_params:
                metric: val_loss
                mode: min
              trial_config:
                logging:
                  images: false
                  "image_shape": [ 9, 1 ]
                dataset:
                  __object__: src.explib.datasets.TabularData
                  dataloc: "/home/mustafa/Downloads/datasets/tabular_data/trashcan/this_should_be_empty/"
                  save_split_dir: "/home/mustafa/Downloads/datasets/tabular_data/concrete"
                  drop_columns: [ ]
                  first_run: false
                  device: "cpu"
                epochs: 200000  # 200000
                patience: &patience 100
                batch_size:
                  __eval__: tune.choice([16])
                optim_cfg:
                  optimizer:
                    __class__: torch.optim.Adam
                  params:
                    lr:
                      __eval__: &lr tune.loguniform(1e-5, 1e-3)
                    weight_decay: 0.0
                model_cfg:
                  type:
                    __class__: src.usflows.flows.USFlow
                  params:
                    in_dims: [ 9 ]
                    coupling_blocks: 2
                    conditioner_cls:
                      __class__: src.usflows.networks.ConvNet2D
                    conditioner_args:
                      c_in: 1
                      num_layers: 3
                    affine_conjugation: true
                    lu_transform: 1
                    householder: 0
                    soft_training: false
                    masktype: checkerboard
                    training_noise_prior:
                      __object__: pyro.distributions.Laplace
                      loc:
                        __eval__: torch.zeros(9).to("cpu")
                      scale:
                        __eval__: torch.ones(9).to("cpu")
                    prior_scale: 1.0
                    base_distribution:
                      __object__: src.usflows.distributions.RadialDistribution
                      device: "cpu"
                      p: 1.0
                      loc:
                        __eval__: torch.zeros(9).to("cpu")
                      norm_distribution:
                        __object__: pyro.distributions.LogNormal # loc=0, scale=1
                        loc:
                          __eval__: torch.ones(1).to("cpu")
                        scale:
                          __eval__: (0.5 * torch.ones(1)).to("cpu")

